{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace BART Inference with TensorRT\n",
    "\n",
    "BART is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows easy steps to convert a [HuggingFace PyTorch BART model](https://huggingface.co/docs/transformers/model_doc/bart) to a TensorRT engine for high-performance inference, with performance comparison between PyTorch and TensorRT inference.\n",
    "\n",
    "1. [Download HuggingFace BART model](#1)\n",
    "1. [PyTorch HuggingFace Inference](#2)\n",
    "1. [TensorRT Engine Building](#3)\n",
    "1. [TensorRT Inference](#4)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Follow the instructions at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip3 install -r ../requirements.txt\n",
    "#!pip3 install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# disable warning in notebook\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# notebook widgets\n",
    "import ipywidgets as widgets\n",
    "widget_style = {'description_width': 'initial'}\n",
    "widget_layout = widgets.Layout(width='auto')\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "from tensorrt import PreviewFeature\n",
    "from polygraphy.backend.trt import Profile\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoModelForPreTraining,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "# BART\n",
    "from BART.BARTModelConfig import BARTModelTRTConfig, BARTMetadata\n",
    "from BART.measurements import encoder_inference, decoder_inference, full_inference_greedy, full_inference_beam\n",
    "from BART.export import BARTEncoderTorchFile, BARTDecoderTorchFile, BARTEncoderONNXFile, BARTDecoderONNXFile, BARTEncoderTRTEngine, BARTDecoderTRTEngine\n",
    "from BART.trt import BARTTRTEncoder, BARTTRTDecoder\n",
    "\n",
    "# Whisper\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata\n",
    "from Whisper.measurements import encoder_inference as wh_encoder_inference, decoder_inference as wh_decoder_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderONNXFile, WhisperDecoderONNXFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder\n",
    "\n",
    "\n",
    "# NNDF\n",
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.general_utils import measure_python_inference_code\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace BART model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch BART model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The BART variants that are suported by TensorRT are: facebook/bart-base (139M), facebook/bart-large (406M), facebook/bart-large-cnn (406M), facebook/mbart-large-50 (680M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14eabc-d863-454d-9078-849acc857bb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model and Inference Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774c89f3-7dbb-423d-88b2-1de693324389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2433e66b41e942fa9e0f9fbde10b25ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Model variant:', layout=Layout(width='auto'), options=('facebook/bart-base', 'facebook/bar…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UI\n",
    "model_widget = widgets.Select(\n",
    "    options=['facebook/bart-base', 'facebook/bart-large', 'facebook/bart-large-cnn', 'facebook/mbart-large-50'],\n",
    "    value='facebook/bart-base',\n",
    "    description='Model variant:',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed04130e-7f20-4a3e-bf76-52aa335f402d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a388986f1234736867b4f029eac46b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=False, description='FP16', indent=False, layout=Layout(width='auto'), style=Chec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BART_VARIANT = model_widget.value\n",
    "\n",
    "disable_preview_dynamic_feature_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Disable 8.6 EA faster dynamic shapes feature',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "FP16_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='FP16',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "HF_KV_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='HuggingFace KV cache',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "TRT_KV_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='TensorRT KV cache (disabled due to performance improvements in progress, not beating non-KV version yet)', #  \n",
    "    disabled=True,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "KV_widgets = widgets.HBox([HF_KV_widget,TRT_KV_widget])\n",
    "\n",
    "batch_size_widget = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Batch size',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "max_input_len_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.MAX_SEQUENCE_LENGTH[BART_VARIANT],\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Max input length',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "min_output_len_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[BART_VARIANT],\n",
    "    min=0,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Min output length',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "max_output_len_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.MAX_OUTPUT_LENGTH[BART_VARIANT],\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Max output length',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "encoder_hidden_size_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.ENCODER_HIDDEN_SIZE[BART_VARIANT],\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Encoder hidden size',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "num_beam_widget = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Number of beams',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "widgets_all = widgets.VBox([\n",
    "    FP16_widget, \n",
    "    disable_preview_dynamic_feature_widget,\n",
    "    KV_widgets,\n",
    "    batch_size_widget, \n",
    "    max_input_len_widget,\n",
    "    min_output_len_widget,\n",
    "    max_output_len_widget, \n",
    "    encoder_hidden_size_widget,\n",
    "    num_beam_widget\n",
    "])\n",
    "\n",
    "display(widgets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077dd494-e8d8-42f9-bdbd-0362f1213118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference config\n",
    "FP16 = FP16_widget.value # flag to use FP16 precision in PyTorch & TRT\n",
    "disable_preview_dynamic_shapes = disable_preview_dynamic_feature_widget.value # flag to disable 8.5 EA feature\n",
    "HF_KV = HF_KV_widget.value # flag to use KV cache in HF\n",
    "TRT_KV = TRT_KV_widget.value # flag to use KV cache in TRT\n",
    "\n",
    "# Model config\n",
    "batch_size = batch_size_widget.value\n",
    "max_input_len = max_input_len_widget.value\n",
    "min_output_len = min_output_len_widget.value\n",
    "max_output_len = max_output_len_widget.value\n",
    "encoder_hidden_size = encoder_hidden_size_widget.value\n",
    "num_beams = num_beam_widget.value\n",
    "\n",
    "# Benchmark config\n",
    "# `TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration and number of warm-up calls, oercentiles, etc.\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5453eff5922a4d1f8a86bb4d74e54326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a36a253149499483996e09978ae771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85132f18f114ea8bd308653f273f231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mbart variant can't be recognized by HF AutoClass yet\n",
    "if \"mbart\" not in BART_VARIANT:    \n",
    "    bart_model = AutoModelForPreTraining.from_pretrained(BART_VARIANT) # BartForConditionalGeneration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BART_VARIANT) # BartTokenizer\n",
    "else:\n",
    "    from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "    bart_model = MBartForConditionalGeneration.from_pretrained(BART_VARIANT)\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(BART_VARIANT, src_lang=\"en_XX\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(BART_VARIANT)\n",
    "\n",
    "bart_model = bart_model.to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model saved to ./models/facebook/bart-base/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(BART_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "if os.path.exists(pytorch_model_dir) and len(os.listdir(pytorch_model_dir)) != 0:\n",
    "    print('PyTorch model already exists. Skipping...')\n",
    "else:\n",
    "    bart_model.save_pretrained(pytorch_model_dir)\n",
    "    print(\"PyTorch model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d1d6e-1cad-43a2-a8c3-4bc221070dc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd1d0d09-be28-42a3-9135-46b796e5be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sequence\n",
    "inputs = \"NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms. TensorRT, built on the NVIDIA CUDA parallel programming model, enables developers to optimize inference by leveraging libraries, development tools, and technologies in CUDA-X for AI, autonomous machines, high performance computing, and graphics. With new NVIDIA Ampere Architecture GPUs, TensorRT also uses sparse tensor cores for an additional performance boost.\"\n",
    "\n",
    "input_ids = tokenizer(inputs, padding=True, return_tensors=\"pt\").input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. PyTorch HuggingFace Inference\n",
    "\n",
    "Next, we will carry out inference with the HuggingFace PyTorch model as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d921-db47-4c45-bdcc-08ccc500ad99",
   "metadata": {},
   "source": [
    "### End-to-End HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10168132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "cuda_114_mode = \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]\n",
    "if cuda_114_mode:\n",
    "    bart_model = bart_model.cpu()\n",
    "    input_ids = input_ids.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d886e29a-1d1d-49e0-a351-3e4418f4bf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 17:47:08.756211: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "bart_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "if not cuda_114_mode:\n",
    "    bart_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab5c682-049a-48b3-830c-e1eecccbd553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 131\n",
      "NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms. TensorRT, built on the NVIDIA CUDA parallel programming model, enables developers to optimize inference by leveraging libraries, development tools, and technologies in CUDA-X for AI, autonomous machines, high performance computing, and graphics. With new NVIDIA Ampere Architecture GPUs, TensorRT also uses sparse tensor cores for an additional performance boost.\n",
      "\n",
      "\n",
      "Output length: 132\n",
      "NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms. TensorR, built on the NVIDIA CUDA parallel programming model, enables developers to accelerate inference by leveraging libraries, development tools, and technologies in CUDA-X for AI, autonomous machines, high performance computing, and graphics. With new NVIDIA Ampere Architecture GPUs, Tensor RT also uses sparse tensor cores for an additional performance boost.\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "HF time (no KV cache): p50 781.97ms, p99 788.99ms\n",
      "HF time (w/ KV cache): p50 563.78ms, p99 591.01ms\n",
      "Precision: FP16, Number of Beams: 1\n",
      "HF time (no KV cache): p50 789.69ms, p99 823.67ms\n",
      "HF time (w/ KV cache): p50 557.90ms, p99 562.12ms\n"
     ]
    }
   ],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Input length: {input_ids.size(1)}')\n",
    "print(inputs)\n",
    "print('\\n')      \n",
    "print(f'Output length: {output_ids[-1,:].size(0)}')\n",
    "print(outputs_hf)\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"HF time (no KV cache): {percentile_print(hf_nonkv_time)}\")\n",
    "print(f\"HF time (w/ KV cache): {percentile_print(hf_kv_time)}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"HF time (no KV cache): {percentile_print(hf_nonkv_time_fp16)}\")\n",
    "print(f\"HF time (w/ KV cache): {percentile_print(hf_kv_time_fp16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "For benchmarking purposes, we will employ helper functions `encoder_inference`, `decoder_inference`, and `full_inference_greedy` which execute the inference repeatedly for the BART encoder and decoder stacks separately as well as end-to-end for the entire output sequence, and measure the execution time. These execution times can be later on compared with TensorRT counterpart to demonstrate the speedup. \n",
    "\n",
    "Encoder and decoder of BART are wrapped as standalone PyTorch module for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c07516f-b02b-4722-b0bd-06b632259702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32\n",
    "bart_model.float()\n",
    "bart_torch_encoder = BARTEncoderTorchFile.TorchModule(bart_model.get_encoder())\n",
    "bart_torch_decoder = BARTDecoderTorchFile.TorchModule(bart_model.get_decoder(), bart_model.lm_head, bart_model.final_logits_bias, bart_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = encoder_inference(bart_torch_encoder, input_ids, timing_profile)\n",
    "    _, decoder_pytorch_time = decoder_inference(bart_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(bart_torch_encoder,bart_torch_decoder,input_ids,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(bart_torch_encoder,bart_torch_decoder,input_ids,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# FP16\n",
    "if not cuda_114_mode:\n",
    "    bart_model.half()\n",
    "else:\n",
    "    print(\"CUDA 11.4 is incompatible with current PyTorch version, using fp32 instead of fp16\")\n",
    "bart_torch_encoder_fp16 = BARTEncoderTorchFile.TorchModule(bart_model.get_encoder())\n",
    "bart_torch_decoder_fp16 = BARTDecoderTorchFile.TorchModule(bart_model.get_decoder(), bart_model.lm_head, bart_model.final_logits_bias, bart_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = encoder_inference(bart_torch_encoder_fp16, input_ids, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = decoder_inference(bart_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(bart_torch_encoder_fp16,bart_torch_decoder_fp16,input_ids,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(bart_torch_encoder_fp16,bart_torch_decoder_fp16,input_ids,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a103e3a6-920b-4c97-818e-6140654abc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 Output identical to HF results? True\n",
      "PyTorch FP16 Output identical to HF results? True\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "Encoder time: p50 2.74ms, p99 2.79ms\n",
      "Decoder time: p50 7.34ms, p99 7.42ms\n",
      "Full E2E time: p50 533.13ms, p99 541.75ms\n",
      "Precision: FP16, Number of Beams: 1\n",
      "Encoder time: p50 3.84ms, p99 3.87ms\n",
      "Decoder time: p50 5.45ms, p99 5.50ms\n",
      "Full E2E time: p50 532.67ms, p99 537.46ms\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {percentile_print(encoder_pytorch_time)}\")\n",
    "print(f\"Decoder time: {percentile_print(decoder_pytorch_time)}\")\n",
    "print(f\"Full E2E time: {percentile_print(full_pytorch_time)}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {percentile_print(encoder_pytorch_time_fp16)}\")\n",
    "print(f\"Decoder time: {percentile_print(decoder_pytorch_time_fp16)}\")\n",
    "print(f\"Full E2E time: {percentile_print(full_pytorch_time_fp16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. TensorRT Engine Building\n",
    "\n",
    "### Convert PyTorch to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT, apply optimizations and generate a TensorRT engine.\n",
    "- Perform inference on the GPU using the engine. \n",
    "\n",
    "For the BART model, we will convert the encoder and decoder to ONNX and build each engine seperately. The logistics of this separate building approach come from the nature of sequence-to-sequence models. BART and T5 are good examples of sequence-to-sequence models which use encoder-decoder architecture. The encoder is only executed once on the input and generates hidden states. Next, the decoder is executed repeatedly in an auto-regressive manner until the entire output finishes generating, i.e. the output sequence length is the number of times the decoder runs. The most efficient way to run encoder-decoder models with TensorRT is to have two separate engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea48be5-1dae-4e93-92a4-840d7017ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/jisu/triton/tensorrt/HuggingFace/BART/export.py:398: FutureWarning: 'torch.onnx._export' is deprecated in version 1.12.0 and will be removed in version 1.14. Please use `torch.onnx.export` instead.\n",
      "  torch.onnx._export(\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:913: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:96: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = './models/{}/onnx'.format(BART_VARIANT)\n",
    "!mkdir -p $onnx_model_path\n",
    "\n",
    "# FP32\n",
    "bart_model.float()\n",
    "metadata = NetworkMetadata(variant=BART_VARIANT, precision=Precision(fp16=False), other=BARTMetadata(kv_cache=TRT_KV))\n",
    "trt_config = BARTModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "bart_torchfile_encoder = BARTEncoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "bart_torchfile_decoder = BARTDecoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_bart_encoder = bart_torchfile_encoder.as_onnx_model(os.path.join(onnx_model_path, encoder_onnx_model_fpath), force_overwrite=False)\n",
    "onnx_bart_decoder = bart_torchfile_decoder.as_onnx_model(os.path.join(onnx_model_path, decoder_onnx_model_fpath), force_overwrite=False)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=BART_VARIANT, precision=Precision(fp16=True), other=BARTMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = BARTModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "bart_torchfile_encoder = BARTEncoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "bart_torchfile_decoder = BARTDecoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_bart_encoder_fp16 = bart_torchfile_encoder.as_onnx_model(os.path.join(onnx_model_path, encoder_onnx_model_fpath_fp16), force_overwrite=False)\n",
    "onnx_bart_decoder_fp16 = bart_torchfile_decoder.as_onnx_model(os.path.join(onnx_model_path, decoder_onnx_model_fpath_fp16), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "### Convert ONNX to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_ids', min=(1, 1), opt=(1, 384), max=(1, 768)).add('encoder_hidden_states', min=(1, 1, 768), opt=(1, 384, 768), max=(1, 768, 768))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorrt_model_path = './models/{}/tensorrt'.format(BART_VARIANT)\n",
    "!mkdir -p $tensorrt_model_path\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_input_len // 2),\n",
    "    max=(batch_size, max_input_len),\n",
    ")\n",
    "\n",
    "# Decoder optimization profiles\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_output_len // 2),\n",
    "    max=(batch_size * num_beams, max_output_len),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, encoder_hidden_size),\n",
    "    opt=(batch_size * num_beams, max_input_len // 2, encoder_hidden_size),\n",
    "    max=(batch_size * num_beams, max_input_len, encoder_hidden_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa5738ff-790e-47a0-ba03-27af87742646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\u001b[0m\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layernorm_embedding/ReduceMean': 'FLOAT', 'encoder/layernorm_embedding/Pow': 'FLOAT', 'encoder/layernorm_embedding/ReduceMean_1': 'FLOAT', 'encoder/layernorm_embedding/Add': 'FLOAT', 'encoder/layernorm_embedding/Sqrt': 'FLOAT', 'encoder/layernorm_embedding/Div': 'FLOAT', 'encoder/layernorm_embedding/Mul': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.4/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.4/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.4/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.4/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.4/final_layer_norm/Add': 'FLOAT', 'encoder/layers.4/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.4/final_layer_norm/Div': 'FLOAT', 'encoder/layers.4/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.5/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.5/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.5/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.5/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.5/final_layer_norm/Add': 'FLOAT', 'encoder/layers.5/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.5/final_layer_norm/Div': 'FLOAT', 'encoder/layers.5/final_layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;11m[W] Using kFASTER_DYNAMIC_SHAPES_0805 preview feature.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 539268086\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 539268086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] onnx2trt_utils.cpp:403: One or more weights outside the range of INT32 was clamped\u001b[0m\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'/decoder/Cast_2': 'FLOAT', '/decoder/Cast_3': 'FLOAT', '/decoder/layernorm_embedding/ReduceMean': 'FLOAT', '/decoder/layernorm_embedding/Pow': 'FLOAT', '/decoder/layernorm_embedding/ReduceMean_1': 'FLOAT', '/decoder/layernorm_embedding/Add': 'FLOAT', '/decoder/layernorm_embedding/Sqrt': 'FLOAT', '/decoder/layernorm_embedding/Div': 'FLOAT', '/decoder/layernorm_embedding/Mul': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/final_layer_norm/Add': 'FLOAT', '/decoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/final_layer_norm/Div': 'FLOAT', '/decoder/layers.0/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/final_layer_norm/Add': 'FLOAT', '/decoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/final_layer_norm/Div': 'FLOAT', '/decoder/layers.1/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/final_layer_norm/Add': 'FLOAT', '/decoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/final_layer_norm/Div': 'FLOAT', '/decoder/layers.2/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/final_layer_norm/Add': 'FLOAT', '/decoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/final_layer_norm/Div': 'FLOAT', '/decoder/layers.3/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.4/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.4/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.4/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.4/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.4/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.4/final_layer_norm/Add': 'FLOAT', '/decoder/layers.4/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.4/final_layer_norm/Div': 'FLOAT', '/decoder/layers.4/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.5/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.5/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.5/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.5/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.5/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.5/final_layer_norm/Add': 'FLOAT', '/decoder/layers.5/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.5/final_layer_norm/Div': 'FLOAT', '/decoder/layers.5/final_layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;11m[W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\u001b[0m\n",
      "\u001b[38;5;11m[W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\u001b[0m\n",
      "\u001b[38;5;11m[W] Check verbose logs for the list of affected weights.\u001b[0m\n",
      "\u001b[38;5;11m[W] - 76 weights are affected by this issue: Detected subnormal FP16 values.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 539268086\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 539268086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] - 122 weights are affected by this issue: Detected subnormal FP16 values.\u001b[0m\n",
      "\u001b[38;5;11m[W] - 1 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "encoder_engine_name = os.path.join(tensorrt_model_path, encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name = os.path.join(tensorrt_model_path, decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name):\n",
    "    bart_trt_encoder_engine = BARTEncoderONNXFile(os.path.join(onnx_model_path, encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_encoder_engine = BARTEncoderTRTEngine(encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(decoder_engine_name):\n",
    "    bart_trt_decoder_engine = BARTDecoderONNXFile(os.path.join(onnx_model_path, decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_decoder_engine = BARTDecoderTRTEngine(decoder_engine_name, metadata)\n",
    "\n",
    "# FP16\n",
    "encoder_engine_name_fp16 = os.path.join(tensorrt_model_path, encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name_fp16 = os.path.join(tensorrt_model_path, decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name_fp16):\n",
    "    bart_trt_encoder_engine_fp16 = BARTEncoderONNXFile(os.path.join(onnx_model_path, encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_encoder_engine_fp16 = BARTEncoderTRTEngine(encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(decoder_engine_name_fp16):\n",
    "    bart_trt_decoder_engine_fp16 = BARTDecoderONNXFile(os.path.join(onnx_model_path, decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_decoder_engine_fp16 = BARTDecoderTRTEngine(decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. TensorRT Inference\n",
    "\n",
    "Great, if you have reached this stage, it means we now have successfully built optimized TensorRT engines for the BART model, ready for us to carry out inference. The BART model with TensorRT backend can now be employed in place of the original HuggingFace BART model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3954f2f4-c393-463b-a44b-3e5335032b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(BART_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "bart_trt_encoder = BARTTRTEncoder(bart_trt_encoder_engine, metadata, trt_config, batch_size=batch_size)\n",
    "bart_trt_decoder = BARTTRTDecoder(bart_trt_decoder_engine, metadata, trt_config, batch_size=batch_size, num_beams=num_beams)\n",
    "\n",
    "# FP16\n",
    "bart_trt_encoder_fp16 = BARTTRTEncoder(bart_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size)\n",
    "bart_trt_decoder_fp16 = BARTTRTDecoder(bart_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7025246-4f14-4449-bb93-6c1566f48773",
   "metadata": {},
   "source": [
    "### End-to-End TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a5bbfe-a576-4a94-99d1-f0862b31fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = BARTModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = BARTModelTRTConfig.MIN_OUTPUT_LENGTH[BART_VARIANT]\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(no_repeat_ngram_size), \n",
    "    MinLengthLogitsProcessor(min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(tokenizer.convert_tokens_to_ids(tokenizer.bos_token)),\n",
    "    ForcedEOSTokenLogitsProcessor(max_output_len, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "]) # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can ensure identical results with raw HuggingFace\n",
    "\n",
    "decoder_initial_input = torch.full(\n",
    "    (batch_size, 1), tokenizer.convert_tokens_to_ids(tokenizer.eos_token), dtype=torch.int32\n",
    ").to('cuda')\n",
    "\n",
    "if num_beams > 1:\n",
    "    decoder_initial_input = expand_inputs_for_beam_search(decoder_initial_input, expand_size=num_beams)\n",
    "    \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = bart_trt_encoder(input_ids=input_ids)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        bart_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = bart_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = bart_trt_decoder.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)\n",
    "\n",
    "# FP16\n",
    "def e2e_trt_fp16():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = bart_trt_encoder_fp16(input_ids=input_ids)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "            \n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        bart_trt_decoder_fp16.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = bart_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = bart_trt_decoder_fp16.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids_fp16 = e2e_trt_fp16()\n",
    "outputs_trt_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)\n",
    "trt_time_fp16 = measure_python_inference_code(e2e_trt_fp16, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6198afcf-70d1-46ef-a515-dcf5ea4c17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Using engine: BART-base-bs1\n",
      "Output identical to HF results? True\n",
      "Precision: FP32\n",
      "TRT time: p50 300.42ms, p99 302.56ms\n",
      "\n",
      "Using engine: BART-base-fp16-bs1\n",
      "Output identical to HF results? True\n",
      "Precision: FP16\n",
      "TRT time: p50 231.99ms, p99 232.51ms\n"
     ]
    }
   ],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using engine: {metadata_string + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt == outputs_hf}')\n",
    "print(f\"Precision: FP32\")\n",
    "print(f'TRT time: {percentile_print(trt_time)}')\n",
    "print()\n",
    "print(f\"Using engine: {metadata_string_fp16 + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt_fp16 == outputs_hf}')\n",
    "print(f\"Precision: FP16\")\n",
    "print(f'TRT time: {percentile_print(trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4a98-b034-470e-a9f8-096d4100b8d4",
   "metadata": {},
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "We will benchmark the encoder, decoder, and full end-to-end as we did for HuggingFace before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2320e4bf-94f2-40d8-9a86-3a1ea352fca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder time: p50 1.14ms, p99 1.16ms\n",
      "Decoder time: p50 1.98ms, p99 1.99ms\n",
      "Full E2E time: p50 300.81ms, p99 306.73ms\n",
      "Encoder FP16 time: p50 0.79ms, p99 0.80ms\n",
      "Decoder FP16 time: p50 1.37ms, p99 1.38ms\n",
      "Full E2E FP16 time: p50 230.49ms, p99 233.23ms\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = encoder_inference(bart_trt_encoder, input_ids, timing_profile)\n",
    "_, decoder_trt_time = decoder_inference(bart_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        bart_trt_encoder,\n",
    "        bart_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        bart_trt_encoder,\n",
    "        bart_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n",
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = encoder_inference(bart_trt_encoder_fp16, input_ids, timing_profile)\n",
    "_, decoder_trt_time_fp16 = decoder_inference(bart_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        bart_trt_encoder_fp16,\n",
    "        bart_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        bart_trt_encoder_fp16,\n",
    "        bart_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cda12-7e56-4a87-935d-ce598557cf26",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1090b46c-adec-4684-8c53-a54a196dedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Framework               | Precision   | Encoder p50 (ms)   | Decoder p50 (ms)   |   Full E2E p50 (ms) | Accuracy   |\n",
      "|-------------------------|-------------|--------------------|--------------------|---------------------|------------|\n",
      "| HuggingFace (w/o cache) | FP32        | -                  | -                  |              781.97 | -          |\n",
      "| HuggingFace (w/ cache)  | FP32        | -                  | -                  |              563.78 | -          |\n",
      "| HuggingFace (w/o cache) | FP16        | -                  | -                  |              789.69 | -          |\n",
      "| HuggingFace (w/ cache)  | FP16        | -                  | -                  |              557.9  | -          |\n",
      "| PyTorch                 | FP32        | 2.74               | 7.34               |              533.13 | True       |\n",
      "| PyTorch                 | FP16        | 3.84               | 5.45               |              532.67 | True       |\n",
      "| TensorRT                | FP32        | 1.14               | 1.98               |              300.81 | True       |\n",
      "| TensorRT                | FP16        | 0.79               | 1.37               |              230.49 | True       |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['Framework', 'Precision', 'Encoder p50 (ms)', 'Decoder p50 (ms)', 'Full E2E p50 (ms)', 'Accuracy'],\n",
    "    ['HuggingFace (w/o cache)', 'FP32', '-', '-', f'{hf_nonkv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP32', '-', '-', f'{hf_kv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/o cache)', 'FP16', '-', '-', f'{hf_nonkv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP16', '-', '-', f'{hf_kv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['PyTorch', 'FP32', f'{encoder_pytorch_time[0]*1000:.2f}', f'{decoder_pytorch_time[0]*1000:.2f}', f'{full_pytorch_time[0]*1000:.2f}', outputs_pytorch == outputs_hf],\n",
    "    ['PyTorch', 'FP16', f'{encoder_pytorch_time_fp16[0]*1000:.2f}', f'{decoder_pytorch_time_fp16[0]*1000:.2f}', f'{full_pytorch_time_fp16[0]*1000:.2f}', outputs_pytorch_fp16 == outputs_hf],\n",
    "    ['TensorRT', 'FP32', f'{encoder_trt_time[0]*1000:.2f}', f'{decoder_trt_time[0]*1000:.2f}', f'{full_trt_time[0]*1000:.2f}', outputs_trt == outputs_hf],\n",
    "    ['TensorRT', 'FP16', f'{encoder_trt_time_fp16[0]*1000:.2f}', f'{decoder_trt_time_fp16[0]*1000:.2f}', f'{full_trt_time_fp16[0]*1000:.2f}', outputs_trt_fp16 == outputs_hf],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "We can now compare the original HuggingFace model and the TensorRT engine, from both separate encoder/decoder and end-to-end speed difference. For bart-base variant on an NVIDIA Titan V GPU and input/output sequence length around 130, this results in about 2x performance improvement with FP16 inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a498672-ba25-42b0-b89e-79e0b869943a",
   "metadata": {},
   "source": [
    "## Variable Input/Output Length\n",
    "\n",
    "We can run more tests by varying input/output length, while using the same engines.\n",
    "\n",
    "Note that TensorRT performance depends on optimal selection of the kernels in the engine. The variable length test here uses the same engine built with max input/output length profile, therefore may not represent the best perf. If the use case has known input/output length ranges, it is highly recommended to specify in the TensorRT engine profiles to ensure optimized kernel selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f25217-be31-45bf-8652-0e18162fa360",
   "metadata": {},
   "source": [
    "### Single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "985d8a01-e5b7-449e-9e43-7c8315a2578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| (input_len, output_len)   |   HF FP32 p50 (s) |   HF FP16 p50 (s) |   TRT FP32 p50 (s) |   TRT FP16 p50 (s) |\n",
      "|---------------------------|-------------------|-------------------|--------------------|--------------------|\n",
      "| (24, 24)                  |          0.112792 |          0.114877 |          0.0603943 |          0.0464584 |\n"
     ]
    }
   ],
   "source": [
    "# ensure HF model are on GPU for testing (cells above moved it CPU). For cuda 11.4, disable this block\n",
    "if not cuda_114_mode:\n",
    "    bart_model = bart_model.to('cuda').eval()\n",
    "\n",
    "    in_len, out_len = 24, 24\n",
    "\n",
    "    data = [\n",
    "        ['(input_len, output_len)', 'HF FP32 p50 (s)', 'HF FP16 p50 (s)', 'TRT FP32 p50 (s)', 'TRT FP16 p50 (s)'],\n",
    "    ]\n",
    "\n",
    "    assert in_len <= max_input_len and out_len <= max_output_len\n",
    "\n",
    "    in_ids = torch.randint(0, BARTModelTRTConfig.VOCAB_SIZE[BART_VARIANT], (batch_size, in_len)).to('cuda')\n",
    "\n",
    "    # HF\n",
    "    bart_model.float()\n",
    "    hf_32 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "    bart_model.half()\n",
    "    hf_16 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "    # TRT\n",
    "    if num_beams == 1:\n",
    "        _, trt_32 = full_inference_greedy(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "        _, trt_16 = full_inference_greedy(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "    else:\n",
    "        _, trt_32 = full_inference_beam(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "        _, trt_16 = full_inference_beam(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "\n",
    "    data.append([(in_len, out_len), hf_32[0], hf_16[0], trt_32[0], trt_16[0]])\n",
    "\n",
    "    print(tabulate(data, headers='firstrow', tablefmt='github'))\n",
    "else:\n",
    "    print(\"CUDA 11.4 is currently incompatible with GPU models, skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf4f5c-49a0-4509-a4d7-8b561dba3f88",
   "metadata": {},
   "source": [
    "### Several representative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e335010-ff7f-4822-85ae-bca8d235de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| (input_len, output_len)   |   HF FP32 p50 (s) |   HF FP16 p50 (s) |   TRT FP32 p50 (s) |   TRT FP16 p50 (s) |\n",
      "|---------------------------|-------------------|-------------------|--------------------|--------------------|\n",
      "| (64, 128)                 |          0.550142 |          0.558705 |          0.287292  |          0.213994  |\n",
      "| (64, 512)                 |          2.21455  |          2.25236  |          1.39557   |          1.04131   |\n",
      "| (512, 64)                 |          0.281083 |          0.287389 |          0.156625  |          0.112647  |\n",
      "| (128, 64)                 |          0.28082  |          0.285652 |          0.146351  |          0.108994  |\n",
      "| (32, 32)                  |          0.143932 |          0.148081 |          0.0770497 |          0.0586314 |\n",
      "| (128, 128)                |          0.549403 |          0.565054 |          0.290236  |          0.213973  |\n",
      "| (512, 512)                |          2.20643  |          2.26729  |          1.51874   |          1.10287   |\n"
     ]
    }
   ],
   "source": [
    "# ensure HF model are on GPU for testing (cells above moved it CPU). For cuda 11.4, disable this block\n",
    "if not cuda_114_mode:\n",
    "    bart_model = bart_model.to('cuda').eval()\n",
    "\n",
    "    input_output_len_list = [\n",
    "        (64, 128), # generation task\n",
    "        (64, 512),\n",
    "        (512, 64), # summarization task\n",
    "        (128, 64),\n",
    "        (32, 32), # translation task\n",
    "        (128, 128),\n",
    "        (512, 512),\n",
    "    ]\n",
    "\n",
    "    data = [\n",
    "        ['(input_len, output_len)', 'HF FP32 p50 (s)', 'HF FP16 p50 (s)', 'TRT FP32 p50 (s)', 'TRT FP16 p50 (s)'],\n",
    "    ]\n",
    "\n",
    "    for (in_len, out_len) in input_output_len_list:\n",
    "        assert in_len <= max_input_len and out_len <= max_output_len\n",
    "\n",
    "        in_ids = torch.randint(0, BARTModelTRTConfig.VOCAB_SIZE[BART_VARIANT], (batch_size, in_len)).to('cuda')\n",
    "\n",
    "        # HF\n",
    "        bart_model.float()\n",
    "        hf_32 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "        bart_model.half()\n",
    "        hf_16 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "        # TRT\n",
    "        if num_beams == 1:\n",
    "            _, trt_32 = full_inference_greedy(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "            _, trt_16 = full_inference_greedy(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "        else:\n",
    "            _, trt_32 = full_inference_beam(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "            _, trt_16 = full_inference_beam(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "\n",
    "        data.append([(in_len, out_len), hf_32[0], hf_16[0], trt_32[0], trt_16[0]])\n",
    "\n",
    "    print(tabulate(data, headers='firstrow', tablefmt='github'))\n",
    "else:\n",
    "    print(\"CUDA 11.4 is currently incompatible with GPU models, skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598a0ae-2e21-4898-ae56-8429a5d00760",
   "metadata": {},
   "source": [
    "It shows around 2x speedup comparing to HuggingFace's KV-cache optimized timing, for relatively short output sequence length. For long output sequence length, due to memory copies overhead between the decoding steps, TensorRT may not provide significant speedup at the current stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch BART model to an optimized TensorRT engine for inference in easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace BART model while providing speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [BART/trt.py](../BART/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcce967-a3f7-4d83-809e-98867d47697e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb819-73e3-4b49-a0d4-d12df4e48dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e58b3-3606-4cd5-98e2-61e5e03f4396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
