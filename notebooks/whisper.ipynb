{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace Whisper Inference with TensorRT\n",
    "\n",
    "Whisper is an encoder-decoder model that converts ASR problems into a speech-to-text format. More specifically, it does so by encoding speech in the input stream. This enables a single model to be trained supervised on a wide variety of Language\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch Whisper model](https://huggingface.co/transformers/model_doc/whisper.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace whisper model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f986826-8e46-47b3-87f7-227e6622e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51016f2c-8016-4937-9206-590369c0cb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36e6e9-8981-44d9-bff7-649c5d1c64b6",
   "metadata": {},
   "source": [
    "# Encoder output이 다름!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b6bf52b-9cc9-4b27-998b-b58ce4873b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "from typing import BinaryIO, Union\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "def decode_audio(\n",
    "    input_file: Union[str, BinaryIO],\n",
    "    sampling_rate: int = 16000,\n",
    "    split_stereo: bool = False,\n",
    "):\n",
    "    \"\"\"Decodes the audio.\n",
    "\n",
    "    Args:\n",
    "      input_file: Path to the input file or a file-like object.\n",
    "      sampling_rate: Resample the audio to this sample rate.\n",
    "      split_stereo: Return separate left and right channels.\n",
    "\n",
    "    Returns:\n",
    "      A float32 Numpy array.\n",
    "\n",
    "      If `split_stereo` is enabled, the function returns a 2-tuple with the\n",
    "      separated left and right channels.\n",
    "    \"\"\"\n",
    "    resampler = av.audio.resampler.AudioResampler(\n",
    "        format=\"s16\",\n",
    "        layout=\"mono\" if not split_stereo else \"stereo\",\n",
    "        rate=sampling_rate,\n",
    "    )\n",
    "\n",
    "    raw_buffer = io.BytesIO()\n",
    "    dtype = None\n",
    "\n",
    "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
    "        frames = container.decode(audio=0)\n",
    "        frames = _ignore_invalid_frames(frames)\n",
    "        frames = _group_frames(frames, 500000)\n",
    "        frames = _resample_frames(frames, resampler)\n",
    "\n",
    "        for frame in frames:\n",
    "            array = frame.to_ndarray()\n",
    "            dtype = array.dtype\n",
    "            raw_buffer.write(array)\n",
    "\n",
    "    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n",
    "\n",
    "    # Convert s16 back to f32.\n",
    "    audio = audio.astype(np.float32) / 32768.0\n",
    "\n",
    "    if split_stereo:\n",
    "        left_channel = audio[0::2]\n",
    "        right_channel = audio[1::2]\n",
    "        return left_channel, right_channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _ignore_invalid_frames(frames):\n",
    "    iterator = iter(frames)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except av.error.InvalidDataError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def _group_frames(frames, num_samples=None):\n",
    "    fifo = av.audio.fifo.AudioFifo()\n",
    "\n",
    "    for frame in frames:\n",
    "        frame.pts = None  # Ignore timestamp check.\n",
    "        fifo.write(frame)\n",
    "\n",
    "        if num_samples is not None and fifo.samples >= num_samples:\n",
    "            yield fifo.read()\n",
    "\n",
    "    if fifo.samples > 0:\n",
    "        yield fifo.read()\n",
    "\n",
    "\n",
    "def _resample_frames(frames, resampler):\n",
    "    # Add None to flush the resampler.\n",
    "    for frame in itertools.chain(frames, [None]):\n",
    "        yield from resampler.resample(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2be0a006-9ee6-45ab-aab1-170c631cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ca242c55-d48f-48e2-a8a3-868e79f3c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = whisper_model.get_encoder()(inputs['input_features'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "62026975-1a6f-4453-9053-391305da2f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a075b-b392-44fe-9fe9-0c9d95d006c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a3d05d2f-bddf-458b-92ef-9887829f16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = whisper_model._get_decoder_start_token_id(None, 50258)\n",
    "input_ids  = torch.ones((1, 1), dtype=torch.long, device='cuda') * decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1f8b0677-6858-49ec-a606-e7da874cf40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.get_decoder().max_source_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "19c39a60-76e6-48c1-b13d-df5851a19f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f35f3248-6abc-43fb-aff8-de662abd0ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 50264), (2, 50359), (3, 50363)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.config.forced_decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4f651bf2-8187-4cdc-98f5-9c9c4a6f3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "4b3769a5-3c7c-4085-95f8-915beba5c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.float().cuda()\n",
    "hf_out = whisper_model.generate(inputs['input_features'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "545e3d24-149e-4b91-9453-6ccf4ab76a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> The main reason for the Japanese-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean-speaking Korean Korean-speaking Korean Korean-speaking Korean Korean-speaking Korean Korean-speaking Korean Korean Korean-speaking Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean Korean']"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "35e698ad-dea2-440f-8239-40e61fefd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = whisper_model.model.decoder(input_ids=input_ids.cuda(), encoder_hidden_states=encoder_outputs['last_hidden_state'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0b297957-91d2-42e6-b8d0-6b79a9f9e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_output = whisper_model.proj_out(decoder_outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bc5bab17-d6ad-435f-86c0-2a231c737d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4892, -5.4267,  2.6655,  ...,  0.2883,  1.1339,  1.4177]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e747f8d-9d47-4ab2-9d98-8be9dfdddad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "97bc0735-d8d9-48e4-9a99-f27dd3cd9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7c359b1a-f549-467e-9abf-26fb4603296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(448)])\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(3),\n",
    "    MinLengthLogitsProcessor(0, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(50258),\n",
    "    ForcedEOSTokenLogitsProcessor(448, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "]) # by checking HuggingFace's generate() implementation carefully, the default logits processor for Whisper has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can ensure identical results with raw HuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4bc73f2f-26b7-4461-bacc-603230173b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs['last_hidden_state'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "51622bbb-a0eb-49dc-89a5-7ad0acffae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258]], device='cuda:0')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ee9b0b82-21be-417e-a496-2770d363ebe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_features'].cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2ccd5ffd-98df-48cf-bb8b-3da3651ddc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs['last_hidden_state'].cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ff1d3-a5ed-47d6-a66f-500f254a9193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9560db4-45c3-463a-b119-afc1017413b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b771df-15ad-4499-bdb4-6434cc17df28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c1b53f86-1f78-4946-a4fc-6d93cc6764f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # legacy: users may modify the model configuration to control generation -- update the generation config\n",
    "# # model attribute accordingly, if it was created from the model config\n",
    "# if self.generation_config._from_model_config:\n",
    "#     new_generation_config = GenerationConfig.from_model_config(self.config)\n",
    "#     if new_generation_config != self.generation_config:\n",
    "#         warnings.warn(\n",
    "#             \"You have modified the pretrained model configuration to control generation. This is a\"\n",
    "#             \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n",
    "#             \" Please use a generation configuration file (see\"\n",
    "#             \" https://huggingface.co/docs/transformers/main_classes/text_generation )\"\n",
    "#         )\n",
    "#         self.generation_config = new_generation_config\n",
    "# generation_config = self.generation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389c1d2-b900-4fcd-99db-81b6426a5469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3754b6fe-a3ef-4bec-a9bd-48d50eb829b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(3),\n",
    "    MinLengthLogitsProcessor(0, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedEOSTokenLogitsProcessor(448, 50364)\n",
    "])\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(448)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f7e3082e-0cc8-4fc2-9a98-4c9e391f619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6ed1e396-00ca-4efc-80d3-868f66a8d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f248da33-003f-4784-9c8f-e1d51d8090d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a422bf6a-63da-4014-8e67-c8f45f93df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = tokenizer.bos_token_id\n",
    "num_beams = whisper_model.config.num_beams\n",
    "length_penalty = whisper_model.config.length_penalty\n",
    "early_stopping = whisper_model.config.early_stopping\n",
    "num_beam_groups = whisper_model.config.num_beam_groups\n",
    "do_sample = whisper_model.config.do_sample\n",
    "num_return_sequences = whisper_model.config.num_return_sequences\n",
    "pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b74f4804-b95a-45d8-8c5b-3ff6681a12a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e596cc6a-030d-4fe0-a62f-c76db390eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tensor, model_input_name, model_kwargs = whisper_model._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4d7ed1f7-0384-41e6-a656-01a3be03772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 50264), (2, 50359), (3, 50363)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.config.forced_decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2e0fc-081a-40a3-8ca1-749ea3f451bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ca6847a5-6eb6-4b8c-a821-d1b520229190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs['encoder_outputs']=encoder_outputs.last_hidden_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "79969012-c641-445a-89a2-1f6962c835e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_repeat_ngram_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a3d06b77-b5ad-49f2-bbd8-77c5c7d3f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = whisper_model._get_logits_processor(\n",
    "    repetition_penalty = None,\n",
    "    no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "    encoder_no_repeat_ngram_size=None,\n",
    "    input_ids_seq_length=input_ids.shape[-1],\n",
    "    encoder_input_ids=inputs_tensor,\n",
    "    bad_words_ids=None,\n",
    "    min_length=min_length,\n",
    "    max_length=max_length,\n",
    "    eos_token_id=eos_token_id,\n",
    "    forced_bos_token_id=None,\n",
    "    forced_eos_token_id=None,\n",
    "    prefix_allowed_tokens_fn=None,\n",
    "    num_beams=1,\n",
    "    num_beam_groups=1,\n",
    "    diversity_penalty=None,\n",
    "    remove_invalid_values=None,\n",
    "    exponential_decay_length_penalty=None,\n",
    "    logits_processor=LogitsProcessorList(),\n",
    "    renormalize_logits=None,\n",
    "    suppress_tokens=None,\n",
    "    begin_suppress_tokens=None,\n",
    "    forced_decoder_ids=whisper_model.config.forced_decoder_ids,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e43cb749-41e4-4f8e-8a95-06dba42c8edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.generation_logits_process.NoRepeatNGramLogitsProcessor at 0x7f640513f3a0>,\n",
       " <transformers.generation_logits_process.SuppressTokensLogitsProcessor at 0x7f640513f400>,\n",
       " <transformers.generation_logits_process.SuppressTokensAtBeginLogitsProcessor at 0x7f640513f460>,\n",
       " <transformers.generation_logits_process.ForceTokensLogitsProcessor at 0x7f640513f4c0>]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "dbff6c86-ec79-4c88-bb34-539df2810b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.generation_logits_process.ForceTokensLogitsProcessor at 0x7f6407688fa0>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForceTokensLogitsProcessor (whisper_model.config.forced_decoder_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "64db451c-ca1f-46af-a878-5a4395734a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "9041a18d-b493-4bb1-aaa2-d9b3677e21ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.generation_logits_process.SuppressTokensAtBeginLogitsProcessor at 0x7f640522d220>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SuppressTokensAtBeginLogitsProcessor(whisper_model.config.begin_suppress_tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "6d7cff71-f6ff-4fdf-a081-57c9d429b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220, 50257]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.config.begin_suppress_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f9c1ccc0-46e3-4f56-b54e-83c311c44644",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'begin_suppress_tokens' and 'begin_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[338], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSuppressTokensAtBeginLogitsProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'begin_suppress_tokens' and 'begin_index'"
     ]
    }
   ],
   "source": [
    "SuppressTokensAtBeginLogitsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "f08d202b-abef-4ce4-b478-1a0e84d78153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258]], device='cuda:0')"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model._prepare_decoder_input_ids_for_generation(\n",
    "    batch_size,\n",
    "    decoder_start_token_id=decoder_start_token_id,\n",
    "    bos_token_id=bos_token_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "04a8cf97-22a0-47fc-ae01-9e8b8989aac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b0290-48e3-4053-bddb-8da920b9e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size,\n",
    "                decoder_start_token_id=decoder_start_token_id,\n",
    "                bos_token_id=bos_token_id,\n",
    "                model_kwargs=model_kwargs,\n",
    "                device=inputs_tensor.device,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f1be90ea-4cf3-42aa-8ae2-e45891c32cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_seq_length = input_ids.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b91e4e07-2cfc-45fe-add0-f3ddb33553c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_index = input_ids_seq_length\n",
    "begin_index = begin_index if (input_ids_seq_length > 1 or whisper_model.config.forced_bos_token_id is None) else begin_index + 1\n",
    "if whisper_model.config.forced_bos_token_id is not None:\n",
    "    begin_index += whisper_model.config.forced_bos_token_id[-1][0]  # generation starts after the last token that is forced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "9859c969-e0f2-4674-b5ac-9c2c2b76ca7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "fa2b1650-3c23-4387-83c5-42e10b162504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "de7195ed-b914-4938-a8af-539821c44f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(3),\n",
    "    SuppressTokensLogitsProcessor(whisper_model.config.suppress_tokens),\n",
    "    SuppressTokensAtBeginLogitsProcessor(whisper_model.config.begin_suppress_tokens, begin_index), \n",
    "    ForceTokensLogitsProcessor (whisper_model.config.forced_decoder_ids)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "66c48dd7-0b7d-4abb-b859-17fa504ec334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 679 ms, sys: 0 ns, total: 679 ms\n",
      "Wall time: 679 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# greedy_search(\n",
    "#     input_ids,\n",
    "#     logits_processor=logits_processor,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "#     pad_token_id=pad_token_id,\n",
    "#     eos_token_id=eos_token_id,\n",
    "#     output_scores=output_scores,\n",
    "#     return_dict_in_generate=return_dict_in_generate,\n",
    "#     synced_gpus=synced_gpus,\n",
    "#     **model_kwargs,\n",
    "# )\n",
    "decoder_output = whisper_model.greedy_search(\n",
    "    input_ids=input_ids.cuda(),\n",
    "    encoder_outputs=encoder_outputs.last_hidden_state.cuda(),\n",
    "   # stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    \n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e7683ae0-f5bb-4ad2-8154-a8aefec1f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|ko|><|transcribe|><|notimestamps|> 제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다. 하지만 이동 속도가 점점 늘여져 거의 정체안 모습입니다. 태풄은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다. 속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다. 새력도 크게 약화하지는 않을 전망입니다.<|endoftext|>']"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "622c3d7c-5f63-47a9-b4f8-19ed821501f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|ko|><|transcribe|><|notimestamps|> 제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다. 하지만 이동 속도가 점점 늘여져 거의 정체안 모습입니다. 태풍은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다. 속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다. 새력도 크게 약화하지는 않을 전망입니다.<|endoftext|>']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c3ad8-e39f-4725-9627-26ee23f8bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "faf90992-bb1c-4f34-8d90-8282d299d36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model._prepare_decoder_input_ids_for_generation(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/nvadmin/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:0')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "203ac7cc-ee7e-4def-9462-4e17ca6fae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2634d56-0118-4240-9fa5-331419bc4ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = whisper_model.generate(inputs=audio_inputs.input_features.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee02055-eb0e-4f02-9366-57b3936a492f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.1 ms, sys: 0 ns, total: 47.1 ms\n",
      "Wall time: 46.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003658406203612685"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_features = audio_inputs.input_features.to('cuda')\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "3324cf43-0a3f-4a48-8fc9-e0eeed63c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1, 1]]) * whisper_model.config.decoder_start_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf81153-0cb8-4e8a-bd51-5de35a2b061e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.4 ms, sys: 0 ns, total: 73.4 ms\n",
      "Wall time: 73 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005992966936901212"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = w_decoder_inference(\n",
    "    whisper_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "2fc88907-7d1f-4453-8863-5425f7ddc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "e5ac34d4-efd4-46b0-a142-397b7bbe6a63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_beams = 1\n",
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "3410dbdc-91a4-48c8-8acf-b13b51358689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.general_utils import measure_python_inference_code\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT).cuda()\n",
    "\n",
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = processor.tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for Whisper\n",
    "# if not cuda_114_mode:\n",
    "whisper_model= whisper_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fee3fe-8a52-44ff-a29f-e4bc02c47710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "3aa517a8-554d-4c8b-9455-e27146158749",
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "08a80940-4ff5-40d4-a82f-35a5cc1de908",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FP32\n",
    "HF_KV=True\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "whisper_model.float()\n",
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = w_encoder_inference(whisper_torch_encoder, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time = w_decoder_inference(whisper_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# # FP16\n",
    "# if not cuda_114_mode:\n",
    "whisper_model.half()\n",
    "input_features= input_features.half()\n",
    "whisper_torch_encoder_fp16 = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder_fp16 = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = w_encoder_inference(whisper_torch_encoder_fp16, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = w_decoder_inference(whisper_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "dda94dca-266c-45bb-ad56-9c1de5c03158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_pytorch_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6dad1-4dbe-488f-84b9-67a9b5a4ca6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "989f32e5-6ca9-4609-b5f7-d300a3919f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 Output identical to HF results? True\n",
      "PyTorch FP16 Output identical to HF results? True\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "Encoder time: [0.00220304518006742, 0.002242377959191799]\n",
      "Decoder time: [0.0034144290257245302, 0.0037977269385010004]\n",
      "Full E2E time: [0.08790272590704262, 0.2159281640779227]\n",
      "Precision: FP16, Number of Beams: 1\n",
      "Encoder time: [0.002915350953117013, 0.003663035109639168]\n",
      "Decoder time: [0.003403657115995884, 0.0034532209392637014]\n",
      "Full E2E time: [0.13136885385029018, 0.2274820499587804]\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time_fp16}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time_fp16}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "d128bd1f-a8b5-4fd4-9163-e38085877537",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "f3663c68-147d-4ead-b3ae-c9d4f2aba230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(output_ids_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105ef60-4141-4058-be58-0b2d268dadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the Whisper model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "8b1fd7ce-d39b-4142-9a05-647143518d6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "TRT_KV = False\n",
    "\n",
    "wh_onnx_model_path = './models/{}/onnx'.format(Whisper_VARIANT)\n",
    "!mkdir -p $wh_onnx_model_path\n",
    "\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config = WhisperModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "wh_encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=True)\n",
    "onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=True)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = WhisperModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "wh_encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder_fp16 = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), force_overwrite=True)\n",
    "onnx_whisper_decoder_fp16 = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), force_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "572b2d68-4004-4724-abd4-079c5487e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, 1500, max_sequence_length),\n",
    "    max=(batch_size * num_beams, 1500, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 3000),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n",
    "\n",
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "9514a333-83eb-4654-b74c-4586a91ec7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_write=True\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name) or force_write:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name) or force_write:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "efba83df-ffd5-4ea5-b74d-070d045d83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16\n",
    "wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name_fp16) or force_write:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name_fp16) or force_write:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf44b1-f951-4380-b2cb-b4b7188cacb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873ac2a-fbda-40c8-8646-f15eae2284a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "033c3627-d054-4bec-8e09-a6daf145021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper-tiny-encoder.onnx\n",
      "Whisper-tiny-decoder-with-lm-head.onnx\n",
      "<Whisper.export.WhisperEncoderONNXFile object at 0x7f6404e1e430>\n",
      "<Whisper.export.WhisperDecoderONNXFile object at 0x7f62378f3fa0>\n"
     ]
    }
   ],
   "source": [
    "print(wh_encoder_onnx_model_fpath)\n",
    "print(wh_decoder_onnx_model_fpath)\n",
    "print(onnx_whisper_encoder)\n",
    "print(onnx_whisper_decoder)\n",
    "#onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "#onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2422-c55d-4d02-85f8-4e2f659e0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5da0a58-fbc1-41ce-be96-358cdca01f9a",
   "metadata": {},
   "source": [
    "# Whisper Tensorrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "a0bfb51e-84cc-42cd-87ea-20a9195e4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder, TRTHFRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63591388-3a8d-416a-ae18-d6be56fe818e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "c11bc6fa-1441-4b11-b605-bbcfd40c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "whisper_trt_encoder = WhisperTRTEncoder(whisper_trt_encoder_engine, metadata, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder = WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size=batch_size, num_beams=num_beams)\n",
    "\n",
    "# FP16\n",
    "whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "9877bd13-1123-4a81-b0fe-5c7c3887038f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 ms, sys: 0 ns, total: 19.7 ms\n",
      "Wall time: 19.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003658406203612685"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_trt_time = w_encoder_inference(\n",
    "    whisper_trt_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088717-bcf6-470f-9c8b-1f9d0564e321",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a252c-75af-4d69-9833-84237f57ae51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91798bb7-12fc-4eb6-803b-31e747a83a0b",
   "metadata": {},
   "source": [
    "### End-to-End TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "3a214df2-cf28-444f-a68a-351aba8c9b5f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[578], line 83\u001b[0m\n\u001b[1;32m     72\u001b[0m             decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[1;32m     73\u001b[0m                 input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m     74\u001b[0m                 beam_scorer\u001b[38;5;241m=\u001b[39mbeam_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m             )\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output\n\u001b[0;32m---> 83\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43me2e_trt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m outputs_trt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m trt_time \u001b[38;5;241m=\u001b[39m measure_python_inference_code(e2e_trt, timing_profile)\n",
      "Cell \u001b[0;32mIn[578], line 63\u001b[0m, in \u001b[0;36me2e_trt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mset_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_beams \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_trt_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[1;32m     73\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m     74\u001b[0m         beam_scorer\u001b[38;5;241m=\u001b[39mbeam_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m         use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1785\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:432\u001b[0m, in \u001b[0;36mTRTNativeRunner.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_context\u001b[38;5;241m.\u001b[39mactive_optimization_profile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:661\u001b[0m, in \u001b[0;36mWhisperTRTDecoder.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     bindings[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdata_ptr()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    663\u001b[0m trt_context\u001b[38;5;241m.\u001b[39mset_binding_shape(\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# If encoder hidden states have not been copied yet, copy the hidden states to the input buffer.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[Whisper_VARIANT]\n",
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    WhisperModelTRTConfig.DECODER_START_TOKEN_ID,\n",
    ")\n",
    "\n",
    "forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\", no_timestamps=True)\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        NoRepeatNGramLogitsProcessor(no_repeat_ngram_size),\n",
    "        SuppressTokensLogitsProcessor(WhisperModelTRTConfig.SUPPRESS_TOKENS),\n",
    "        SuppressTokensAtBeginLogitsProcessor(\n",
    "            WhisperModelTRTConfig.BEGIN_SUPPRESS_TOKENS, decoder_input_ids.shape[-1]\n",
    "        ),\n",
    "        ForceTokensLogitsProcessor(forced_decoder_ids),\n",
    "    ]\n",
    ")  # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can get identical results with raw HuggingFace\n",
    "\n",
    "encoder_outputs.last_hidden_state = encoder_outputs.last_hidden_state.cpu()\n",
    "if num_beams > 1:\n",
    "    decoder_input_ids = expand_inputs_for_beam_search(decoder_input_ids, expand_size=num_beams)\n",
    "    \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                encoder_hidden_states=encoder_outputs.last_hidden_state.cuda(),\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder.beam_search(\n",
    "                input_ids=decoder_input_ids,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "d26c962a-e3a2-4c50-9e72-7d56b851de4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[576], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_trt_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1785\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:432\u001b[0m, in \u001b[0;36mTRTNativeRunner.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_context\u001b[38;5;241m.\u001b[39mactive_optimization_profile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:661\u001b[0m, in \u001b[0;36mWhisperTRTDecoder.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     bindings[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdata_ptr()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    663\u001b[0m trt_context\u001b[38;5;241m.\u001b[39mset_binding_shape(\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# If encoder hidden states have not been copied yet, copy the hidden states to the input buffer.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "decoder_output = whisper_trt_decoder.greedy_search(\n",
    "    input_ids=decoder_input_ids.cuda(),\n",
    "    encoder_hidden_states=encoder_outputs.last_hidden_state.cuda(),\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=metadata.other.kv_cache,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a445f-ac52-475f-8ea5-d9424d6dc2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "6ab6221e-8c28-462f-a983-76e223a5c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "e67f3294-64f3-4e3f-8f63-66ba3e1a105b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x7f640759e360>"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StoppingCriteriaList.max_length.getter(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae901f5-4357-4003-963d-5824fce026ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ba6ca-bdb4-4fcb-8806-62957af35951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.generation_beam_constraints import Constraint, DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation_beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation_logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.pytorch_utils import torch_int_div\n",
    "\n",
    "max_length = max_output_len\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "output_scores = whisper_model.config.output_scores\n",
    "output_attentions = whisper_model.config.output_attentions\n",
    "output_hidden_states = encoder_outputs.last_hidden_state\n",
    "return_dict_in_generate= whisper_model.config.return_dict_in_generate\n",
    "synced_gpus=False\n",
    "\n",
    "whisper_trt_decoder.prepare_inputs_for_generation\n",
    "\n",
    "model_kwargs = {\n",
    "    \"encoder_hidden_states\":whisper_trt_encoder(inputs['input_features']),\n",
    "    \"stopping_criteria\":stopping_criteria,\n",
    "    \"logits_processor\":logits_processor,\n",
    "    \"use_cache\":metadata.other.kv_cache,\n",
    "    \"use_cuda\":True\n",
    "}\n",
    "decoder_initial_input = torch.full(\n",
    "    (batch_size, 1), 50257, dtype=torch.int32\n",
    ").to('cuda')\n",
    "\n",
    "input_ids=decoder_initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d051e-f656-4a6c-84cd-14c7061f7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = whisper_trt_decoder.prepare_inputs_for_generation(input_ids, **model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375bd6c-faf6-45b6-b849-5a36075d3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = whisper_trt_decoder(\n",
    "    **model_inputs,\n",
    "    return_dict=True,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=whisper_trt_encoder(inputs['input_features']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ebe8f-63b9-4c9b-ac2b-ab9448c9c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dafd32-c768-4197-ba6f-088107a76ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80b6df-71cf-4958-9822-d66fe31a10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = outputs.logits[:, -1, :]\n",
    "next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "tokenizer.decode(next_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404acaff-e28f-45f7-8a63-903750e5c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256869ad-ba0d-4193-8611-05a2b956cb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa991ad-acc5-4b0c-a46a-3d502fe68f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "if max_length is not None:\n",
    "    warnings.warn(\n",
    "        \"`max_length` is deprecated in this function, use\"\n",
    "        \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "        UserWarning,\n",
    "    )\n",
    "    stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "output_hidden_states = (\n",
    "    output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    ")\n",
    "return_dict_in_generate = (\n",
    "    return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    ")\n",
    "\n",
    "# init attention / hidden states / scores tuples\n",
    "scores = () if (return_dict_in_generate and output_scores) else None\n",
    "decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "# if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "    encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "    encoder_hidden_states = (\n",
    "        model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "    )\n",
    "\n",
    "# keep track of which sequences are already finished\n",
    "unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "\n",
    "this_peer_finished = False  # used by synced_gpus only\n",
    "while True:\n",
    "    if synced_gpus:\n",
    "        # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "        # The following logic allows an early break if all peers finished generating their sequence\n",
    "        this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "        # send 0.0 if we finished, 1.0 otherwise\n",
    "        dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "        # did all peers finish? the reduced sum will be 0.0 then\n",
    "        if this_peer_finished_flag.item() == 0.0:\n",
    "            break\n",
    "\n",
    "    # prepare model inputs\n",
    "    model_inputs = whisper_trt_decoder.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "    # forward pass to get next token\n",
    "    outputs = whisper_trt_decoder(\n",
    "        **model_inputs,\n",
    "        return_dict=True,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "    )\n",
    "\n",
    "    if synced_gpus and this_peer_finished:\n",
    "        continue  # don't waste resources running the code we don't need\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    print(outputs)\n",
    "\n",
    "    # pre-process distribution\n",
    "    next_tokens_scores = logits_processor(input_ids, next_toke_logits)\n",
    "\n",
    "    # Store scores, attentions and hidden_states when required\n",
    "    if return_dict_in_generate:\n",
    "        if output_scores:\n",
    "            scores += (next_tokens_scores,)\n",
    "        if output_attentions:\n",
    "            decoder_attentions += (\n",
    "                (outputs.decoder_attentions,) if whisper_trt_decoder.config.is_encoder_decoder else (outputs.attentions,)\n",
    "            )\n",
    "            if whisper_trt_decoder.config.is_encoder_decoder:\n",
    "                cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            decoder_hidden_states += (\n",
    "                (outputs.decoder_hidden_states,)\n",
    "                if whisper_trt_decoder.config.is_encoder_decoder\n",
    "                else (outputs.hidden_states,)\n",
    "            )\n",
    "    print(1)\n",
    "\n",
    "    # argmax\n",
    "    next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "    print(tokenizer.decode(next_tokens))\n",
    "\n",
    "    # finished sentences should have their next token be a padding token\n",
    "    if eos_token_id is not None:\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "        next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "    print(2)\n",
    "\n",
    "    # update generated ids, model inputs, and length for next step\n",
    "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "    model_kwargs = whisper_trt_decoder._update_model_kwargs_for_generation(\n",
    "        outputs, model_kwargs, is_encoder_decoder=whisper_trt_decoder.config.is_encoder_decoder\n",
    "    )\n",
    "    print(3)\n",
    "\n",
    "    # if eos_token was found in one sentence, set sentence to finished\n",
    "    if eos_token_id is not None:\n",
    "        unfinished_sequences = unfinished_sequences.mul((next_tokens != eos_token_id).long())\n",
    "    print(4)\n",
    "\n",
    "    # stop when each sentence is finished, or if we exceed the maximum length\n",
    "    if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "        if not synced_gpus:\n",
    "            print(synced_gpus)\n",
    "            print(5)\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            this_peer_finished = True\n",
    "    #print(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5b351-1eb7-40b7-a7a8-e391dd3b51e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc55a6d-b6ed-42f5-87d4-c90977cb20a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d572c-ecd4-4f13-8ea9-ce62efd8cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564bd503-de3d-4e0a-b4f4-3e46302f3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70563a7-3652-4e26-9a27-83ff1099497c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49dfb6-44be-4a85-ba4e-5bc71830b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_decoder(\n",
    "    **model_inputs,\n",
    "    return_dict=True,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879165a-b81b-4edf-96af-df099bdbbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([50257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa89cb-1ee5-4145-ae9c-ff84ea5cc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584608a0-a74f-4733-839f-1e7cba830855",
   "metadata": {},
   "outputs": [],
   "source": [
    "(next_tokens != eos_token_id).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dcda3-197d-4a28-b1b6-ceff797edb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72621c7-035b-4a8f-951a-3f95ef8efecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "synced_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8a131-7b77-461d-b543-39735d13d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de273b6-de48-4a0e-b26b-84d76d316751",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_sequences.mul((next_tokens != eos_token_id).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c34caa-7c46-4edf-9748-8d1fc81741f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GreedySearchEncoderDecoderOutput(\n",
    "#     sequences=input_ids,\n",
    "#     scores=scores,\n",
    "#     encoder_attentions=encoder_attentions,\n",
    "#     encoder_hidden_states=encoder_hidden_states,\n",
    "#     decoder_attentions=decoder_attentions,\n",
    "#     cross_attentions=cross_attentions,\n",
    "#     decoder_hidden_states=decoder_hidden_states,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dccfcd-951d-4aa3-842a-22fc3950e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab4d0c-90c9-40b8-8bac-0bcf3f407628",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8ab48-936e-4343-a00d-744a55603205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if return_dict_in_generate:\n",
    "#     if whisper_trt_decoder.config.is_encoder_decoder:\n",
    "#         return GreedySearchEncoderDecoderOutput(\n",
    "#             sequences=input_ids,\n",
    "#             scores=scores,\n",
    "#             encoder_attentions=encoder_attentions,\n",
    "#             encoder_hidden_states=encoder_hidden_states,\n",
    "#             decoder_attentions=decoder_attentions,\n",
    "#             cross_attentions=cross_attentions,\n",
    "#             decoder_hidden_states=decoder_hidden_states,\n",
    "#         )\n",
    "#     else:\n",
    "#         return GreedySearchDecoderOnlyOutput(\n",
    "#             sequences=input_ids,\n",
    "#             scores=scores,\n",
    "#             attentions=decoder_attentions,\n",
    "#             hidden_states=decoder_hidden_states,\n",
    "#         )\n",
    "# else:\n",
    "#     return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd43325-787a-496c-9be9-dd8041e969cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaed903-6365-401b-b1f7-f66a63c7b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_outputs.last_hidden_state, expand_size=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684f25a-6e90-4f8b-9afd-66c6bdb5351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06f879-3a79-4e25-a94a-1074215d805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = whisper_trt_decoder.greedy_search(\n",
    "    input_ids=decoder_initial_input,\n",
    "    encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=metadata.other.kv_cache,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607c12a-be8e-4cca-9025-0e7e6e234cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b43dbf-2cd7-429f-87cb-22d0a7cb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e730a-3b50-4471-8b3d-a4152ceaeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b151248-242e-46af-b443-d5fb63102ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881e9d6-268e-47b4-bf2c-497a464a517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b9bb1-704f-421e-a034-2e820d0200c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77737291-81db-4d25-aee6-61aa41738f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ea322cb7-ca33-4811-86bb-4002e2e03534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16\n",
    "def e2e_trt_fp16():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder_fp16(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "            \n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder_fp16.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states) \n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder_fp16.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids_fp16 = e2e_trt_fp16()\n",
    "outputs_trt_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)\n",
    "trt_time_fp16 = measure_python_inference_code(e2e_trt_fp16, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b5d9dc90-4b4c-4916-8784-f3ed99249a1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_last_hidden_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder_fp16\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[1;32m      2\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_initial_input,\n\u001b[0;32m----> 3\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[43mencoder_last_hidden_states\u001b[49m,\n\u001b[1;32m      4\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m      5\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m      6\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mother\u001b[38;5;241m.\u001b[39mkv_cache,\n\u001b[1;32m      7\u001b[0m     use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_last_hidden_states' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_output = whisper_trt_decoder_fp16.greedy_search(\n",
    "    input_ids=decoder_initial_input,\n",
    "    encoder_hidden_states=encoder_last_hidden_states,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=metadata.other.kv_cache,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c520d11-286d-4d91-a1f6-bc5c1856f11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436034b-5752-4b2c-9eb1-58e5a499e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(decoder_output[0], skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544450ac-204b-4b2b-9a5b-dde269f23486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef464bf-23ed-4134-8a6f-3dd57d6c0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output_ids_fp16[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef06300-fac5-4c46-af14-573f6403b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trt_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eecfc972-3fde-4838-af73-af68f41bca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12557019293308258, 0.3864895810838789]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_time_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9362687-15d8-4eda-88f9-0831c6a5efb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b0dacea5-15c1-4fbc-b848-852ea84b0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Using engine: Whisper-tiny-bs1\n",
      "Output identical to HF results? True\n",
      "Precision: FP32\n",
      "TRT time: [0.036946029867976904, 0.0820648071821779]\n",
      "\n",
      "Using engine: Whisper-tiny-fp16-bs1\n",
      "Output identical to HF results? False\n",
      "Precision: FP16\n",
      "TRT time: [0.12557019293308258, 0.3864895810838789]\n"
     ]
    }
   ],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using engine: {metadata_string + '-' + engine_tag}\")\n",
    "print(f'Output identical to HF results? {outputs_trt == outputs_hf}')\n",
    "print(f\"Precision: FP32\")\n",
    "print(f'TRT time: {trt_time}')\n",
    "print()\n",
    "print(f\"Using engine: {metadata_string_fp16 + '-' + engine_tag}\")\n",
    "print(f'Output identical to HF results? {outputs_trt_fp16 == outputs_hf}')\n",
    "print(f\"Precision: FP16\")\n",
    "print(f'TRT time: {trt_time_fp16}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8b3e3bad-86c4-486e-aef3-4fb9da753d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 ms, sys: 489 µs, total: 18.2 ms\n",
      "Wall time: 17.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a8eef-445c-420c-a1a7-2a1348ea6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7db23658-fbcb-4cc8-831e-7434347b5a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0010103480890393257, 0.0036271950230002403]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_trt_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef28764-c754-4045-915a-14909b75a27b",
   "metadata": {},
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "We will benchmark the encoder, decoder, and full end-to-end as we did for HuggingFace before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49992cf-3737-4091-a300-b3bf806e2a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d0383bc9-f02e-467a-b5c2-092850dbb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder inference \n",
    "whisper_model.float()\n",
    "whisper_model = whisper_model.cuda()\n",
    "\n",
    "input_features = input_features.float().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "input_features = input_features.float().cuda()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bf0deb89-c258-419c-8a9d-5098b63188ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.9 ms, sys: 3.99 ms, total: 7.89 ms\n",
      "Wall time: 7.45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "decoder_outputs = whisper_model.model.decoder(input_ids=input_ids.cuda(), encoder_hidden_states=whisper_model.get_encoder()(input_features)['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "74b81143-af8d-4876-a005-6a860e525131",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_last_hidden_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_last_hidden_states' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, input_features, timing_profile)\n",
    "decoder_outputs2 = whisper_model.model.decoder(input_ids=input_ids.cuda(), encoder_hidden_states=encoder_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57cfc7-2b83-4825-b108-4c3736385977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aa8821fe-006f-4a10-a04b-0466464fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(encoder_last_hidden_states[0][0][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8f3b1409-b5fe-40b8-a247-927438985aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_last_hidden_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370d424-f719-4962-9938-1e7f55a6028a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6e9d06d2-2a0b-4f09-9c14-0249d92a46d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder time: p50 1.48ms, p99 1.49ms\n",
      "Decoder time: p50 0.99ms, p99 1.01ms\n",
      "Full E2E time: p50 144.67ms, p99 147.17ms\n",
      "Encoder FP16 time: p50 1.49ms, p99 1.49ms\n",
      "Decoder FP16 time: p50 0.87ms, p99 0.88ms\n",
      "Full E2E FP16 time: p50 110.37ms, p99 123.43ms\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, input_features, timing_profile)\n",
    "_, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n",
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = w_encoder_inference(whisper_trt_encoder_fp16, input_features, timing_profile)\n",
    "_, decoder_trt_time_fp16 = w_decoder_inference(whisper_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3dccd523-e56f-46fe-bdb8-b93730701958",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Framework               | Precision   | Encoder p50 (ms)   | Decoder p50 (ms)   |   Full E2E p50 (ms) | Accuracy   |\n",
      "|-------------------------|-------------|--------------------|--------------------|---------------------|------------|\n",
      "| HuggingFace (w/o cache) | FP32        | -                  | -                  |              100.22 | -          |\n",
      "| HuggingFace (w/ cache)  | FP32        | -                  | -                  |               73.68 | -          |\n",
      "| HuggingFace (w/o cache) | FP16        | -                  | -                  |               97.69 | -          |\n",
      "| HuggingFace (w/ cache)  | FP16        | -                  | -                  |               73    | -          |\n",
      "| PyTorch                 | FP32        | 2.19               | 3.22               |              390.05 | False      |\n",
      "| PyTorch                 | FP16        | 3.95               | 3.35               |              293.63 | False      |\n",
      "| TensorRT                | FP32        | 1.48               | 0.99               |              144.67 | True       |\n",
      "| TensorRT                | FP16        | 1.49               | 0.87               |              110.37 | False      |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['Framework', 'Precision', 'Encoder p50 (ms)', 'Decoder p50 (ms)', 'Full E2E p50 (ms)', 'Accuracy'],\n",
    "    ['HuggingFace (w/o cache)', 'FP32', '-', '-', f'{hf_nonkv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP32', '-', '-', f'{hf_kv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/o cache)', 'FP16', '-', '-', f'{hf_nonkv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP16', '-', '-', f'{hf_kv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['PyTorch', 'FP32', f'{encoder_pytorch_time[0]*1000:.2f}', f'{decoder_pytorch_time[0]*1000:.2f}', f'{full_pytorch_time[0]*1000:.2f}', outputs_pytorch == outputs_hf],\n",
    "    ['PyTorch', 'FP16', f'{encoder_pytorch_time_fp16[0]*1000:.2f}', f'{decoder_pytorch_time_fp16[0]*1000:.2f}', f'{full_pytorch_time_fp16[0]*1000:.2f}', outputs_pytorch_fp16 == outputs_hf],\n",
    "    ['TensorRT', 'FP32', f'{encoder_trt_time[0]*1000:.2f}', f'{decoder_trt_time[0]*1000:.2f}', f'{full_trt_time[0]*1000:.2f}', outputs_trt == outputs_hf],\n",
    "    ['TensorRT', 'FP16', f'{encoder_trt_time_fp16[0]*1000:.2f}', f'{decoder_trt_time_fp16[0]*1000:.2f}', f'{full_trt_time_fp16[0]*1000:.2f}', outputs_trt_fp16 == outputs_hf],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7f3af69e-29ac-4a9e-9d40-5848336d4311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281bf29-3ac8-4bdd-9c91-2e3f16396cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3fd5d-4c75-423a-9738-00cfb96152ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a8454-4226-4585-818a-2f25b416b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"tiny\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(\"korean_news.mp4\", beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7866b40-b3ef-43b8-8f7a-18cc82eb47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463539b-34f3-46b0-b48a-b49f5e25a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2497dd-c80f-421e-8557-ad7c4eacf9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f8f44-23fd-412f-ba5e-9638a9ab7b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e1dfd-d618-48e3-9262-db5bb4809ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "from typing import BinaryIO, Union\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "def decode_audio(\n",
    "    input_file: Union[str, BinaryIO],\n",
    "    sampling_rate: int = 16000,\n",
    "    split_stereo: bool = False,\n",
    "):\n",
    "    \"\"\"Decodes the audio.\n",
    "\n",
    "    Args:\n",
    "      input_file: Path to the input file or a file-like object.\n",
    "      sampling_rate: Resample the audio to this sample rate.\n",
    "      split_stereo: Return separate left and right channels.\n",
    "\n",
    "    Returns:\n",
    "      A float32 Numpy array.\n",
    "\n",
    "      If `split_stereo` is enabled, the function returns a 2-tuple with the\n",
    "      separated left and right channels.\n",
    "    \"\"\"\n",
    "    resampler = av.audio.resampler.AudioResampler(\n",
    "        format=\"s16\",\n",
    "        layout=\"mono\" if not split_stereo else \"stereo\",\n",
    "        rate=sampling_rate,\n",
    "    )\n",
    "\n",
    "    raw_buffer = io.BytesIO()\n",
    "    dtype = None\n",
    "\n",
    "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
    "        frames = container.decode(audio=0)\n",
    "        frames = _ignore_invalid_frames(frames)\n",
    "        frames = _group_frames(frames, 500000)\n",
    "        frames = _resample_frames(frames, resampler)\n",
    "\n",
    "        for frame in frames:\n",
    "            array = frame.to_ndarray()\n",
    "            dtype = array.dtype\n",
    "            raw_buffer.write(array)\n",
    "\n",
    "    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n",
    "\n",
    "    # Convert s16 back to f32.\n",
    "    audio = audio.astype(np.float32) / 32768.0\n",
    "\n",
    "    if split_stereo:\n",
    "        left_channel = audio[0::2]\n",
    "        right_channel = audio[1::2]\n",
    "        return left_channel, right_channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _ignore_invalid_frames(frames):\n",
    "    iterator = iter(frames)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except av.error.InvalidDataError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def _group_frames(frames, num_samples=None):\n",
    "    fifo = av.audio.fifo.AudioFifo()\n",
    "\n",
    "    for frame in frames:\n",
    "        frame.pts = None  # Ignore timestamp check.\n",
    "        fifo.write(frame)\n",
    "\n",
    "        if num_samples is not None and fifo.samples >= num_samples:\n",
    "            yield fifo.read()\n",
    "\n",
    "    if fifo.samples > 0:\n",
    "        yield fifo.read()\n",
    "\n",
    "\n",
    "def _resample_frames(frames, resampler):\n",
    "    # Add None to flush the resampler.\n",
    "    for frame in itertools.chain(frames, [None]):\n",
    "        yield from resampler.resample(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee18989-bb79-492f-8ad8-f493bef4d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe97ce-dd53-4132-a4f5-e1d590271eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import feature_extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b191b3c-306c-420b-99be-89f16d2ce3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03007ec5-ac3e-44ab-8eec-e0827a2f7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, inputs['input_features'], timing_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fc4ea-1969-4db8-be65-b7def75424a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af443c-a4db-4860-97e7-a0c478a06d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = whisper_model.model.encoder(inputs['input_features'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ecfa08-907d-41a1-b590-69bb67124c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b250796-79cd-485d-89d4-ce382379d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_de = whisper_model.model.decoder(input_ids.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7103f6-09e1-47e7-8d7d-a5202b4c78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_de[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c0797-f9f0-424f-9340-1a30995919aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ead8e-9857-4d44-82af-78bbf9df0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065aa77e-3e2d-40a7-9e9d-e339ede84a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.other.kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18ebbb-ed2b-4c0c-a722-47d483113025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.tensorrt_utils import TRTNativeRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979359f5-e75c-4218-994c-caf29a06621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        NoRepeatNGramLogitsProcessor(no_repeat_ngram_size),\n",
    "        MinLengthLogitsProcessor(\n",
    "            min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        ),\n",
    "        ForcedBOSTokenLogitsProcessor(\n",
    "            50258\n",
    "        ),\n",
    "        ForcedEOSTokenLogitsProcessor(\n",
    "            max_output_len, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        ),\n",
    "    ]\n",
    ")  # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can get identical results with raw HuggingFace\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    tokenizer.convert_tokens_to_ids(tokenizer.eos_token),\n",
    "    dtype=torch.int32,\n",
    ")\n",
    "\n",
    "if False:\n",
    "    decoder_input_ids = decoder_input_ids.to(\"cuda\")\n",
    "else:\n",
    "    decoder_input_ids = decoder_input_ids.to(\"cpu\")\n",
    "\n",
    "def _e2e():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "        decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbb0f6-a16f-4c00-9026-94ddeb47f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _e2e_trt():\n",
    "        with torch.no_grad():\n",
    "            encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "            whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(\n",
    "                encoder_last_hidden_state\n",
    "            )\n",
    "            decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_last_hidden_state,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=False,\n",
    "            )\n",
    "        return decoder_output_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34647f96-2548-4010-980e-cb453d6bdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_function = _e2e\n",
    "if isinstance(whisper_trt_decoder, TRTNativeRunner):\n",
    "    whisper_trt_decoder.set_return_device(\"cuda\" if False else \"cpu\")\n",
    "    measurement_function = _e2e_trt\n",
    "\n",
    "full_e2e_time = measure_python_inference_code(measurement_function, timing_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39cf07-a094-471b-874f-a16aa1cc8502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995d4c8-03a2-4b7f-8cc7-f8a099d19155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return (measurement_function(), full_e2e_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41721f12-2df3-4126-b812-92366ebd97bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be521c-7342-4c90-b2d3-2e4fd7337019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25e391-8d09-4fab-8776-60bf6f206603",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model._get_decoder_start_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa3766-3ac4-47fc-ae18-8fc1af22f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e1bd3-a77f-46d3-94c6-a8abd65edb57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
