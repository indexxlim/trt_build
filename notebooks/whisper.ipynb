{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace Whisper Inference with TensorRT\n",
    "\n",
    "Whisper is an encoder-decoder model that converts ASR problems into a speech-to-text format. More specifically, it does so by encoding speech in the input stream. This enables a single model to be trained supervised on a wide variety of Language\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch Whisper model](https://huggingface.co/transformers/model_doc/whisper.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace whisper model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f986826-8e46-47b3-87f7-227e6622e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51016f2c-8016-4937-9206-590369c0cb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36e6e9-8981-44d9-bff7-649c5d1c64b6",
   "metadata": {},
   "source": [
    "# Encoder output이 다름!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b6bf52b-9cc9-4b27-998b-b58ce4873b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "from typing import BinaryIO, Union\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "def decode_audio(\n",
    "    input_file: Union[str, BinaryIO],\n",
    "    sampling_rate: int = 16000,\n",
    "    split_stereo: bool = False,\n",
    "):\n",
    "    \"\"\"Decodes the audio.\n",
    "\n",
    "    Args:\n",
    "      input_file: Path to the input file or a file-like object.\n",
    "      sampling_rate: Resample the audio to this sample rate.\n",
    "      split_stereo: Return separate left and right channels.\n",
    "\n",
    "    Returns:\n",
    "      A float32 Numpy array.\n",
    "\n",
    "      If `split_stereo` is enabled, the function returns a 2-tuple with the\n",
    "      separated left and right channels.\n",
    "    \"\"\"\n",
    "    resampler = av.audio.resampler.AudioResampler(\n",
    "        format=\"s16\",\n",
    "        layout=\"mono\" if not split_stereo else \"stereo\",\n",
    "        rate=sampling_rate,\n",
    "    )\n",
    "\n",
    "    raw_buffer = io.BytesIO()\n",
    "    dtype = None\n",
    "\n",
    "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
    "        frames = container.decode(audio=0)\n",
    "        frames = _ignore_invalid_frames(frames)\n",
    "        frames = _group_frames(frames, 500000)\n",
    "        frames = _resample_frames(frames, resampler)\n",
    "\n",
    "        for frame in frames:\n",
    "            array = frame.to_ndarray()\n",
    "            dtype = array.dtype\n",
    "            raw_buffer.write(array)\n",
    "\n",
    "    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n",
    "\n",
    "    # Convert s16 back to f32.\n",
    "    audio = audio.astype(np.float32) / 32768.0\n",
    "\n",
    "    if split_stereo:\n",
    "        left_channel = audio[0::2]\n",
    "        right_channel = audio[1::2]\n",
    "        return left_channel, right_channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _ignore_invalid_frames(frames):\n",
    "    iterator = iter(frames)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except av.error.InvalidDataError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def _group_frames(frames, num_samples=None):\n",
    "    fifo = av.audio.fifo.AudioFifo()\n",
    "\n",
    "    for frame in frames:\n",
    "        frame.pts = None  # Ignore timestamp check.\n",
    "        fifo.write(frame)\n",
    "\n",
    "        if num_samples is not None and fifo.samples >= num_samples:\n",
    "            yield fifo.read()\n",
    "\n",
    "    if fifo.samples > 0:\n",
    "        yield fifo.read()\n",
    "\n",
    "\n",
    "def _resample_frames(frames, resampler):\n",
    "    # Add None to flush the resampler.\n",
    "    for frame in itertools.chain(frames, [None]):\n",
    "        yield from resampler.resample(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be0a006-9ee6-45ab-aab1-170c631cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca242c55-d48f-48e2-a8a3-868e79f3c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = whisper_model.get_encoder()(inputs['input_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d05d2f-bddf-458b-92ef-9887829f16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = whisper_model._get_decoder_start_token_id(None, 50258)\n",
    "input_ids  = torch.ones((1, 1), dtype=torch.long, device='cuda') * decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f8b0677-6858-49ec-a606-e7da874cf40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.get_decoder().max_source_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c39a60-76e6-48c1-b13d-df5851a19f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3769a5-3c7c-4085-95f8-915beba5c02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jisu/miniconda/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "whisper_model.float().cuda(1)\n",
    "hf_out = whisper_model.generate(inputs['input_features'].cuda(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c0a3b94-5d1d-42c6-b6c1-ea1130a3b053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|ko|><|transcribe|><|notimestamps|> 제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다. 하지만 이동 속도가 점점 늘여져 거의 정체안 모습입니다. 태풍은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다. 속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다. 새력도 크게 약화하지는 않을 전망입니다.<|endoftext|>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35e698ad-dea2-440f-8239-40e61fefd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = whisper_model.model.decoder(input_ids=input_ids.cuda(1), encoder_hidden_states=encoder_outputs['last_hidden_state'].cuda(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b297957-91d2-42e6-b8d0-6b79a9f9e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_output = whisper_model.proj_out(decoder_outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bab17-d6ad-435f-86c0-2a231c737d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e747f8d-9d47-4ab2-9d98-8be9dfdddad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b771df-15ad-4499-bdb4-6434cc17df28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b53f86-1f78-4946-a4fc-6d93cc6764f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # legacy: users may modify the model configuration to control generation -- update the generation config\n",
    "# # model attribute accordingly, if it was created from the model config\n",
    "# if self.generation_config._from_model_config:\n",
    "#     new_generation_config = GenerationConfig.from_model_config(self.config)\n",
    "#     if new_generation_config != self.generation_config:\n",
    "#         warnings.warn(\n",
    "#             \"You have modified the pretrained model configuration to control generation. This is a\"\n",
    "#             \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n",
    "#             \" Please use a generation configuration file (see\"\n",
    "#             \" https://huggingface.co/docs/transformers/main_classes/text_generation )\"\n",
    "#         )\n",
    "#         self.generation_config = new_generation_config\n",
    "# generation_config = self.generation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c389c1d2-b900-4fcd-99db-81b6426a5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    ")\n",
    "\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed1e396-00ca-4efc-80d3-868f66a8d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a422bf6a-63da-4014-8e67-c8f45f93df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = tokenizer.bos_token_id\n",
    "num_beams = whisper_model.config.num_beams\n",
    "length_penalty = whisper_model.config.length_penalty\n",
    "early_stopping = whisper_model.config.early_stopping\n",
    "num_beam_groups = whisper_model.config.num_beam_groups\n",
    "do_sample = whisper_model.config.do_sample\n",
    "num_return_sequences = whisper_model.config.num_return_sequences\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f4804-b95a-45d8-8c5b-3ff6681a12a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e596cc6a-030d-4fe0-a62f-c76db390eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs_tensor, model_input_name, model_kwargs = whisper_model._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b08146-5419-4357-8965-1f76d54489bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = whisper_model._get_stopping_criteria(\n",
    "    max_length=whisper_model.config.max_length, max_time=None, stopping_criteria=StoppingCriteriaList()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f2c7b33-1d17-451b-9fb6-5df1d189ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_seq_length = input_ids.shape[-1]\n",
    "\n",
    "\n",
    "begin_index = input_ids_seq_length\n",
    "begin_index = begin_index if (input_ids_seq_length > 1 or whisper_model.config.forced_bos_token_id is None) else begin_index + 1\n",
    "if whisper_model.config.forced_bos_token_id is not None:\n",
    "    begin_index += whisper_model.config.forced_bos_token_id[-1][0]  # generation starts after the last token that is forced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de7195ed-b914-4938-a8af-539821c44f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = LogitsProcessorList([\n",
    "    SuppressTokensLogitsProcessor(whisper_model.config.suppress_tokens),\n",
    "    SuppressTokensAtBeginLogitsProcessor(whisper_model.config.begin_suppress_tokens, begin_index), \n",
    "    ForceTokensLogitsProcessor (processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c48dd7-0b7d-4abb-b859-17fa504ec334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 40.3 ms, total: 1.89 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# greedy_search(\n",
    "#     input_ids,\n",
    "#     logits_processor=logits_processor,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "#     pad_token_id=pad_token_id,\n",
    "#     eos_token_id=eos_token_id,\n",
    "#     output_scores=output_scores,\n",
    "#     return_dict_in_generate=return_dict_in_generate,\n",
    "#     synced_gpus=synced_gpus,\n",
    "#     **model_kwargs,\n",
    "# )\n",
    "decoder_output = whisper_model.greedy_search(\n",
    "    input_ids=input_ids.cuda(1),\n",
    "    encoder_outputs=encoder_outputs.last_hidden_state.cuda(1),\n",
    "   # stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7683ae0-f5bb-4ad2-8154-a8aefec1f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|ko|><|transcribe|><|notimestamps|> 제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다. 하지만 이동 속도가 점점 늘여져 거의 정체안 모습입니다. 태풍은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다. 속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다. 새력도 크게 약화하지는 않을 전망입니다.<|endoftext|>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622c3d7c-5f63-47a9-b4f8-19ed821501f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|ko|><|transcribe|><|notimestamps|> 제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다. 하지만 이동 속도가 점점 늘여져 거의 정체안 모습입니다. 태풍은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다. 속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다. 새력도 크게 약화하지는 않을 전망입니다.<|endoftext|>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c3ad8-e39f-4725-9627-26ee23f8bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "# input_features = audio_inputs.input_features\n",
    "\n",
    "# # WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "# if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "#     whisper_model = whisper_model.cpu()\n",
    "#     input_features = input_features.to('cpu')\n",
    "# else:\n",
    "#     whisper_model = whisper_model.cuda()\n",
    "#     input_features = input_features.to('cuda:1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d194b4bd-5137-49a9-b351-c56caf2266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = inputs['input_features'].cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "203ac7cc-ee7e-4def-9462-4e17ca6fae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다. 하지만 이동 속도가 점점 늘여져 거의 정체안 모습입니다. 태풍은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다. 속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다. 새력도 크게 약화하지는 않을 전망입니다.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.cuda(1)\n",
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2634d56-0118-4240-9fa5-331419bc4ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = whisper_model.generate(inputs=input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.22 s, sys: 337 ms, total: 4.56 s\n",
      "Wall time: 4.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0027240449999226257"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_features = input_features\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3324cf43-0a3f-4a48-8fc9-e0eeed63c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1, 1]]) * whisper_model.config.decoder_start_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 ms, sys: 16.1 ms, total: 288 ms\n",
      "Wall time: 287 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.006112638002377935"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "decoder_output, decoder_e2e_median_time = w_decoder_inference(\n",
    "    whisper_torch_decoder, input_ids.cuda(), encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fc88907-7d1f-4453-8863-5425f7ddc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5ac34d4-efd4-46b0-a142-397b7bbe6a63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_beams = 1\n",
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length\n",
    "tokenizer = processor.tokenizer\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)\n",
    "whisper_model.config.forced_decoder_ids = forced_decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410dbdc-91a4-48c8-8acf-b13b51358689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.general_utils import measure_python_inference_code\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT).cuda(1)\n",
    "\n",
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)\n",
    "    outputs = processor.tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)\n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for Whisper\n",
    "# if not cuda_114_mode:\n",
    "whisper_model= whisper_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a47fd8-b9d7-422f-bbd3-d35a8c8bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fee3fe-8a52-44ff-a29f-e4bc02c47710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa517a8-554d-4c8b-9455-e27146158749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a80940-4ff5-40d4-a82f-35a5cc1de908",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FP32\n",
    "HF_KV=True\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "whisper_model.float()\n",
    "input_features =input_features.float()\n",
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = w_encoder_inference(whisper_torch_encoder, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time = w_decoder_inference(whisper_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# # FP16\n",
    "# if not cuda_114_mode:\n",
    "whisper_model.half()\n",
    "input_features= input_features.half()\n",
    "whisper_torch_encoder_fp16 = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder_fp16 = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = w_encoder_inference(whisper_torch_encoder_fp16, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = w_decoder_inference(whisper_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV, forced_decoder_ids=forced_decoder_ids)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556eda2-3a32-4ec7-8620-10057104211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ab379-08c6-43b5-a15e-597723e46558",
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcf316-01a7-44e2-b18b-1fde97aab990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (1, 1),\n",
    "    whisper_torch_decoder.config.decoder_start_token_id,\n",
    "    dtype=torch.int32,\n",
    ")\n",
    "if forced_decoder_ids is None:\n",
    "    forced_decoder_ids = whisper_torch_decoder.config.forced_decoder_ids\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(224)])\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        SuppressTokensLogitsProcessor(whisper_torch_decoder.config.suppress_tokens),\n",
    "        SuppressTokensAtBeginLogitsProcessor(\n",
    "            whisper_torch_decoder.config.begin_suppress_tokens,\n",
    "            decoder_input_ids.shape[-1],\n",
    "        ),\n",
    "        ForceTokensLogitsProcessor(forced_decoder_ids),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "decoder_input_ids = decoder_input_ids.to(\"cuda\")\n",
    "\n",
    "def _e2e():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_state = whisper_torch_encoder(input_features=input_features)\n",
    "        decoder_output_greedy = whisper_torch_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "    return decoder_output_greedy\n",
    "\n",
    "# With e2e we can opt to bind inputs only once for hidden states for optimization\n",
    "def _e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_state = whisper_torch_encoder(input_features=input_features)\n",
    "        whisper_torch_decoder.set_encoder_hidden_states_for_inference_cycle(\n",
    "            encoder_last_hidden_state\n",
    "        )\n",
    "        decoder_output_greedy = whisper_torch_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "    return decoder_output_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d8aae-f3c1-4d0b-b92a-20c1f2354513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c31278-4a4b-4cf1-b108-95f96a7f2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7efd46-49ce-4a58-8888-189245393278",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ac932-4a10-4cd4-b850-69ce303c7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15190573-4eb7-4fd1-8806-71cf4e973168",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f32e5-6ca9-4609-b5f7-d300a3919f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time_fp16}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time_fp16}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time_fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the Whisper model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1fd7ce-d39b-4142-9a05-647143518d6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "TRT_KV = False\n",
    "\n",
    "wh_onnx_model_path = './models/{}/onnx'.format(Whisper_VARIANT)\n",
    "!mkdir -p $wh_onnx_model_path\n",
    "\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config = WhisperModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "wh_encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=True)\n",
    "onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=True)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = WhisperModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config_fp16.get_metadata_string(metadata_fp16)\n",
    "\n",
    "wh_encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder_fp16 = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), force_overwrite=True)\n",
    "onnx_whisper_decoder_fp16 = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), force_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_size = whisper_model.config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb269a5-9753-4c1a-9f4a-036552ff643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b2d68-4004-4724-abd4-079c5487e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, encoder_hidden_size),\n",
    "    opt=(batch_size * num_beams, 1500, encoder_hidden_size),\n",
    "    max=(batch_size * num_beams, 1500, encoder_hidden_size),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 3000),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n",
    "\n",
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514a333-83eb-4654-b74c-4586a91ec7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_write=False\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name) or force_write:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name, \n",
    "        force_overwrite=force_write,\n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name) or force_write:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name, \n",
    "        force_overwrite=force_write,\n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba83df-ffd5-4ea5-b74d-070d045d83ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layer_norm/ReduceMean': 'FLOAT', 'encoder/layer_norm/Pow': 'FLOAT', 'encoder/layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layer_norm/Add': 'FLOAT', 'encoder/layer_norm/Sqrt': 'FLOAT', 'encoder/layer_norm/Div': 'FLOAT', 'encoder/layer_norm/Mul': 'FLOAT'}\n",
      "[W] Detected layernorm nodes in FP16: /decoder/layers.0/self_attn_layer_norm/Sub, , , , , , /decoder/layers.0/self_attn_layer_norm/Add_1, /decoder/layers.0/encoder_attn_layer_norm/Sub, , , , , , , /decoder/layers.0/encoder_attn_layer_norm/Add_1, /decoder/layers.0/final_layer_norm/Sub, , , , , , , /decoder/layers.0/final_layer_norm/Add_1, /decoder/layers.1/self_attn_layer_norm/Sub, , , , , , , /decoder/layers.1/self_attn_layer_norm/Add_1, /decoder/layers.1/encoder_attn_layer_norm/Sub, , , , , , , /decoder/layers.1/encoder_attn_layer_norm/Add_1, /decoder/layers.1/final_layer_norm/Sub, , , , , , , /decoder/layers.1/final_layer_norm/Add_1, /decoder/layers.2/self_attn_layer_norm/Sub, , , , , , , /decoder/layers.2/self_attn_layer_norm/Add_1, /decoder/layers.2/encoder_attn_layer_norm/Sub, , , , , , , /decoder/layers.2/encoder_attn_layer_norm/Add_1, /decoder/layers.2/final_layer_norm/Sub, , , , , , , /decoder/layers.2/final_layer_norm/Add_1, /decoder/layers.3/self_attn_layer_norm/Sub, , , , , , , /decoder/layers.3/self_attn_layer_norm/Add_1, /decoder/layers.3/encoder_attn_layer_norm/Sub, , , , , , , /decoder/layers.3/encoder_attn_layer_norm/Add_1, /decoder/layers.3/final_layer_norm/Sub, , , , , , , /decoder/layers.3/final_layer_norm/Add_1, /decoder/layer_norm/Sub, , , , , , , /decoder/layer_norm/Add_1, \n",
      "[W] Running layernorm after self-attention in FP16 may cause overflow. Exporting the model to the latest available ONNX opset (later than opset 17) to use the INormalizationLayer, or forcing layernorm layers to run in FP32 precision can help with preserving accuracy.\n"
     ]
    }
   ],
   "source": [
    "# FP16\n",
    "wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name_fp16) or force_write:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_encoder_engine_name_fp16, \n",
    "        force_overwrite=force_write,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name_fp16) or force_write:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_decoder_engine_name_fp16, \n",
    "        force_overwrite=force_write,\n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3159336-afc4-4446-8652-f6db14ca3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_encoder_engine_name_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c3627-d054-4bec-8e09-a6daf145021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wh_encoder_onnx_model_fpath)\n",
    "print(wh_decoder_onnx_model_fpath)\n",
    "print(onnx_whisper_encoder)\n",
    "print(onnx_whisper_decoder)\n",
    "#onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "#onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2422-c55d-4d02-85f8-4e2f659e0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5da0a58-fbc1-41ce-be96-358cdca01f9a",
   "metadata": {},
   "source": [
    "# Whisper Tensorrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfb51e-84cc-42cd-87ea-20a9195e4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder, TRTHFRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cf599-1da1-45d1-9caf-13632ac43b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bc6fa-1441-4b11-b605-bbcfd40c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "whisper_trt_encoder = WhisperTRTEncoder(whisper_trt_encoder_engine, metadata, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder = WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size=batch_size, num_beams=num_beams)\n",
    "\n",
    "# FP16\n",
    "whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e686e87-0318-4dc1-8495-d8002dc91ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877bd13-1123-4a81-b0fe-5c7c3887038f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_trt_time = w_encoder_inference(\n",
    "    whisper_trt_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088717-bcf6-470f-9c8b-1f9d0564e321",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_last_hidden_states = whisper_trt_encoder_fp16(input_features=inputs['input_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c68a3d-9095-4d1b-941d-488447980196",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    WhisperModelTRTConfig.DECODER_START_TOKEN_ID,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a252c-75af-4d69-9833-84237f57ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = whisper_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                encoder_hidden_states=encoder_last_hidden_states.cuda(),\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata_fp16.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91e494-6256-4e5a-ba03-d26332f2fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(decoder_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91798bb7-12fc-4eb6-803b-31e747a83a0b",
   "metadata": {},
   "source": [
    "### End-to-End TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a214df2-cf28-444f-a68a-351aba8c9b5f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    LogitsProcessorList,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    ")\n",
    "\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[Whisper_VARIANT]\n",
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    WhisperModelTRTConfig.DECODER_START_TOKEN_ID,\n",
    ")\n",
    "\n",
    "forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\", no_timestamps=True)\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(whisper_model.config.max_length)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        SuppressTokensLogitsProcessor(WhisperModelTRTConfig.SUPPRESS_TOKENS),\n",
    "        SuppressTokensAtBeginLogitsProcessor(\n",
    "            WhisperModelTRTConfig.BEGIN_SUPPRESS_TOKENS, decoder_input_ids.shape[-1]\n",
    "        ),\n",
    "        ForceTokensLogitsProcessor(forced_decoder_ids),\n",
    "    ]\n",
    ")  # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can get identical results with raw HuggingFace\n",
    "\n",
    "encoder_outputs.last_hidden_state = encoder_outputs.last_hidden_state.cpu()\n",
    "if num_beams > 1:\n",
    "    decoder_input_ids = expand_inputs_for_beam_search(decoder_input_ids, expand_size=num_beams)\n",
    "    \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda:1\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                encoder_hidden_states=encoder_outputs.last_hidden_state.cuda(1),\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder.beam_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea322cb7-ca33-4811-86bb-4002e2e03534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16\n",
    "def e2e_trt_fp16():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder_fp16(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "            \n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda:1\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder_fp16.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states) \n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder_fp16.beam_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids_fp16 = e2e_trt_fp16()\n",
    "outputs_trt_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)\n",
    "trt_time_fp16 = measure_python_inference_code(e2e_trt_fp16, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dacea5-15c1-4fbc-b848-852ea84b0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using engine: {metadata_string + '-' + engine_tag}\")\n",
    "print(f'Output identical to HF results? {outputs_trt == outputs_hf}')\n",
    "print(f\"Precision: FP32\")\n",
    "print(f'TRT time: {trt_time}')\n",
    "print()\n",
    "print(f\"Using engine: {metadata_string_fp16 + '-' + engine_tag}\")\n",
    "print(f'Output identical to HF results? {outputs_trt_fp16 == outputs_hf}')\n",
    "print(f\"Precision: FP16\")\n",
    "print(f'TRT time: {trt_time_fp16}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e3bad-86c4-486e-aef3-4fb9da753d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef28764-c754-4045-915a-14909b75a27b",
   "metadata": {},
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "We will benchmark the encoder, decoder, and full end-to-end as we did for HuggingFace before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49992cf-3737-4091-a300-b3bf806e2a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0383bc9-f02e-467a-b5c2-092850dbb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder inference \n",
    "whisper_model.float()\n",
    "whisper_model = whisper_model.cuda(1)\n",
    "\n",
    "input_features = input_features.float().cuda(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "input_features = input_features.float().cuda(1)\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d06d2-2a0b-4f09-9c14-0249d92a46d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, input_features, timing_profile)\n",
    "_, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=0,\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=0,\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n",
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = w_encoder_inference(whisper_trt_encoder_fp16, input_features, timing_profile)\n",
    "_, decoder_trt_time_fp16 = w_decoder_inference(whisper_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=0,\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=0,\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccd523-e56f-46fe-bdb8-b93730701958",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['Framework', 'Precision', 'Encoder p50 (ms)', 'Decoder p50 (ms)', 'Full E2E p50 (ms)', 'Accuracy'],\n",
    "    ['HuggingFace (w/o cache)', 'FP32', '-', '-', f'{hf_nonkv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP32', '-', '-', f'{hf_kv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/o cache)', 'FP16', '-', '-', f'{hf_nonkv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP16', '-', '-', f'{hf_kv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['PyTorch', 'FP32', f'{encoder_pytorch_time[0]*1000:.2f}', f'{decoder_pytorch_time[0]*1000:.2f}', f'{full_pytorch_time[0]*1000:.2f}', outputs_pytorch == outputs_hf],\n",
    "    ['PyTorch', 'FP16', f'{encoder_pytorch_time_fp16[0]*1000:.2f}', f'{decoder_pytorch_time_fp16[0]*1000:.2f}', f'{full_pytorch_time_fp16[0]*1000:.2f}', outputs_pytorch_fp16 == outputs_hf],\n",
    "    ['TensorRT', 'FP32', f'{encoder_trt_time[0]*1000:.2f}', f'{decoder_trt_time[0]*1000:.2f}', f'{full_trt_time[0]*1000:.2f}', outputs_trt == outputs_hf],\n",
    "    ['TensorRT', 'FP16', f'{encoder_trt_time_fp16[0]*1000:.2f}', f'{decoder_trt_time_fp16[0]*1000:.2f}', f'{full_trt_time_fp16[0]*1000:.2f}', outputs_trt_fp16 == outputs_hf],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdfb6d-5958-4cf2-8ff6-04cb64aeecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c33d16-ea2a-4539-8544-09e78ce73a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3600f7-1925-465f-adeb-e02b267b6c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f024702-4a23-4e63-a51d-0bf7b71107d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc710f-9100-4ebc-96fb-4649d2549cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
