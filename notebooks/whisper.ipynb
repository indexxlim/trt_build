{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace Whisper Inference with TensorRT\n",
    "\n",
    "Whisper is an encoder-decoder model that converts ASR problems into a speech-to-text format. More specifically, it does so by encoding speech in the input stream. This enables a single model to be trained supervised on a wide variety of Language\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch Whisper model](https://huggingface.co/transformers/model_doc/whisper.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace whisper model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f986826-8e46-47b3-87f7-227e6622e1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51865"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51016f2c-8016-4937-9206-590369c0cb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36e6e9-8981-44d9-bff7-649c5d1c64b6",
   "metadata": {},
   "source": [
    "# Encoder output이 다름!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bf52b-9cc9-4b27-998b-b58ce4873b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2be0a006-9ee6-45ab-aab1-170c631cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ca242c55-d48f-48e2-a8a3-868e79f3c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = whisper_model.get_encoder()(inputs['input_features'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "62026975-1a6f-4453-9053-391305da2f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f77ba642-d3be-4d6c-8541-0e78b27404d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0650,  0.0491,  0.0228,  ...,  0.0256,  0.0469, -0.0124],\n",
       "         [-0.8297, -1.4162,  0.2438,  ...,  0.8406,  0.1607,  0.2904],\n",
       "         [-0.6757, -1.3830,  0.4186,  ...,  0.0135, -0.1272,  0.6469],\n",
       "         ...,\n",
       "         [ 0.7634, -1.6631,  1.0584,  ..., -0.8335,  0.1044,  0.6690],\n",
       "         [ 0.6760, -1.7349,  0.5412,  ..., -0.2875, -0.0158,  0.4131],\n",
       "         [ 0.2625, -0.1128, -1.3304,  ...,  0.2592, -0.4166, -0.0922]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_trt_encoder(inputs['input_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a075b-b392-44fe-9fe9-0c9d95d006c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a3d05d2f-bddf-458b-92ef-9887829f16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = whisper_model._get_decoder_start_token_id(None, 50258)\n",
    "input_ids  = torch.ones((1, 1), dtype=torch.long, device='cuda') * decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1f8b0677-6858-49ec-a606-e7da874cf40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.get_decoder().max_source_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4b3769a5-3c7c-4085-95f8-915beba5c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.cuda()\n",
    "hf_out = whisper_model.generate(inputs['input_features'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "35e698ad-dea2-440f-8239-40e61fefd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = whisper_model.model.decoder(input_ids=input_ids.cuda(), encoder_hidden_states=encoder_outputs['last_hidden_state'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0b297957-91d2-42e6-b8d0-6b79a9f9e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_output = whisper_model.proj_out(decoder_outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bc5bab17-d6ad-435f-86c0-2a231c737d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4892, -5.4267,  2.6655,  ...,  0.2883,  1.1339,  1.4177]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8b2dfb90-8105-491f-b3e7-5fda7d223f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_trt_decoder.MAX_SOURCE_POSITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "54dd7cea-281b-40dd-b4b4-f7400d62067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(whisper_trt_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "de18795a-384d-41ff-bf9e-72abdaa4c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_output = whisper_trt_decoder(input_ids,encoder_outputs['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "49a990f0-b659-4f5a-8d4c-5fc3b037c14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4938, -5.4309,  2.6605,  ...,  0.2829,  1.1290,  1.4134]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f1fff-a360-4309-8f47-cf2847c255a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac3c11-7dd9-41f8-8def-eefff16f1a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e747f8d-9d47-4ab2-9d98-8be9dfdddad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "97bc0735-d8d9-48e4-9a99-f27dd3cd9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(whisper_model.config.max_length)])\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        NoRepeatNGramLogitsProcessor(\n",
    "            WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "        ),\n",
    "        MinLengthLogitsProcessor(\n",
    "            min_length, WhisperModelTRTConfig.EOS_TOKEN_ID\n",
    "        ),\n",
    "        ForcedBOSTokenLogitsProcessor(50258),\n",
    "        ForcedEOSTokenLogitsProcessor(\n",
    "            whisper_model.config.max_length, WhisperModelTRTConfig.EOS_TOKEN_ID\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3650267a-51f7-405e-9a36-01d7db2927ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=None, logits=tensor([[[-2.4892, -5.4267,  2.6655,  ...,  0.2883,  1.1339,  1.4177]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model(\n",
    "    input_features=input_features,\n",
    "    decoder_input_ids=input_ids,\n",
    "    encoder_outputs=encoder_outputs,\n",
    "    #stopping_criteria=stopping_criteria,\n",
    "    #logits_processor=logits_processor,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a0021c63-8928-4598-88af-08f35c7c19cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Whisper.trt.WhisperTRTDecoder at 0x7f69adffe490>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_trt_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0ad89cd0-2eeb-47eb-952a-cbeb6994bac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6d42c157-404b-4cad-b05f-c5f6742b2a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1785\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:432\u001b[0m, in \u001b[0;36mTRTNativeRunner.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_context\u001b[38;5;241m.\u001b[39mactive_optimization_profile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:662\u001b[0m, in \u001b[0;36mWhisperTRTDecoder.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mprint\u001b[39m(bs \u001b[38;5;241m*\u001b[39m input_length)\n\u001b[0;32m--> 662\u001b[0m     \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    664\u001b[0m trt_context\u001b[38;5;241m.\u001b[39mset_binding_shape(\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# If encoder hidden states have not been copied yet, copy the hidden states to the input buffer.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#decoder_output = whisper_trt_decoder.greedy_search(\n",
    "decoder_output = whisper_trt_decoder.greedy_search(\n",
    "    input_ids=input_ids,\n",
    "    encoder_hidden_states=encoder_outputs,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f46fcd-1878-4fae-9e63-8491ee31a17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06452aa-088c-4f01-ac4e-851441a06e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "40ab98d1-7c51-4da5-8086-c46d87ec2a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a42bce93-dfc4-4c20-af91-76d6ad53f708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[-8.3201e-01, -8.3201e-01, -8.3201e-01,  ..., -2.4743e-01,\n",
       "          -3.2218e-02, -1.1799e-01],\n",
       "         [-8.3201e-01, -8.3201e-01, -8.3201e-01,  ...,  8.0496e-04,\n",
       "          -1.2268e-01, -8.3760e-02],\n",
       "         [-8.3201e-01, -8.3201e-01, -8.3201e-01,  ...,  2.2572e-02,\n",
       "          -3.2526e-01, -1.9711e-01],\n",
       "         ...,\n",
       "         [-8.3201e-01, -8.3201e-01, -8.3201e-01,  ..., -8.1406e-01,\n",
       "          -7.2681e-01, -4.5137e-01],\n",
       "         [-8.3201e-01, -8.3201e-01, -8.3201e-01,  ..., -8.0641e-01,\n",
       "          -7.7886e-01, -5.0440e-01],\n",
       "         [-8.3201e-01, -8.3201e-01, -8.3201e-01,  ..., -8.3201e-01,\n",
       "          -8.3201e-01, -6.4279e-01]]])}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "66c48dd7-0b7d-4abb-b859-17fa504ec334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 ms, sys: 0 ns, total: 38.6 ms\n",
      "Wall time: 38.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "decoder_output = whisper_model.greedy_search(\n",
    "    input_ids=input_ids,\n",
    "    encoder_outputs=encoder_outputs,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e2808f31-1d2b-4666-a033-bab517b0acbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50258, 50358, 50358, 50363,   440, 50257]], device='cuda:0')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e7683ae0-f5bb-4ad2-8154-a8aefec1f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|startoftranscript|><|translate|><|translate|><|notimestamps|> The<|endoftext|>']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414bfa5-d837-4685-88e4-0548508e1d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "622c3d7c-5f63-47a9-b4f8-19ed821501f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|startoftranscript|><|en|><|transcribe|><|notimestamps|> The most powerful tap-kana is still being used to strengthen the strong energy. However, the movement speed is almost the same as the slow speed. The tap-kana is going to move in the same direction as the middle of the road. The speed is getting faster, and the next week, the first-ever Kyushu men's shopping mall will be held. The strength is not too weak.<|endoftext|>\"]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b146951-b1a3-42a5-b516-adc88daa7ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_decoder_prompt_ids() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forced_decoder_ids \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_decoder_prompt_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_decoder_prompt_ids() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07978574-f314-496e-b542-237c8f2410d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e9f58-ebc1-4de1-b005-36216f75e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size,\n",
    "decoder_start_token_id=decoder_start_token_id,\n",
    "model_kwargs=model_kwargs,\n",
    "device=inputs_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c3ad8-e39f-4725-9627-26ee23f8bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "faf90992-bb1c-4f34-8d90-8282d299d36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258]], device='cuda:0')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model._prepare_decoder_input_ids_for_generation(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1941c458-9c7d-400a-a227-d690b2f5a262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|>'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50258])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "515b783f-0199-4876-a74a-4431b1c09b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function transformers.models.whisper.processing_whisper.WhisperProcessor.get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhisperProcessor.get_decoder_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0f7e1-c146-4fc2-a43c-98e0669e0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/nvadmin/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:0')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2634d56-0118-4240-9fa5-331419bc4ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db582b07-48f3-4372-8456-3614b298a8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = whisper_model.generate(inputs=audio_inputs.input_features.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee02055-eb0e-4f02-9366-57b3936a492f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.4 ms, sys: 273 µs, total: 37.7 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0027338999789208174"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_features = audio_inputs.input_features.to('cuda')\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3324cf43-0a3f-4a48-8fc9-e0eeed63c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1, 1]]) * whisper_model.config.decoder_start_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf81153-0cb8-4e8a-bd51-5de35a2b061e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.1 ms, sys: 1.02 ms, total: 58.1 ms\n",
      "Wall time: 57.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004369891015812755"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = w_decoder_inference(\n",
    "    whisper_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fc88907-7d1f-4453-8863-5425f7ddc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5ac34d4-efd4-46b0-a142-397b7bbe6a63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_beams = 1\n",
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3410dbdc-91a4-48c8-8acf-b13b51358689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.general_utils import measure_python_inference_code\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT).cuda()\n",
    "\n",
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = processor.tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for Whisper\n",
    "# if not cuda_114_mode:\n",
    "whisper_model= whisper_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fee3fe-8a52-44ff-a29f-e4bc02c47710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08a80940-4ff5-40d4-a82f-35a5cc1de908",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FP32\n",
    "HF_KV=True\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "whisper_model.float()\n",
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = w_encoder_inference(whisper_torch_encoder, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time = w_decoder_inference(whisper_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# # FP16\n",
    "# if not cuda_114_mode:\n",
    "whisper_model.half()\n",
    "input_features= input_features.half()\n",
    "whisper_torch_encoder_fp16 = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder_fp16 = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = w_encoder_inference(whisper_torch_encoder_fp16, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = w_decoder_inference(whisper_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda94dca-266c-45bb-ad56-9c1de5c03158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "989f32e5-6ca9-4609-b5f7-d300a3919f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 Output identical to HF results? False\n",
      "PyTorch FP16 Output identical to HF results? False\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "Encoder time: [0.0022491710260510445, 0.0022752098739147186]\n",
      "Decoder time: [0.0034474600106477737, 0.0037134091835469007]\n",
      "Full E2E time: [0.020401515997946262, 0.022245587082579732]\n",
      "Precision: FP16, Number of Beams: 1\n",
      "Encoder time: [0.0065692279022186995, 0.019419423071667552]\n",
      "Decoder time: [0.00563500402495265, 0.0058245910331606865]\n",
      "Full E2E time: [0.015776696149259806, 0.03169610211625695]\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time_fp16}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time_fp16}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d128bd1f-a8b5-4fd4-9163-e38085877537",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3663c68-147d-4ead-b3ae-c9d4f2aba230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(output_ids_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b105ef60-4141-4058-be58-0b2d268dadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4788-c322-4f9f-b4e3-e0068a95235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the Whisper model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b1fd7ce-d39b-4142-9a05-647143518d6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:198: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:237: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:742: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:72: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n"
     ]
    }
   ],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "TRT_KV = False\n",
    "\n",
    "wh_onnx_model_path = './models/{}/onnx'.format(Whisper_VARIANT)\n",
    "!mkdir -p $wh_onnx_model_path\n",
    "\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config = WhisperModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "wh_encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=True)\n",
    "onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=True)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = WhisperModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "wh_encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder_fp16 = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), force_overwrite=True)\n",
    "onnx_whisper_decoder_fp16 = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), force_overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285130f2-a801-406e-8a1f-fd44110a0ceb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whisper-tiny-fp16-decoder-with-lm-head.onnx'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_decoder_onnx_model_fpath_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "572b2d68-4004-4724-abd4-079c5487e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, 1500, max_sequence_length),\n",
    "    max=(batch_size * num_beams, 1500, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 3000),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n",
    "\n",
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9514a333-83eb-4654-b74c-4586a91ec7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\u001b[0m\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layer_norm/ReduceMean': 'FLOAT', 'encoder/layer_norm/Pow': 'FLOAT', 'encoder/layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layer_norm/Add': 'FLOAT', 'encoder/layer_norm/Sqrt': 'FLOAT', 'encoder/layer_norm/Div': 'FLOAT', 'encoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;11m[W] Using kFASTER_DYNAMIC_SHAPES_0805 preview feature.\u001b[0m\n",
      "\u001b[38;5;11m[W] onnx2trt_utils.cpp:403: One or more weights outside the range of INT32 was clamped\u001b[0m\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'/decoder/Cast_2': 'FLOAT', '/decoder/Cast_3': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/final_layer_norm/Add': 'FLOAT', '/decoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/final_layer_norm/Div': 'FLOAT', '/decoder/layers.0/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/final_layer_norm/Add': 'FLOAT', '/decoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/final_layer_norm/Div': 'FLOAT', '/decoder/layers.1/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/final_layer_norm/Add': 'FLOAT', '/decoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/final_layer_norm/Div': 'FLOAT', '/decoder/layers.2/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/final_layer_norm/Add': 'FLOAT', '/decoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/final_layer_norm/Div': 'FLOAT', '/decoder/layers.3/final_layer_norm/Mul': 'FLOAT', '/decoder/layer_norm/ReduceMean': 'FLOAT', '/decoder/layer_norm/Pow': 'FLOAT', '/decoder/layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layer_norm/Add': 'FLOAT', '/decoder/layer_norm/Sqrt': 'FLOAT', '/decoder/layer_norm/Div': 'FLOAT', '/decoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "force_write=True\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name) or force_write:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name) or force_write:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efba83df-ffd5-4ea5-b74d-070d045d83ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layer_norm/ReduceMean': 'FLOAT', 'encoder/layer_norm/Pow': 'FLOAT', 'encoder/layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layer_norm/Add': 'FLOAT', 'encoder/layer_norm/Sqrt': 'FLOAT', 'encoder/layer_norm/Div': 'FLOAT', 'encoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;11m[W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\u001b[0m\n",
      "\u001b[38;5;11m[W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\u001b[0m\n",
      "\u001b[38;5;11m[W] Check verbose logs for the list of affected weights.\u001b[0m\n",
      "\u001b[38;5;11m[W] - 67 weights are affected by this issue: Detected subnormal FP16 values.\u001b[0m\n",
      "\u001b[38;5;11m[W] - 1 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# FP16\n",
    "wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name_fp16) or force_write:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name_fp16) or force_write:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf44b1-f951-4380-b2cb-b4b7188cacb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873ac2a-fbda-40c8-8646-f15eae2284a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "033c3627-d054-4bec-8e09-a6daf145021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper-tiny-encoder.onnx\n",
      "Whisper-tiny-decoder-with-lm-head.onnx\n",
      "<Whisper.export.WhisperEncoderONNXFile object at 0x7f69adffe0a0>\n",
      "<Whisper.export.WhisperDecoderONNXFile object at 0x7f6acc312cd0>\n"
     ]
    }
   ],
   "source": [
    "print(wh_encoder_onnx_model_fpath)\n",
    "print(wh_decoder_onnx_model_fpath)\n",
    "print(onnx_whisper_encoder)\n",
    "print(onnx_whisper_decoder)\n",
    "#onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "#onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2422-c55d-4d02-85f8-4e2f659e0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5da0a58-fbc1-41ce-be96-358cdca01f9a",
   "metadata": {},
   "source": [
    "# Whisper Tensorrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0bfb51e-84cc-42cd-87ea-20a9195e4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder, TRTHFRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63591388-3a8d-416a-ae18-d6be56fe818e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c11bc6fa-1441-4b11-b605-bbcfd40c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "whisper_trt_encoder = WhisperTRTEncoder(whisper_trt_encoder_engine, metadata, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder = WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size=batch_size, num_beams=num_beams)\n",
    "\n",
    "# FP16\n",
    "whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318da12b-deaa-4598-957f-6f7e993be4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9877bd13-1123-4a81-b0fe-5c7c3887038f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.4 ms, sys: 0 ns, total: 35.4 ms\n",
      "Wall time: 35.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0027338999789208174"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_trt_time = w_encoder_inference(\n",
    "    whisper_trt_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088717-bcf6-470f-9c8b-1f9d0564e321",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a252c-75af-4d69-9833-84237f57ae51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91798bb7-12fc-4eb6-803b-31e747a83a0b",
   "metadata": {},
   "source": [
    "### End-to-End TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "37097dac-aa8e-45e3-8ba0-924927031865",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "fd04970a-54ac-49ca-af78-2b2d8de52e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_len=384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "69506b6a-7ca9-43d2-b7eb-af073eec83c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|>'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(whisper_model.config.decoder_start_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "3a214df2-cf28-444f-a68a-351aba8c9b5f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[395], line 73\u001b[0m\n\u001b[1;32m     62\u001b[0m             decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[1;32m     63\u001b[0m                 input_ids\u001b[38;5;241m=\u001b[39mdecoder_initial_input,\n\u001b[1;32m     64\u001b[0m                 beam_scorer\u001b[38;5;241m=\u001b[39mbeam_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     70\u001b[0m             )\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output\n\u001b[0;32m---> 73\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43me2e_trt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m outputs_trt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m trt_time \u001b[38;5;241m=\u001b[39m measure_python_inference_code(e2e_trt, timing_profile)\n",
      "Cell \u001b[0;32mIn[395], line 53\u001b[0m, in \u001b[0;36me2e_trt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mset_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_beams \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_trt_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_initial_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[1;32m     63\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mdecoder_initial_input,\n\u001b[1;32m     64\u001b[0m         beam_scorer\u001b[38;5;241m=\u001b[39mbeam_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1785\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:432\u001b[0m, in \u001b[0;36mTRTNativeRunner.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_context\u001b[38;5;241m.\u001b[39mactive_optimization_profile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:661\u001b[0m, in \u001b[0;36mWhisperTRTDecoder.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     bindings[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdata_ptr()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    663\u001b[0m trt_context\u001b[38;5;241m.\u001b[39mset_binding_shape(\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# If encoder hidden states have not been copied yet, copy the hidden states to the input buffer.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[Whisper_VARIANT]\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(no_repeat_ngram_size),\n",
    "    MinLengthLogitsProcessor(min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(50258),\n",
    "    ForcedEOSTokenLogitsProcessor(max_output_len, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "]) # by checking HuggingFace's generate() implementation carefully, the default logits processor for Whisper has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can ensure identical results with raw HuggingFace\n",
    "\n",
    "decoder_initial_input = torch.full(\n",
    "    (batch_size, 1), 50258, dtype=torch.int32\n",
    ").to('cuda')\n",
    "\n",
    "if num_beams > 1:\n",
    "    decoder_initial_input = expand_inputs_for_beam_search(decoder_initial_input, expand_size=num_beams)\n",
    "    \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "e67f3294-64f3-4e3f-8f63-66ba3e1a105b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x7f6b6e926450>"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StoppingCriteriaList.max_length.getter(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae901f5-4357-4003-963d-5824fce026ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "da6ba6ca-bdb4-4fcb-8806-62957af35951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.generation_beam_constraints import Constraint, DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation_beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation_logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.pytorch_utils import torch_int_div\n",
    "\n",
    "max_length = max_output_len\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "output_scores = whisper_model.config.output_scores\n",
    "output_attentions = whisper_model.config.output_attentions\n",
    "output_hidden_states = encoder_outputs.last_hidden_state\n",
    "return_dict_in_generate= whisper_model.config.return_dict_in_generate\n",
    "synced_gpus=False\n",
    "\n",
    "whisper_trt_decoder.prepare_inputs_for_generation\n",
    "\n",
    "model_kwargs = {\n",
    "    \"encoder_hidden_states\":encoder_outputs.last_hidden_state,\n",
    "    \"stopping_criteria\":stopping_criteria,\n",
    "    \"logits_processor\":logits_processor,\n",
    "    \"use_cache\":metadata.other.kv_cache,\n",
    "    \"use_cuda\":True\n",
    "}\n",
    "\n",
    "input_ids=decoder_initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "69aae2e4-a501-48d9-b638-6b560c208c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|>'"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(decoder_initial_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a722f4-7f33-422d-bea6-2f97308aebe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "9aa991ad-acc5-4b0c-a46a-3d502fe68f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2200118/2796392029.py:4: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[399], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_trt_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:432\u001b[0m, in \u001b[0;36mTRTNativeRunner.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_context\u001b[38;5;241m.\u001b[39mactive_optimization_profile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:661\u001b[0m, in \u001b[0;36mWhisperTRTDecoder.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     bindings[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdata_ptr()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    663\u001b[0m trt_context\u001b[38;5;241m.\u001b[39mset_binding_shape(\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# If encoder hidden states have not been copied yet, copy the hidden states to the input buffer.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "if max_length is not None:\n",
    "    warnings.warn(\n",
    "        \"`max_length` is deprecated in this function, use\"\n",
    "        \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "        UserWarning,\n",
    "    )\n",
    "    stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "output_hidden_states = (\n",
    "    output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    ")\n",
    "return_dict_in_generate = (\n",
    "    return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    ")\n",
    "\n",
    "# init attention / hidden states / scores tuples\n",
    "scores = () if (return_dict_in_generate and output_scores) else None\n",
    "decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "# if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "    encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "    encoder_hidden_states = (\n",
    "        model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "    )\n",
    "\n",
    "# keep track of which sequences are already finished\n",
    "unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "\n",
    "this_peer_finished = False  # used by synced_gpus only\n",
    "while True:\n",
    "    if synced_gpus:\n",
    "        # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "        # The following logic allows an early break if all peers finished generating their sequence\n",
    "        this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "        # send 0.0 if we finished, 1.0 otherwise\n",
    "        dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "        # did all peers finish? the reduced sum will be 0.0 then\n",
    "        if this_peer_finished_flag.item() == 0.0:\n",
    "            break\n",
    "\n",
    "    # prepare model inputs\n",
    "    model_inputs = whisper_trt_decoder.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "    # forward pass to get next token\n",
    "    outputs = whisper_trt_decoder(\n",
    "        **model_inputs,\n",
    "        return_dict=True,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "    )\n",
    "\n",
    "    if synced_gpus and this_peer_finished:\n",
    "        continue  # don't waste resources running the code we don't need\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # pre-process distribution\n",
    "    next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "    # Store scores, attentions and hidden_states when required\n",
    "    if return_dict_in_generate:\n",
    "        if output_scores:\n",
    "            scores += (next_tokens_scores,)\n",
    "        if output_attentions:\n",
    "            decoder_attentions += (\n",
    "                (outputs.decoder_attentions,) if whisper_trt_decoder.config.is_encoder_decoder else (outputs.attentions,)\n",
    "            )\n",
    "            if whisper_trt_decoder.config.is_encoder_decoder:\n",
    "                cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            decoder_hidden_states += (\n",
    "                (outputs.decoder_hidden_states,)\n",
    "                if whisper_trt_decoder.config.is_encoder_decoder\n",
    "                else (outputs.hidden_states,)\n",
    "            )\n",
    "    print(1)\n",
    "\n",
    "    # argmax\n",
    "    next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "    # finished sentences should have their next token be a padding token\n",
    "    if eos_token_id is not None:\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "        next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "    print(2)\n",
    "\n",
    "    # update generated ids, model inputs, and length for next step\n",
    "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "    model_kwargs = whisper_trt_decoder._update_model_kwargs_for_generation(\n",
    "        outputs, model_kwargs, is_encoder_decoder=whisper_trt_decoder.config.is_encoder_decoder\n",
    "    )\n",
    "    print(3)\n",
    "\n",
    "    # if eos_token was found in one sentence, set sentence to finished\n",
    "    if eos_token_id is not None:\n",
    "        unfinished_sequences = unfinished_sequences.mul((next_tokens != eos_token_id).long())\n",
    "    print(4)\n",
    "\n",
    "    # stop when each sentence is finished, or if we exceed the maximum length\n",
    "    if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "        if not synced_gpus:\n",
    "            print(synced_gpus)\n",
    "            print(5)\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            this_peer_finished = True\n",
    "    #print(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "0f49dfb6-44be-4a85-ba4e-5bc71830b1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=None, logits=tensor([[[15.7015, 15.4863, 17.9366,  ..., 12.0131, 11.3766, 10.9509]]],\n",
       "       device='cuda:0'), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=None, encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_trt_decoder(\n",
    "    **model_inputs,\n",
    "    return_dict=True,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "d879165a-b81b-4edf-96af-df099bdbbb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "1efa89cb-1ee5-4145-ae9c-ff84ea5cc858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50257]], device='cuda:0', dtype=torch.int32),\n",
       " 'encoder_hidden_states': tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "          [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "          [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "          ...,\n",
       "          [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "          [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "          [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>)}"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "584608a0-a74f-4733-839f-1e7cba830855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next_tokens != eos_token_id).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c46dcda3-197d-4a28-b1b6-ceff797edb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfinished_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e72621c7-035b-4a8f-951a-3f95ef8efecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synced_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "2fc8a131-7b77-461d-b543-39735d13d47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0de273b6-de48-4a0e-b26b-84d76d316751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfinished_sequences.mul((next_tokens != eos_token_id).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "b6c34caa-7c46-4edf-9748-8d1fc81741f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GreedySearchEncoderDecoderOutput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mGreedySearchEncoderDecoderOutput\u001b[49m(\n\u001b[1;32m      2\u001b[0m     sequences\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m      3\u001b[0m     scores\u001b[38;5;241m=\u001b[39mscores,\n\u001b[1;32m      4\u001b[0m     encoder_attentions\u001b[38;5;241m=\u001b[39mencoder_attentions,\n\u001b[1;32m      5\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m      6\u001b[0m     decoder_attentions\u001b[38;5;241m=\u001b[39mdecoder_attentions,\n\u001b[1;32m      7\u001b[0m     cross_attentions\u001b[38;5;241m=\u001b[39mcross_attentions,\n\u001b[1;32m      8\u001b[0m     decoder_hidden_states\u001b[38;5;241m=\u001b[39mdecoder_hidden_states,\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GreedySearchEncoderDecoderOutput' is not defined"
     ]
    }
   ],
   "source": [
    "GreedySearchEncoderDecoderOutput(\n",
    "    sequences=input_ids,\n",
    "    scores=scores,\n",
    "    encoder_attentions=encoder_attentions,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    decoder_attentions=decoder_attentions,\n",
    "    cross_attentions=cross_attentions,\n",
    "    decoder_hidden_states=decoder_hidden_states,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "80dccfcd-951d-4aa3-842a-22fc3950e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "55ab4d0c-90c9-40b8-8bac-0bcf3f407628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50257]], device='cuda:0')"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8ab48-936e-4343-a00d-744a55603205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if return_dict_in_generate:\n",
    "    if whisper_trt_decoder.config.is_encoder_decoder:\n",
    "        return GreedySearchEncoderDecoderOutput(\n",
    "            sequences=input_ids,\n",
    "            scores=scores,\n",
    "            encoder_attentions=encoder_attentions,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            decoder_attentions=decoder_attentions,\n",
    "            cross_attentions=cross_attentions,\n",
    "            decoder_hidden_states=decoder_hidden_states,\n",
    "        )\n",
    "    else:\n",
    "        return GreedySearchDecoderOnlyOutput(\n",
    "            sequences=input_ids,\n",
    "            scores=scores,\n",
    "            attentions=decoder_attentions,\n",
    "            hidden_states=decoder_hidden_states,\n",
    "        )\n",
    "else:\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd43325-787a-496c-9be9-dd8041e969cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6c0043c2-b2a9-4c38-8512-816e7ad2dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1843,  0.1644,  0.1285,  ..., -0.0349,  0.0105, -0.0849],\n",
       "         [ 0.9001,  2.4472,  0.7696,  ...,  0.8979, -0.0847,  1.0418],\n",
       "         [ 0.6817,  2.8782,  1.2798,  ...,  0.5644, -0.6482,  0.9689],\n",
       "         ...,\n",
       "         [ 0.7794, -1.3334,  0.7304,  ...,  0.9130,  1.3428,  0.1077],\n",
       "         [ 1.3128, -1.1221,  0.1789,  ..., -0.3750, -0.0723, -0.6635],\n",
       "         [ 0.0936, -0.1169, -1.3959,  ..., -0.0278, -0.5646, -0.2211]]],\n",
       "       device='cuda:0', grad_fn=<IndexSelectBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9eaed903-6365-401b-b1f7-f66a63c7b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_outputs.last_hidden_state, expand_size=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7684f25a-6e90-4f8b-9afd-66c6bdb5351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ef06f879-3a79-4e25-a94a-1074215d805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = whisper_trt_decoder.greedy_search(\n",
    "    input_ids=decoder_initial_input,\n",
    "    encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=metadata.other.kv_cache,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607c12a-be8e-4cca-9025-0e7e6e234cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b7b43dbf-2cd7-429f-87cb-22d0a7cb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3e5e730a-3b50-4471-8b3d-a4152ceaeba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50257]], device='cuda:0')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0b151248-242e-46af-b443-d5fb63102ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a881e9d6-268e-47b4-bf2c-497a464a517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "605b9bb1-704f-421e-a034-2e820d0200c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "77737291-81db-4d25-aee6-61aa41738f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea322cb7-ca33-4811-86bb-4002e2e03534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FP16\n",
    "def e2e_trt_fp16():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder_fp16(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "            \n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder_fp16.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder_fp16.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids_fp16 = e2e_trt_fp16()\n",
    "outputs_trt_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)\n",
    "trt_time_fp16 = measure_python_inference_code(e2e_trt_fp16, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ef06300-fac5-4c46-af14-573f6403b4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_trt_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eecfc972-3fde-4838-af73-af68f41bca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0152398650534451, 0.021135421004146338]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_time_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9362687-15d8-4eda-88f9-0831c6a5efb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0dacea5-15c1-4fbc-b848-852ea84b0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Using engine: Whisper-tiny-bs1\n",
      "Output identical to HF results? False\n",
      "Precision: FP32\n",
      "TRT time: [0.011139628943055868, 0.018518408993259072]\n",
      "\n",
      "Using engine: Whisper-tiny-fp16-bs1\n",
      "Output identical to HF results? False\n",
      "Precision: FP16\n",
      "TRT time: [0.0152398650534451, 0.021135421004146338]\n"
     ]
    }
   ],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using engine: {metadata_string + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt == outputs_hf}')\n",
    "print(f\"Precision: FP32\")\n",
    "print(f'TRT time: {trt_time}')\n",
    "print()\n",
    "print(f\"Using engine: {metadata_string_fp16 + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt_fp16 == outputs_hf}')\n",
    "print(f\"Precision: FP16\")\n",
    "print(f'TRT time: {trt_time_fp16}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b3e3bad-86c4-486e-aef3-4fb9da753d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 ms, sys: 88 µs, total: 36.4 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a8eef-445c-420c-a1a7-2a1348ea6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7db23658-fbcb-4cc8-831e-7434347b5a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.003225164022296667, 0.0032994120847433805]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_trt_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef28764-c754-4045-915a-14909b75a27b",
   "metadata": {},
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "We will benchmark the encoder, decoder, and full end-to-end as we did for HuggingFace before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49992cf-3737-4091-a300-b3bf806e2a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0383bc9-f02e-467a-b5c2-092850dbb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder inference \n",
    "whisper_model.float()\n",
    "whisper_model = whisper_model.cuda()\n",
    "\n",
    "input_features = input_features.float().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "input_features = input_features.float().cuda()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e9d06d2-2a0b-4f09-9c14-0249d92a46d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder time: p50 3.84ms, p99 3.89ms\n",
      "Decoder time: p50 1.29ms, p99 2.34ms\n",
      "Full E2E time: p50 10.72ms, p99 18.58ms\n",
      "Encoder FP16 time: p50 1.48ms, p99 3.80ms\n",
      "Decoder FP16 time: p50 0.90ms, p99 1.72ms\n",
      "Full E2E FP16 time: p50 15.43ms, p99 21.18ms\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, input_features, timing_profile)\n",
    "_, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n",
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = w_encoder_inference(whisper_trt_encoder_fp16, input_features, timing_profile)\n",
    "_, decoder_trt_time_fp16 = w_decoder_inference(whisper_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dccd523-e56f-46fe-bdb8-b93730701958",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Framework               | Precision   | Encoder p50 (ms)   | Decoder p50 (ms)   |   Full E2E p50 (ms) | Accuracy   |\n",
      "|-------------------------|-------------|--------------------|--------------------|---------------------|------------|\n",
      "| HuggingFace (w/o cache) | FP32        | -                  | -                  |              184.55 | -          |\n",
      "| HuggingFace (w/ cache)  | FP32        | -                  | -                  |              142.76 | -          |\n",
      "| HuggingFace (w/o cache) | FP16        | -                  | -                  |              176.84 | -          |\n",
      "| HuggingFace (w/ cache)  | FP16        | -                  | -                  |              125.35 | -          |\n",
      "| PyTorch                 | FP32        | 2.25               | 3.45               |               20.4  | False      |\n",
      "| PyTorch                 | FP16        | 6.57               | 5.64               |               16.06 | False      |\n",
      "| TensorRT                | FP32        | 3.84               | 1.29               |               10.72 | False      |\n",
      "| TensorRT                | FP16        | 1.48               | 0.90               |               15.43 | False      |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['Framework', 'Precision', 'Encoder p50 (ms)', 'Decoder p50 (ms)', 'Full E2E p50 (ms)', 'Accuracy'],\n",
    "    ['HuggingFace (w/o cache)', 'FP32', '-', '-', f'{hf_nonkv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP32', '-', '-', f'{hf_kv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/o cache)', 'FP16', '-', '-', f'{hf_nonkv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP16', '-', '-', f'{hf_kv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['PyTorch', 'FP32', f'{encoder_pytorch_time[0]*1000:.2f}', f'{decoder_pytorch_time[0]*1000:.2f}', f'{full_pytorch_time[0]*1000:.2f}', outputs_pytorch == outputs_hf],\n",
    "    ['PyTorch', 'FP16', f'{encoder_pytorch_time_fp16[0]*1000:.2f}', f'{decoder_pytorch_time_fp16[0]*1000:.2f}', f'{full_pytorch_time_fp16[0]*1000:.2f}', outputs_pytorch_fp16 == outputs_hf],\n",
    "    ['TensorRT', 'FP32', f'{encoder_trt_time[0]*1000:.2f}', f'{decoder_trt_time[0]*1000:.2f}', f'{full_trt_time[0]*1000:.2f}', outputs_trt == outputs_hf],\n",
    "    ['TensorRT', 'FP16', f'{encoder_trt_time_fp16[0]*1000:.2f}', f'{decoder_trt_time_fp16[0]*1000:.2f}', f'{full_trt_time_fp16[0]*1000:.2f}', outputs_trt_fp16 == outputs_hf],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2281bf29-3ac8-4bdd-9c91-2e3f16396cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3fd5d-4c75-423a-9738-00cfb96152ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e61a8454-4226-4585-818a-2f25b416b674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'ko' with probability 0.998047\n",
      "[0.00s -> 5.50s]  제 6코 태풍 가능은 여전히 매우 강한 세력을 유지한 채 북서진하고 있습니다.\n",
      "[5.50s -> 10.00s]  하지만 이동 속도가 점점들여져 거의 정체하는 모습입니다.\n",
      "[10.00s -> 15.00s]  태풍은 동중국회의 머물나 동쪽으로 방향을 급격히 틀어 이동할 걸로 보입니다.\n",
      "[15.00s -> 22.00s]  속도도 조금씩 빨라지면 다음 주 초중반에는 일본 교수 남쪽의 상까지 진출하겠습니다.\n",
      "[22.00s -> 25.00s]  새력도 크게 약화하지는 않을 전망입니다.\n",
      "[25.00s -> 28.00s]  태양 열령량도 좋은 적건을 보이고 있습니다.\n",
      "[28.00s -> 36.00s]  따라서 북상하면서 급격히 약화되는 것이 아니라 어느 정도 세력은 계속해서 유지해서 북상할 것으로 예상되고 있습니다.\n",
      "[36.00s -> 40.00s]  이후 진로는 유동적이지만 크게 두 가지 경로가 유력합니다.\n",
      "[40.00s -> 46.00s]  갑자기 방향을 북쪽으로 들어 일본 열돌을 관통한 뒤 동일로 북상할 가능성이 있습니다.\n",
      "[46.00s -> 52.00s]  반대로 일본의 상륙한 뒤 열돌을 따라 계속 동진하는 진로를 택할 수도 있습니다.\n",
      "[52.00s -> 56.00s]  만일 동일로 올라온다면 다음 주 중후반점이 될 걸로 보입니다.\n",
      "[56.00s -> 61.00s]  이 경우 우리나라 동쪽 지역이 태풍 직점 영향권에 들게 됩니다.\n",
      "[61.00s -> 68.00s]  기상청은 주변 기약 개편화에 따라 태풍 진로가 최종 결정될 거라면 태풍에 대한 경계를 당부했습니다.\n",
      "[68.00s -> 71.00s]  YTN 김민경입니다.\n",
      "CPU times: user 5.52 s, sys: 4.48 s, total: 9.99 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"tiny\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(\"korean_news.mp4\", beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7866b40-b3ef-43b8-8f7a-18cc82eb47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a463539b-34f3-46b0-b48a-b49f5e25a2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'align',\n",
       " 'detect_language',\n",
       " 'device',\n",
       " 'device_index',\n",
       " 'encode',\n",
       " 'generate',\n",
       " 'is_multilingual',\n",
       " 'num_active_batches',\n",
       " 'num_queued_batches',\n",
       " 'num_workers']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2497dd-c80f-421e-8557-ad7c4eacf9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f8f44-23fd-412f-ba5e-9638a9ab7b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d02e1dfd-d618-48e3-9262-db5bb4809ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "from typing import BinaryIO, Union\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "def decode_audio(\n",
    "    input_file: Union[str, BinaryIO],\n",
    "    sampling_rate: int = 16000,\n",
    "    split_stereo: bool = False,\n",
    "):\n",
    "    \"\"\"Decodes the audio.\n",
    "\n",
    "    Args:\n",
    "      input_file: Path to the input file or a file-like object.\n",
    "      sampling_rate: Resample the audio to this sample rate.\n",
    "      split_stereo: Return separate left and right channels.\n",
    "\n",
    "    Returns:\n",
    "      A float32 Numpy array.\n",
    "\n",
    "      If `split_stereo` is enabled, the function returns a 2-tuple with the\n",
    "      separated left and right channels.\n",
    "    \"\"\"\n",
    "    resampler = av.audio.resampler.AudioResampler(\n",
    "        format=\"s16\",\n",
    "        layout=\"mono\" if not split_stereo else \"stereo\",\n",
    "        rate=sampling_rate,\n",
    "    )\n",
    "\n",
    "    raw_buffer = io.BytesIO()\n",
    "    dtype = None\n",
    "\n",
    "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
    "        frames = container.decode(audio=0)\n",
    "        frames = _ignore_invalid_frames(frames)\n",
    "        frames = _group_frames(frames, 500000)\n",
    "        frames = _resample_frames(frames, resampler)\n",
    "\n",
    "        for frame in frames:\n",
    "            array = frame.to_ndarray()\n",
    "            dtype = array.dtype\n",
    "            raw_buffer.write(array)\n",
    "\n",
    "    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n",
    "\n",
    "    # Convert s16 back to f32.\n",
    "    audio = audio.astype(np.float32) / 32768.0\n",
    "\n",
    "    if split_stereo:\n",
    "        left_channel = audio[0::2]\n",
    "        right_channel = audio[1::2]\n",
    "        return left_channel, right_channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _ignore_invalid_frames(frames):\n",
    "    iterator = iter(frames)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except av.error.InvalidDataError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def _group_frames(frames, num_samples=None):\n",
    "    fifo = av.audio.fifo.AudioFifo()\n",
    "\n",
    "    for frame in frames:\n",
    "        frame.pts = None  # Ignore timestamp check.\n",
    "        fifo.write(frame)\n",
    "\n",
    "        if num_samples is not None and fifo.samples >= num_samples:\n",
    "            yield fifo.read()\n",
    "\n",
    "    if fifo.samples > 0:\n",
    "        yield fifo.read()\n",
    "\n",
    "\n",
    "def _resample_frames(frames, resampler):\n",
    "    # Add None to flush the resampler.\n",
    "    for frame in itertools.chain(frames, [None]):\n",
    "        yield from resampler.resample(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee18989-bb79-492f-8ad8-f493bef4d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5fe97ce-dd53-4132-a4f5-e1d590271eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import feature_extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b191b3c-306c-420b-99be-89f16d2ce3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03007ec5-ac3e-44ab-8eec-e0827a2f7dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 453 ms, sys: 1.94 s, total: 2.39 s\n",
      "Wall time: 47.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, inputs['input_features'], timing_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e82fc4ea-1969-4db8-be65-b7def75424a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95af443c-a4db-4860-97e7-a0c478a06d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = whisper_model.model.encoder(inputs['input_features'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45ecfa08-907d-41a1-b590-69bb67124c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b250796-79cd-485d-89d4-ce382379d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_de = whisper_model.model.decoder(input_ids.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7103f6-09e1-47e7-8d7d-a5202b4c78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_de[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c0797-f9f0-424f-9340-1a30995919aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ead8e-9857-4d44-82af-78bbf9df0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065aa77e-3e2d-40a7-9e9d-e339ede84a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.other.kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18ebbb-ed2b-4c0c-a722-47d483113025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.tensorrt_utils import TRTNativeRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979359f5-e75c-4218-994c-caf29a06621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        NoRepeatNGramLogitsProcessor(no_repeat_ngram_size),\n",
    "        MinLengthLogitsProcessor(\n",
    "            min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        ),\n",
    "        ForcedBOSTokenLogitsProcessor(\n",
    "            50258\n",
    "        ),\n",
    "        ForcedEOSTokenLogitsProcessor(\n",
    "            max_output_len, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        ),\n",
    "    ]\n",
    ")  # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can get identical results with raw HuggingFace\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    tokenizer.convert_tokens_to_ids(tokenizer.eos_token),\n",
    "    dtype=torch.int32,\n",
    ")\n",
    "\n",
    "if False:\n",
    "    decoder_input_ids = decoder_input_ids.to(\"cuda\")\n",
    "else:\n",
    "    decoder_input_ids = decoder_input_ids.to(\"cpu\")\n",
    "\n",
    "def _e2e():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "        decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbb0f6-a16f-4c00-9026-94ddeb47f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _e2e_trt():\n",
    "        with torch.no_grad():\n",
    "            encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "            whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(\n",
    "                encoder_last_hidden_state\n",
    "            )\n",
    "            decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_last_hidden_state,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=False,\n",
    "            )\n",
    "        return decoder_output_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34647f96-2548-4010-980e-cb453d6bdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_function = _e2e\n",
    "if isinstance(whisper_trt_decoder, TRTNativeRunner):\n",
    "    whisper_trt_decoder.set_return_device(\"cuda\" if False else \"cpu\")\n",
    "    measurement_function = _e2e_trt\n",
    "\n",
    "full_e2e_time = measure_python_inference_code(measurement_function, timing_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39cf07-a094-471b-874f-a16aa1cc8502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995d4c8-03a2-4b7f-8cc7-f8a099d19155",
   "metadata": {},
   "outputs": [],
   "source": [
    "return (measurement_function(), full_e2e_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41721f12-2df3-4126-b812-92366ebd97bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be521c-7342-4c90-b2d3-2e4fd7337019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25e391-8d09-4fab-8776-60bf6f206603",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model._get_decoder_start_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa3766-3ac4-47fc-ae18-8fc1af22f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e1bd3-a77f-46d3-94c6-a8abd65edb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0a6da-b513-4437-9535-bc1e7ef9bccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
