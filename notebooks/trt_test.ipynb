{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd0f338-383a-4862-a8f4-bad736cb5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")\n",
    "\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "from typing import BinaryIO, Union\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "def decode_audio(\n",
    "    input_file: Union[str, BinaryIO],\n",
    "    sampling_rate: int = 16000,\n",
    "    split_stereo: bool = False,\n",
    "):\n",
    "    \"\"\"Decodes the audio.\n",
    "\n",
    "    Args:\n",
    "      input_file: Path to the input file or a file-like object.\n",
    "      sampling_rate: Resample the audio to this sample rate.\n",
    "      split_stereo: Return separate left and right channels.\n",
    "\n",
    "    Returns:\n",
    "      A float32 Numpy array.\n",
    "\n",
    "      If `split_stereo` is enabled, the function returns a 2-tuple with the\n",
    "      separated left and right channels.\n",
    "    \"\"\"\n",
    "    resampler = av.audio.resampler.AudioResampler(\n",
    "        format=\"s16\",\n",
    "        layout=\"mono\" if not split_stereo else \"stereo\",\n",
    "        rate=sampling_rate,\n",
    "    )\n",
    "\n",
    "    raw_buffer = io.BytesIO()\n",
    "    dtype = None\n",
    "\n",
    "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
    "        frames = container.decode(audio=0)\n",
    "        frames = _ignore_invalid_frames(frames)\n",
    "        frames = _group_frames(frames, 500000)\n",
    "        frames = _resample_frames(frames, resampler)\n",
    "\n",
    "        for frame in frames:\n",
    "            array = frame.to_ndarray()\n",
    "            dtype = array.dtype\n",
    "            raw_buffer.write(array)\n",
    "\n",
    "    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n",
    "\n",
    "    # Convert s16 back to f32.\n",
    "    audio = audio.astype(np.float32) / 32768.0\n",
    "\n",
    "    if split_stereo:\n",
    "        left_channel = audio[0::2]\n",
    "        right_channel = audio[1::2]\n",
    "        return left_channel, right_channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _ignore_invalid_frames(frames):\n",
    "    iterator = iter(frames)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except av.error.InvalidDataError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def _group_frames(frames, num_samples=None):\n",
    "    fifo = av.audio.fifo.AudioFifo()\n",
    "\n",
    "    for frame in frames:\n",
    "        frame.pts = None  # Ignore timestamp check.\n",
    "        fifo.write(frame)\n",
    "\n",
    "        if num_samples is not None and fifo.samples >= num_samples:\n",
    "            yield fifo.read()\n",
    "\n",
    "    if fifo.samples > 0:\n",
    "        yield fifo.read()\n",
    "\n",
    "\n",
    "def _resample_frames(frames, resampler):\n",
    "    # Add None to flush the resampler.\n",
    "    for frame in itertools.chain(frames, [None]):\n",
    "        yield from resampler.resample(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1d0dc0-7eba-47e4-bbd3-74b5bc0f464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig,\n",
    "    AutoConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f968e2d-abd1-4689-81c7-d1814f47e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e18a5d53-ef74-49e5-8222-75d2e294a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "137c95bd-7e94-441f-beb1-3aaa561cfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f87d3d91-c633-4967-834f-438ac3d56ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder, TRTHFRunner\n",
    "TRT_KV=False\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7978d807-7abe-4bbb-a1bc-7c4737c64d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_encoder_engine = WhisperEncoderTRTEngine('./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine', metadata)\n",
    "whisper_trt_decoder_engine = WhisperDecoderTRTEngine('./models/openai/whisper-tiny/tensorrt/Whisper-tiny-decoder-with-lm-head.onnx-bs1.engine', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "587c317c-869a-46c3-9481-3e37b6bac939",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77456dbe-7822-4daf-95a4-aec1452a202e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "388b9ee0-05d0-4c7c-8f5f-bcfcbdbef5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/09/2023-10:47:12] [TRT] [E] 1: [runtime.cpp::parsePlan::314] Error Code 1: Serialization (Serialization assertion plan->header.magicTag == rt::kPLAN_MAGIC_TAG failed.)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create_execution_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m trt_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(Whisper_VARIANT, use_cache \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mother\u001b[38;5;241m.\u001b[39mkv_cache)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# FP32\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m whisper_trt_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperTRTEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhisper_trt_encoder_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrt_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m whisper_trt_decoder \u001b[38;5;241m=\u001b[39m WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# # FP16\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=1, num_beams=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:163\u001b[0m, in \u001b[0;36mWhisperTRTEncoder.__init__\u001b[0;34m(self, trt_engine_file, network_metadata, hf_config, batch_size, benchmarking_args)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     trt_engine_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     benchmarking_args: WhisperTRTBenchmarkingArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m ):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrt_engine_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# In benchmarking mode, the max_sequence_length should be the designated input_profile_max_len\u001b[39;00m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/trt.py:147\u001b[0m, in \u001b[0;36mTRTHFRunner.__init__\u001b[0;34m(self, trt_engine_file, network_metadata, hf_config, batch_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    142\u001b[0m     trt_engine_file: TRTEngineFile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    146\u001b[0m ):\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrt_engine_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m hf_config\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:383\u001b[0m, in \u001b[0;36mTRTNativeRunner.__init__\u001b[0;34m(self, trt_engine_file, network_metadata)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_runtime \u001b[38;5;241m=\u001b[39m trt\u001b[38;5;241m.\u001b[39mRuntime(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_logger)\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_runtime\u001b[38;5;241m.\u001b[39mdeserialize_cuda_engine(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrt_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_execution_context\u001b[49m()\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# By default set optimization profile to 0\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create_execution_context'"
     ]
    }
   ],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "whisper_trt_encoder = WhisperTRTEncoder(whisper_trt_encoder_engine, metadata, trt_config, batch_size=1)\n",
    "whisper_trt_decoder = WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size=1, num_beams=1)\n",
    "\n",
    "# # FP16\n",
    "# whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=1)\n",
    "# whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=1, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2967d4-8a94-4457-b6d4-ca20f564dd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e95db6-92a1-4d78-892f-d0571827feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, input_features, timing_profile)\n",
    "_, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n",
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = w_encoder_inference(whisper_trt_encoder_fp16, input_features, timing_profile)\n",
    "_, decoder_trt_time_fp16 = w_decoder_inference(whisper_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fad96-b479-4964-8708-5c8f86807c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7b765-dfae-4216-9020-3a8bca18decc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
