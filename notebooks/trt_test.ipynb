{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd0f338-383a-4862-a8f4-bad736cb5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")\n",
    "\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "from typing import BinaryIO, Union\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "def decode_audio(\n",
    "    input_file: Union[str, BinaryIO],\n",
    "    sampling_rate: int = 16000,\n",
    "    split_stereo: bool = False,\n",
    "):\n",
    "    \"\"\"Decodes the audio.\n",
    "\n",
    "    Args:\n",
    "      input_file: Path to the input file or a file-like object.\n",
    "      sampling_rate: Resample the audio to this sample rate.\n",
    "      split_stereo: Return separate left and right channels.\n",
    "\n",
    "    Returns:\n",
    "      A float32 Numpy array.\n",
    "\n",
    "      If `split_stereo` is enabled, the function returns a 2-tuple with the\n",
    "      separated left and right channels.\n",
    "    \"\"\"\n",
    "    resampler = av.audio.resampler.AudioResampler(\n",
    "        format=\"s16\",\n",
    "        layout=\"mono\" if not split_stereo else \"stereo\",\n",
    "        rate=sampling_rate,\n",
    "    )\n",
    "\n",
    "    raw_buffer = io.BytesIO()\n",
    "    dtype = None\n",
    "\n",
    "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
    "        frames = container.decode(audio=0)\n",
    "        frames = _ignore_invalid_frames(frames)\n",
    "        frames = _group_frames(frames, 500000)\n",
    "        frames = _resample_frames(frames, resampler)\n",
    "\n",
    "        for frame in frames:\n",
    "            array = frame.to_ndarray()\n",
    "            dtype = array.dtype\n",
    "            raw_buffer.write(array)\n",
    "\n",
    "    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n",
    "\n",
    "    # Convert s16 back to f32.\n",
    "    audio = audio.astype(np.float32) / 32768.0\n",
    "\n",
    "    if split_stereo:\n",
    "        left_channel = audio[0::2]\n",
    "        right_channel = audio[1::2]\n",
    "        return left_channel, right_channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _ignore_invalid_frames(frames):\n",
    "    iterator = iter(frames)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except av.error.InvalidDataError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def _group_frames(frames, num_samples=None):\n",
    "    fifo = av.audio.fifo.AudioFifo()\n",
    "\n",
    "    for frame in frames:\n",
    "        frame.pts = None  # Ignore timestamp check.\n",
    "        fifo.write(frame)\n",
    "\n",
    "        if num_samples is not None and fifo.samples >= num_samples:\n",
    "            yield fifo.read()\n",
    "\n",
    "    if fifo.samples > 0:\n",
    "        yield fifo.read()\n",
    "\n",
    "\n",
    "def _resample_frames(frames, resampler):\n",
    "    # Add None to flush the resampler.\n",
    "    for frame in itertools.chain(frames, [None]):\n",
    "        yield from resampler.resample(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1d0dc0-7eba-47e4-bbd3-74b5bc0f464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig,\n",
    "    AutoConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f968e2d-abd1-4689-81c7-d1814f47e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18a5d53-ef74-49e5-8222-75d2e294a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=decode_audio(\"korean_news.mp4\")\n",
    "duration = audio.shape[0] / 16000\n",
    "inputs = processor(audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137c95bd-7e94-441f-beb1-3aaa561cfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87d3d91-c633-4967-834f-438ac3d56ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder, TRTHFRunner\n",
    "TRT_KV=False\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7978d807-7abe-4bbb-a1bc-7c4737c64d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_encoder_engine = WhisperEncoderTRTEngine('./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine', metadata)\n",
    "whisper_trt_decoder_engine = WhisperDecoderTRTEngine('./models/openai/whisper-tiny/tensorrt/Whisper-tiny-decoder-with-lm-head.onnx-bs1.engine', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587c317c-869a-46c3-9481-3e37b6bac939",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77456dbe-7822-4daf-95a4-aec1452a202e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "388b9ee0-05d0-4c7c-8f5f-bcfcbdbef5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/31/2023-10:09:48] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[08/31/2023-10:09:51] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "whisper_trt_encoder = WhisperTRTEncoder(whisper_trt_encoder_engine, metadata, trt_config, batch_size=1)\n",
    "whisper_trt_decoder = WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size=1, num_beams=1)\n",
    "\n",
    "# # FP16\n",
    "# whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=1)\n",
    "# whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=1, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2967d4-8a94-4457-b6d4-ca20f564dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/jisu/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "#Get input_features\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360aa6c9-5ff3-41d5-b198-2b08064420b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 1\n",
    "batch_size = 1\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    WhisperModelTRTConfig.DECODER_START_TOKEN_ID,\n",
    ")\n",
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b7b6f6a-478e-4e83-9cc5-86be536db61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "# from HuggingFace transformers\n",
    "from transformers.generation_logits_process import (\n",
    "    LogitsProcessorList,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    ")\n",
    "from NNDF.tensorrt_utils import TRTNativeRunner\n",
    "from NNDF.general_utils import measure_python_inference_code\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    whisper_trt_decoder.config.decoder_start_token_id,\n",
    "    dtype=torch.int32,\n",
    ")\n",
    "forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\", no_timestamps=True)\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        SuppressTokensLogitsProcessor(whisper_trt_decoder.config.suppress_tokens),\n",
    "        SuppressTokensAtBeginLogitsProcessor(\n",
    "            whisper_trt_decoder.config.begin_suppress_tokens, decoder_input_ids.shape[-1]\n",
    "        ),\n",
    "        ForceTokensLogitsProcessor(forced_decoder_ids),\n",
    "    ]\n",
    ")\n",
    "\n",
    "decoder_input_ids = decoder_input_ids.to(\"cuda\")\n",
    "\n",
    "def _e2e():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "        decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    return decoder_output_greedy\n",
    "\n",
    "def _e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "        whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(\n",
    "            encoder_last_hidden_state\n",
    "        )\n",
    "        decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    return decoder_output_greedy    \n",
    "\n",
    "measurement_function = _e2e\n",
    "if isinstance(whisper_trt_decoder, TRTNativeRunner):\n",
    "    whisper_trt_decoder.set_return_device(\"cuda\")\n",
    "    measurement_function = _e2e_trt\n",
    "\n",
    "full_e2e_time = measure_python_inference_code(measurement_function, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d64e37-65e5-4cb7-ade0-8735884b151e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32083027000771835, 0.323539470002288]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_e2e_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07478b5-e4c8-4eb2-a7e6-34748d071d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32083027000771835, 0.323539470002288]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_e2e_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a9d1e6-9228-48ee-8107-2375ac50b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "min_length = WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[Whisper_VARIANT]\n",
    "decoder_input_ids = torch.full(\n",
    "    (batch_size, 1),\n",
    "    WhisperModelTRTConfig.DECODER_START_TOKEN_ID,\n",
    ")\n",
    "\n",
    "forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\", no_timestamps=True)\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(whisper_model.config.max_length)])\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        SuppressTokensLogitsProcessor(WhisperModelTRTConfig.SUPPRESS_TOKENS),\n",
    "        SuppressTokensAtBeginLogitsProcessor(\n",
    "            WhisperModelTRTConfig.BEGIN_SUPPRESS_TOKENS, decoder_input_ids.shape[-1]\n",
    "        ),\n",
    "        ForceTokensLogitsProcessor(forced_decoder_ids),\n",
    "    ]\n",
    ")  # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can get identical results with raw HuggingFace\n",
    "\n",
    "   \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda:1\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder.beam_search(\n",
    "                input_ids=decoder_input_ids.cuda(),\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eb5d1a7-d290-4644-85ee-f04c47eda831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3231202009774279, 0.32537682401016355]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e7822d3-8c6f-4980-8329-c0e31e696f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 324 ms, sys: 0 ns, total: 324 ms\n",
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n",
    "whisper_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "decoder_output = whisper_trt_decoder.greedy_search(\n",
    "    input_ids=decoder_input_ids.cuda(),\n",
    "    encoder_hidden_states=encoder_last_hidden_states,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=metadata.other.kv_cache,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e02ed1-04f7-4410-b0ce-7d63feb541a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70015ec-f4e1-484e-a0d8-c75cd1d93495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50259, 50359, 50363,  2221,    13,  2326,   388,   391,   307,\n",
       "           264, 50244,   295,   264,  2808,  5359,   293,   321,   366,  5404,\n",
       "           281,  2928,   702, 14943,    13, 50257]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146cb19-e5b5-4f41-82be-42e99c0d96fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe45b089-cfb4-4efd-a2b1-9487b3c94db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Whisper.trt.WhisperTRTDecoder at 0x7f1b4857e6d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_trt_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62d78144-85f5-45de-93bb-1cf93e0f2eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 ms, sys: 1.35 ms, total: 3.87 ms\n",
      "Wall time: 3.13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state = whisper_trt_encoder(input_features=input_features)\n",
    "decoder_output_greedy = whisper_trt_decoder.greedy_search(\n",
    "    input_ids=decoder_input_ids.cuda(),\n",
    "    encoder_hidden_states=encoder_last_hidden_state.cuda(),\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    logits_processor=logits_processor,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f4ed9bd-866f-4fe3-8c16-c81b1c5dc5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0650,  0.0488,  0.0229,  ...,  0.0255,  0.0470, -0.0124],\n",
       "         [-0.8294, -1.4169,  0.2431,  ...,  0.8398,  0.1603,  0.2906],\n",
       "         [-0.6758, -1.3836,  0.4196,  ...,  0.0107, -0.1264,  0.6475],\n",
       "         ...,\n",
       "         [ 0.7624, -1.6620,  1.0597,  ..., -0.8330,  0.1066,  0.6695],\n",
       "         [ 0.6750, -1.7347,  0.5413,  ..., -0.2869, -0.0160,  0.4132],\n",
       "         [ 0.2626, -0.1129, -1.3307,  ...,  0.2595, -0.4166, -0.0923]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_state.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e95db6-92a1-4d78-892f-d0571827feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-31 10:10:24,899][OSS][WARNING] Unable to execute program using cuda compatible device: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [2]\n",
      "[2023-08-31 10:10:24,900][OSS][WARNING] Retrying using CPU only.\n",
      "[2023-08-31 10:10:27,924][OSS][WARNING] Successfully obtained result using CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder time: p50 3.97ms, p99 4.97ms\n",
      "Decoder time: p50 3.41ms, p99 4.36ms\n",
      "Full E2E time: p50 252.95ms, p99 258.05ms\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = w_encoder_inference(whisper_trt_encoder, input_features, timing_profile)\n",
    "_, decoder_trt_time = w_decoder_inference(whisper_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        whisper_trt_encoder,\n",
    "        whisper_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fad96-b479-4964-8708-5c8f86807c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7b765-dfae-4216-9020-3a8bca18decc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0f489-9e73-4b45-a66e-5638bcccd926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01950d6-87f6-4903-ae0c-068d14449f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcfdda-2d30-4c70-8e71-185f30b2556c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04964b85-6346-458a-af28-2ba85205d959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173270d-7d09-4584-9f61-4ec6f85a4071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cadda5-4f7d-48a4-a3a3-bb5c3d23068e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639b68b-51af-4c30-9bc6-fa365cb23e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf2fbc-ffcb-4f2a-a94c-459fce0080e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e363ebd-507c-4af0-a450-eacadecf64e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f452db-6d50-4462-a6a4-bd1c57413d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd847765-758b-41f2-8de6-f36c683bbea7",
   "metadata": {},
   "source": [
    "# fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768b711-20bc-481c-bb5e-9d070668724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = w_encoder_inference(whisper_trt_encoder_fp16, input_features, timing_profile)\n",
    "_, decoder_trt_time_fp16 = w_decoder_inference(whisper_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_features,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        whisper_trt_encoder_fp16,\n",
    "        whisper_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
