{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace T5 Inference with TensorRT\n",
    "\n",
    "T5 is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding  different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch T5 model](https://huggingface.co/transformers/model_doc/t5.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace T5 model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5Config,\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_VARIANT = 't5-small' # choices: t5-small | t5-base | t5-large | t5-3b | t5-11b\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_VARIANT).to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_VARIANT)\n",
    "config = T5Config.from_pretrained(T5_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f96c37-9dc8-45ef-9873-a5df53a34684",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/t5-small/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(T5_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "t5_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0f7e1-c146-4fc2-a43c-98e0669e0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544dea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\")\n",
    "num_beams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1edf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    t5_model = t5_model.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    t5_model = t5_model.cuda()\n",
    "    inputs = inputs.to('cuda:0')\n",
    "input_ids = inputs.input_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13913fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on a single example\n",
    "t5_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = t5_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f7fd8b-2ee3-4d25-9204-7713eb7e90b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-07-21 18:20:21.131101: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "# Generate sequence for an input\n",
    "outputs = t5_model.generate(input_ids, num_beams=num_beams)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bc0b8-648c-40f9-ba2f-553a61b9551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/nvadmin/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:0')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1bb5e-3a35-43a4-ab2b-d8e9bee7c674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.measurements import decoder_inference, encoder_inference, full_inference\n",
    "from T5.export import T5EncoderTorchFile, T5DecoderTorchFile, T5EncoderTRTEngine, T5DecoderTRTEngine\n",
    "\n",
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search\n",
    "\n",
    "t5_torch_encoder = T5EncoderTorchFile.TorchModule(t5_model.encoder)\n",
    "t5_torch_decoder = T5DecoderTorchFile.TorchModule(\n",
    "    t5_model.decoder, t5_model.lm_head, t5_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db78e8-fddb-4e97-93b2-1b852de6317d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_inputs\n",
    "generated_ids = whisper_model.generate(inputs=audio_inputs.input_features.to('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87d401-7bad-48e8-ba98-2002dc5a512c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be755fbc-c53e-4f8d-a9c2-4817167cf93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.8 ms, sys: 391 µs, total: 58.2 ms\n",
      "Wall time: 57.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004584808018989861"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_torch_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/torch_utils.py:65\u001b[0m, in \u001b[0;36muse_cuda.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     new_kwargs \u001b[38;5;241m=\u001b[39m _send_args_to_device(caller_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# If a device has cuda installed but no compatible kernels, cuda.is_available() will still return True.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# This exception is necessary to catch remaining incompat errors.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m used_cuda:\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/measurements.py:56\u001b[0m, in \u001b[0;36mencoder_inference\u001b[0;34m(t5_encoder, input_ids, timing_profile, use_cuda)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@use_cuda\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_inference\u001b[39m(t5_encoder, input_ids, timing_profile, use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     encoder_stmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: t5_encoder(input_ids\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[0;32m---> 56\u001b[0m     encoder_e2e_time \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_python_inference_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_stmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiming_profile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (encoder_stmt(), encoder_e2e_time)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/general_utils.py:202\u001b[0m, in \u001b[0;36mmeasure_python_inference_code\u001b[0;34m(stmt, timing_profile)\u001b[0m\n\u001b[1;32m    196\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasuring inference call with warmup: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and number: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and iterations \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and duration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m secs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    198\u001b[0m         warmup, number, iterations, duration\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m warmup_mintime \u001b[38;5;241m=\u001b[39m \u001b[43mtimeit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarmup times: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(warmup_mintime))\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Actual measurement cycles\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/timeit.py:238\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(stmt, setup, timer, repeat, number, globals)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat\u001b[39m(stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, timer\u001b[38;5;241m=\u001b[39mdefault_timer,\n\u001b[1;32m    236\u001b[0m            repeat\u001b[38;5;241m=\u001b[39mdefault_repeat, number\u001b[38;5;241m=\u001b[39mdefault_number, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to create Timer object and call repeat method.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/timeit.py:205\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 205\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/measurements.py:55\u001b[0m, in \u001b[0;36mencoder_inference.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@use_cuda\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_inference\u001b[39m(t5_encoder, input_ids, timing_profile, use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 55\u001b[0m     encoder_stmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mt5_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     encoder_e2e_time \u001b[38;5;241m=\u001b[39m measure_python_inference_code(encoder_stmt, timing_profile)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (encoder_stmt(), encoder_e2e_time)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/export.py:245\u001b[0m, in \u001b[0;36mWhisperEncoderTorchFile.TorchModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/export.py:242\u001b[0m, in \u001b[0;36mWhisperEncoderTorchFile.TorchModule.forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_features = audio_inputs.input_features.to('cuda')\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "960f05fc-f572-4832-ad82-8a75823866b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.7 ms, sys: 0 ns, total: 96.7 ms\n",
      "Wall time: 96.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0075572499772533774"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.T5ModelConfig import T5ModelTRTConfig, T5Metadata\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db170b1d-dd6a-4b4f-8630-3d226549caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams=1\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": t5_model.get_encoder()(inputs.input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39d511cf-d963-4629-be54-22e9a258716d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['encoder_outputs'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/torch_utils.py:65\u001b[0m, in \u001b[0;36muse_cuda.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     new_kwargs \u001b[38;5;241m=\u001b[39m _send_args_to_device(caller_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# If a device has cuda installed but no compatible kernels, cuda.is_available() will still return True.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# This exception is necessary to catch remaining incompat errors.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m used_cuda:\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/T5/measurements.py:112\u001b[0m, in \u001b[0;36mfull_inference\u001b[0;34m(t5_encoder, t5_decoder, input_ids, tokenizer, timing_profile, max_length, min_length, num_beams, batch_size, use_cuda, early_stopping, use_cache)\u001b[0m\n\u001b[1;32m    108\u001b[0m measurement_function \u001b[38;5;241m=\u001b[39m _e2e\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# full_e2e_time = measure_python_inference_code(measurement_function, timing_profile)\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mmeasurement_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/T5/measurements.py:90\u001b[0m, in \u001b[0;36mfull_inference.<locals>._e2e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_e2e\u001b[39m():\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m         decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mt5_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt5_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt5_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBaseModelOutput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_last_hidden_state\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1207\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;66;03m# 0. Validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_class()\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# 1. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m bos_token_id \u001b[38;5;241m=\u001b[39m bos_token_id \u001b[38;5;28;01mif\u001b[39;00m bos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbos_token_id\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:909\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['encoder_outputs'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "decoder_output, _ = full_inference(\n",
    "    t5_torch_encoder,\n",
    "    t5_torch_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    num_beams=num_beams,\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e31cd6e0-4e95-4980-b895-346ccc6728b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "798be388-29f8-4e48-9318-fa1d8faeaae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_outputs': BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.0141,  0.1282,  0.0321,  ..., -0.0082,  0.0340,  0.0936],\n",
       "          [ 0.0616, -0.0724, -0.0481,  ...,  0.0811, -0.1882,  0.0876],\n",
       "          [ 0.0102, -0.0185,  0.0025,  ...,  0.0020, -0.0040, -0.0408],\n",
       "          ...,\n",
       "          [-0.1562, -0.0032, -0.1109,  ..., -0.0964, -0.0977, -0.1620],\n",
       "          [-0.0242, -0.1908, -0.0640,  ..., -0.0090, -0.2134, -0.1214],\n",
       "          [ 0.0859, -0.0072, -0.0470,  ..., -0.0105, -0.0869,  0.0403]]],\n",
       "        device='cuda:0', grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bab15-3ed8-4f62-99f1-df25d7c29826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cff48fc-b792-4852-b638-6e2c54099cb2",
   "metadata": {},
   "source": [
    "Let us decode the model's output back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "839bc6bc-65dc-499d-ac26-81456dbc1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "# De-tokenize output to raw text\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the T5 model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "from NNDF.networks import NetworkMetadata, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c50346f7-6c2c-4e4b-ba70-875688947b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(T5_VARIANT)\n",
    "\n",
    "metadata=NetworkMetadata(variant=T5_VARIANT, precision=Precision(fp16=True), other=T5Metadata(kv_cache=False))\n",
    "\n",
    "encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "!mkdir -p $encoder_onnx_model_path\n",
    "!mkdir -p $decoder_onnx_model_path\n",
    "\n",
    "encoder_onnx_model_fpath = T5_VARIANT + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath = T5_VARIANT + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "t5_encoder = T5EncoderTorchFile(t5_model.to('cpu'), metadata)\n",
    "t5_decoder = T5DecoderTorchFile(t5_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_t5_encoder = t5_encoder.as_onnx_model(\n",
    "    os.path.join(encoder_onnx_model_path, encoder_onnx_model_fpath), force_overwrite=False\n",
    ")\n",
    "onnx_t5_decoder = t5_decoder.as_onnx_model(\n",
    "    os.path.join(decoder_onnx_model_path, decoder_onnx_model_fpath), force_overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbe35e17-aaff-4218-bb5f-def711da061c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/openai/whisper-tiny/ONNX/encoder'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_onnx_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea945da2-e35f-44ff-b309-b763c8ba18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: command substitution: line 1: syntax error near unexpected token `encoder_onnx_model_path,'\n",
      "/bin/bash: command substitution: line 1: `os.path.join(encoder_onnx_model_path, \"openai\"))'\n",
      "/bin/bash: command substitution: line 1: syntax error near unexpected token `decoder_onnx_model_path,'\n",
      "/bin/bash: command substitution: line 1: `os.path.join(decoder_onnx_model_path, \"openai\"))'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WhisperModel' object has no attribute 'proj_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m whisper_decoder \u001b[38;5;241m=\u001b[39m WhisperDecoderTorchFile(whisper_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), metadata)\n\u001b[1;32m     18\u001b[0m onnx_t5_encoder \u001b[38;5;241m=\u001b[39m whisper_encoder\u001b[38;5;241m.\u001b[39mas_onnx_model(\n\u001b[1;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(encoder_onnx_model_path, encoder_onnx_model_fpath), force_overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m onnx_t5_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_onnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_onnx_model_fpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:320\u001b[0m, in \u001b[0;36mTorchModelFile.as_onnx_model\u001b[0;34m(self, output_fpath, converter, force_overwrite)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_overwrite \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_fpath):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter\u001b[38;5;241m.\u001b[39monnx_class(output_fpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_metadata)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_to_onnx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_metadata\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/export.py:333\u001b[0m, in \u001b[0;36mWhisperDecoderConverter.torch_to_onnx\u001b[0;34m(self, output_fpath, model, network_metadata)\u001b[0m\n\u001b[1;32m    330\u001b[0m simplified_encoder \u001b[38;5;241m=\u001b[39m WhisperEncoderTorchFile\u001b[38;5;241m.\u001b[39mTorchModule(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Exports to ONNX\u001b[39;00m\n\u001b[1;32m    332\u001b[0m decoder_with_lm_head \u001b[38;5;241m=\u001b[39m WhisperDecoderTorchFile\u001b[38;5;241m.\u001b[39mTorchModule(\n\u001b[0;32m--> 333\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_out\u001b[49m, model\u001b[38;5;241m.\u001b[39mconfig, is_trt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    334\u001b[0m )\n\u001b[1;32m    336\u001b[0m inputs \u001b[38;5;241m=\u001b[39m WhisperModelTRTConfig\u001b[38;5;241m.\u001b[39mget_input_dims(network_metadata)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m WhisperModelTRTConfig\u001b[38;5;241m.\u001b[39mget_output_dims(network_metadata)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WhisperModel' object has no attribute 'proj_out'"
     ]
    }
   ],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(Whisper_VARIANT)\n",
    "\n",
    "metadata=NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=False))\n",
    "\n",
    "encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "\n",
    "\n",
    "!mkdir -p $(os.path.join(encoder_onnx_model_path, \"openai\"))\n",
    "!mkdir -p $(os.path.join(decoder_onnx_model_path, \"openai\"))\n",
    "\n",
    "encoder_onnx_model_fpath = Whisper_VARIANT + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath = Whisper_VARIANT + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "whisper_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_t5_encoder = whisper_encoder.as_onnx_model(\n",
    "    os.path.join(encoder_onnx_model_path, encoder_onnx_model_fpath), force_overwrite=False\n",
    ")\n",
    "onnx_t5_decoder = whisper_decoder.as_onnx_model(\n",
    "    os.path.join(decoder_onnx_model_path, decoder_onnx_model_fpath), force_overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec19f6-2289-4da3-a06d-8fc6652f89cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032e174-e045-416e-9182-5e7fb83264c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "!mkdir -p $encoder_onnx_model_path\n",
    "!mkdir -p $decoder_onnx_model_path\n",
    "\n",
    "encoder_onnx_model_fpath = T5_VARIANT + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath = T5_VARIANT + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "t5_encoder = T5EncoderTorchFile(t5_model.to('cpu'), metadata)\n",
    "t5_decoder = T5DecoderTorchFile(t5_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_t5_encoder = t5_encoder.as_onnx_model(\n",
    "    os.path.join(encoder_onnx_model_path, encoder_onnx_model_fpath), force_overwrite=False\n",
    ")\n",
    "onnx_t5_decoder = t5_decoder.as_onnx_model(\n",
    "    os.path.join(decoder_onnx_model_path, decoder_onnx_model_fpath), force_overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.export import T5DecoderONNXFile, T5EncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_ids', min=(1, 1), opt=(1, 256), max=(1, 512))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorrt_model_path = './models/{}/tensorrt'.format(T5_VARIANT)\n",
    "!mkdir -p tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_sequence_length // 2),\n",
    "    max=(batch_size, max_sequence_length),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "encoder_engine_name = os.path.join(tensorrt_model_path, encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name = os.path.join(tensorrt_model_path, decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(encoder_onnx_model_path, encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(encoder_engine_name, metadata)\n",
    "\n",
    "if not os.path.exists(decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(decoder_onnx_model_path, decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(decoder_engine_name, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the T5 model, ready for us to carry out inference. \n",
    "\n",
    "#### Single example inference\n",
    "The T5 model with TensorRT backend can now be employed in place of the original HuggingFace T5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3954f2f4-c393-463b-a44b-3e5335032b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "from T5.trt import T5TRTEncoder, T5TRTDecoder\n",
    "\n",
    "t5_trt_encoder = T5TRTEncoder(\n",
    "                t5_trt_encoder_engine, metadata, config\n",
    "            )\n",
    "t5_trt_decoder = T5TRTDecoder(\n",
    "                t5_trt_decoder_engine, metadata, config, num_beams=num_beams\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9544ecb-2671-4b53-a544-08f13424cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a single sample\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)\n",
    "outputs = t5_trt_decoder(\n",
    "    expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d71a327-546f-4b5b-bd42-caaffcceafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequence for an input\n",
    "max_length = 64\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (1, 1), tokenizer.convert_tokens_to_ids(tokenizer.pad_token), dtype=torch.int32\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4a98-b034-470e-a9f8-096d4100b8d4",
   "metadata": {},
   "source": [
    "#### TRT engine inference benchmark: encoder and decoder stacks\n",
    "First, we will bechmark the encoder and decoder stacks as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70b37591-4398-40ff-8a39-5f75347192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.41 ms, sys: 4.36 ms, total: 13.8 ms\n",
      "Wall time: 12.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0009187690448015928"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_trt_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e5459da-a01b-4894-88dc-01b3637ded53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 ms, sys: 287 µs, total: 19.8 ms\n",
      "Wall time: 19.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0014965159934945405"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebfe03-7a60-4dd0-ad32-4e53d6012b07",
   "metadata": {},
   "source": [
    "### Full model inference benchmark\n",
    "\n",
    "Next, we will try the full TensorRT T5 engine for the task of translation. As before, note the time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f31cb550-24b9-48cd-a4ec-0bf18ac5e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n",
      "CPU times: user 111 ms, sys: 0 ns, total: 111 ms\n",
      "Wall time: 110 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008810858998913318"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "decoder_output, full_e2e_median_runtime = full_inference(\n",
    "    t5_trt_encoder,\n",
    "    t5_trt_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[metadata.variant],\n",
    "    num_beams=num_beams,\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))\n",
    "full_e2e_median_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f270-e7c8-4081-9587-2a3dc7ea95d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in upto ~10x performance improvement (from 0.0802s to 0.0082s for the T5-small variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch T5 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace T5 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [T5/trt.py](../T5/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8b7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4f958-15c8-4622-8ef3-d20cec1586f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5679d-bc83-4ed1-9d5f-e97b6c0c77a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24c287-8443-4290-9fad-dc442bd80a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
