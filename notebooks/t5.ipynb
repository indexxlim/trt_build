{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace T5 Inference with TensorRT\n",
    "\n",
    "T5 is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding  different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch T5 model](https://huggingface.co/transformers/model_doc/t5.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace T5 model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5Config,\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_VARIANT = 't5-small' # choices: t5-small | t5-base | t5-large | t5-3b | t5-11b\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_VARIANT).to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_VARIANT)\n",
    "t5_config = T5Config.from_pretrained(T5_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f96c37-9dc8-45ef-9873-a5df53a34684",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/t5-small/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(T5_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "t5_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01a8db2-d27b-46d6-8614-ee0e476dd1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_config.max_source_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0f7e1-c146-4fc2-a43c-98e0669e0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "544dea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\")\n",
    "num_beams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed1edf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    t5_model = t5_model.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    t5_model = t5_model.cuda()\n",
    "    inputs = inputs.to('cuda:0')\n",
    "input_ids = inputs.input_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13913fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on a single example\n",
    "t5_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = t5_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f7fd8b-2ee3-4d25-9204-7713eb7e90b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-08-03 18:04:43.488813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "# Generate sequence for an input\n",
    "outputs = t5_model.generate(input_ids, num_beams=num_beams)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bc0b8-648c-40f9-ba2f-553a61b9551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/nvadmin/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:0')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37005780-f1b4-4643-8185-90366abda4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c71b9b7-0d14-47ee-9b21-89654c2497ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.model.encoder(input_features=input_features)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.measurements import decoder_inference, encoder_inference, full_inference\n",
    "from T5.export import T5EncoderTorchFile, T5DecoderTorchFile, T5EncoderTRTEngine, T5DecoderTRTEngine\n",
    "\n",
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search\n",
    "\n",
    "t5_torch_encoder = T5EncoderTorchFile.TorchModule(t5_model.encoder)\n",
    "t5_torch_decoder = T5DecoderTorchFile.TorchModule(\n",
    "    t5_model.decoder, t5_model.lm_head, t5_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = whisper_model.generate(inputs=audio_inputs.input_features.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be755fbc-c53e-4f8d-a9c2-4817167cf93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.5 ms, sys: 0 ns, total: 59.5 ms\n",
      "Wall time: 59.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00469854602124542"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_torch_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "573dc690-7643-42bc-9221-d22dc7606fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94.2 ms, sys: 0 ns, total: 94.2 ms\n",
      "Wall time: 93.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00756344199180603"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585f4eb-b6ce-455e-8352-f90c6228a719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.4 ms, sys: 0 ns, total: 34.4 ms\n",
      "Wall time: 33.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0025924149667844176"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_features = audio_inputs.input_features.to('cuda')\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39699019-4c5b-4306-854d-9a48bb2c678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3b48f-a8e2-4ca2-b880-8fdb91208f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae7f4a-c83b-446d-a1f9-1d6d2fbfbe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 ms, sys: 1.34 ms, total: 51 ms\n",
      "Wall time: 50.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003986836993135512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = w_decoder_inference(\n",
    "    whisper_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f05fc-f572-4832-ad82-8a75823866b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.T5ModelConfig import T5ModelTRTConfig, T5Metadata\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc88907-7d1f-4453-8863-5425f7ddc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac38df99-e191-4d0e-8096-40604a0d078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.23.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39d511cf-d963-4629-be54-22e9a258716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.8 ms, sys: 531 µs, total: 37.3 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "decoder_output, _ = full_inference(\n",
    "    t5_torch_encoder,\n",
    "    t5_torch_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    num_beams=num_beams,\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0be1f845-f574-4180-9fbf-706f3a9aa502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "\"Let us decode the model's output back into text.\"\n",
    "# De-tokenize output to raw text\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ad775-a312-4d4b-bc65-a93e0094ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5ac34d4-efd4-46b0-a142-397b7bbe6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3410dbdc-91a4-48c8-8acf-b13b51358689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.general_utils import measure_python_inference_code\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT).cuda()\n",
    "\n",
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = processor.tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "# if not cuda_114_mode:\n",
    "whisper_model= whisper_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08a80940-4ff5-40d4-a82f-35a5cc1de908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "HF_KV=True\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "whisper_model.float()\n",
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = w_encoder_inference(whisper_torch_encoder, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time = w_decoder_inference(whisper_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# # FP16\n",
    "# if not cuda_114_mode:\n",
    "whisper_model.half()\n",
    "input_features= input_features.half()\n",
    "whisper_torch_encoder_fp16 = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder_fp16 = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = w_encoder_inference(whisper_torch_encoder_fp16, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = w_decoder_inference(whisper_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d20e591-4a14-497b-9279-931b26e4386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda94dca-266c-45bb-ad56-9c1de5c03158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "989f32e5-6ca9-4609-b5f7-d300a3919f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 Output identical to HF results? False\n",
      "PyTorch FP16 Output identical to HF results? False\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "Encoder time: 0.0021469868952408433\n",
      "Decoder time: 0.003236535005271435\n",
      "Full E2E time: 0.08386038697790354\n",
      "Precision: FP16, Number of Beams: 1\n",
      "Encoder time: 0.003080081893131137\n",
      "Decoder time: 0.0033120280131697655\n",
      "Full E2E time: 0.08407521108165383\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time_fp16}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time_fp16}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d128bd1f-a8b5-4fd4-9163-e38085877537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3663c68-147d-4ead-b3ae-c9d4f2aba230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"!<|startoftranscript|>.<|translate|><|notimestamps|> Mr. Kilder is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(output_ids_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b105ef60-4141-4058-be58-0b2d268dadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4788-c322-4f9f-b4e3-e0068a95235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the T5 model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "from NNDF.networks import NetworkMetadata, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c50346f7-6c2c-4e4b-ba70-875688947b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py:729: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(T5_VARIANT)\n",
    "\n",
    "t5_metadata=NetworkMetadata(variant=T5_VARIANT, precision=Precision(fp16=True), other=T5Metadata(kv_cache=False))\n",
    "\n",
    "t5_encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "t5_decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "!mkdir -p $t5_encoder_onnx_model_path\n",
    "!mkdir -p $t5_decoder_onnx_model_path\n",
    "\n",
    "t5_encoder_onnx_model_fpath = T5_VARIANT + \"-encoder.onnx\"\n",
    "t5_decoder_onnx_model_fpath = T5_VARIANT + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "t5_encoder = T5EncoderTorchFile(t5_model.to('cpu'), t5_metadata)\n",
    "t5_decoder = T5DecoderTorchFile(t5_model.to('cpu'), t5_metadata)\n",
    "\n",
    "onnx_t5_encoder = t5_encoder.as_onnx_model(\n",
    "    os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath), force_overwrite=True\n",
    ")\n",
    "onnx_t5_decoder = t5_decoder.as_onnx_model(\n",
    "    os.path.join(t5_decoder_onnx_model_path, t5_decoder_onnx_model_fpath), force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa4985-cf88-4aab-a1d8-3109113e108d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea945da2-e35f-44ff-b309-b763c8ba18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(Whisper_VARIANT)\n",
    "\n",
    "wh_metadata=NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=False))\n",
    "\n",
    "wh_encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "wh_decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "\n",
    "\n",
    "!mkdir -p $wh_encoder_onnx_model_path\n",
    "!mkdir -p $wh_decoder_onnx_model_path\n",
    "\n",
    "wh_encoder_onnx_model_fpath = Whisper_VARIANT.split('/')[1] + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = Whisper_VARIANT.split('/')[1] + \"-decoder-with-lm-head.onnx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4f25b5b-1aa1-4fc0-be6e-f17d78b7c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:198: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:237: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:742: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:72: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n"
     ]
    }
   ],
   "source": [
    "whisper_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), wh_metadata)\n",
    "whisper_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), wh_metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_encoder.as_onnx_model(\n",
    "    os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=True\n",
    ")\n",
    "onnx_whisper_decoder = whisper_decoder.as_onnx_model(\n",
    "    os.path.join(wh_decoder_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca2edb0f-e3ed-44d9-9da2-6b232c6f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NNDF.tensorrt_utils import OnnxProcessOperation, process_onnx, move_t5_cast_op\n",
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)\n",
    "# #output_fpath = os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath)\n",
    "# config = [OnnxProcessOperation.MOVE_CAST_OP2, OnnxProcessOperation.CLAMP_WEIGHTS]\n",
    "\n",
    "# import onnx_graphsurgeon as gs\n",
    "# import onnx\n",
    "# import numpy as np\n",
    "# graph = gs.import_onnx(onnx.load(output_fpath))\n",
    "# folder = os.path.split(output_fpath)[0]\n",
    "# for op in config:\n",
    "#     if op == OnnxProcessOperation.CLAMP_WEIGHTS:\n",
    "#         graph = clamp_weights_onnx_to_fp16_bounds(graph, **kwargs)\n",
    "#     elif op == OnnxProcessOperation.MOVE_CAST_OP2:\n",
    "#         graph = move_t5_cast_op(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2e046-861b-41f0-a8ad-373fa6221a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45bc60fe-ef72-4351-ab7f-efadfdaf35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)\n",
    "# converter= ModelFileConverter\n",
    "# force_overwrite= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0f5bbae-4ca9-406e-9800-a82d0c26d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter.torch_to_onnx(\n",
    "#     output_fpath, self.load_model(), self.network_metadata\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d689d0bd-2dd7-4790-99f2-6ee04fc09bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperEncoderConverter\n",
    "from NNDF.models import ONNXModelFile\n",
    "\n",
    "from NNDF.networks import NetworkMetadata, Precision, Dims\n",
    "from NNDF.models import ModelFileConverter\n",
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "\n",
    "\n",
    "network_metadata_cp_dct = wh_metadata._asdict()\n",
    "del network_metadata_cp_dct[\"precision\"]\n",
    "network_metadata = NetworkMetadata(\n",
    "    **network_metadata_cp_dct, precision=Precision(fp16=False)\n",
    ")\n",
    "ModelFileConverter(\n",
    "    WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine\n",
    ")\n",
    "suconverter = ModelFileConverter(WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine)\n",
    "# suconverter.onnx_to_trt(\n",
    "#     output_fpath, fpath, network_metadata, profiles, preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0742c7d7-8ed4-4b83-9234-153c5e937aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74eaedf8-fb67-4dc0-8d90-e619b98d25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2611d763-4809-422c-8fc9-1894b9d47319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = model.device\n",
    "# input_features = torch.ones(1, 80, 1500).to(device)\n",
    "# simplified_encoder = WhisperEncoderTorchFile.TorchModule(model.model.encoder)\n",
    "# inputs = WhisperModelTRTConfig.get_input_dims(network_metadata)[\"encoder\"]\n",
    "# outputs = WhisperModelTRTConfig.get_output_dims(network_metadata)[\"encoder\"]\n",
    "\n",
    "# # Exports to ONNX\n",
    "# opt_args = {}\n",
    "\n",
    "# version_major = int((torch.__version__).split(\".\")[0])\n",
    "# version_minor = int((torch.__version__).split(\".\")[1])\n",
    "# if version_major < 1 or (version_major == 1 and version_minor < 11):\n",
    "#     opt_args[\"use_external_data_format\"] = True\n",
    "# torch.onnx.export(\n",
    "#     simplified_encoder,\n",
    "#     input_features,\n",
    "#     output_fpath,\n",
    "#     do_constant_folding=True,\n",
    "#     opset_version=13,\n",
    "#     input_names=inputs.get_names(),\n",
    "#     output_names=outputs.get_names(),\n",
    "#     dynamic_axes={\n",
    "#         **inputs.get_torch_dynamic_axis_encoding(),\n",
    "#         **outputs.get_torch_dynamic_axis_encoding(),\n",
    "#     },\n",
    "#     training=torch.onnx.TrainingMode.EVAL,\n",
    "#     **opt_args,\n",
    "# )\n",
    "\n",
    "# if network_metadata.precision.fp16:\n",
    "#     process_onnx(\n",
    "#         [OnnxProcessOperation.MOVE_CAST_OP2, OnnxProcessOperation.CLAMP_WEIGHTS],\n",
    "#         output_fpath,\n",
    "#         output_fpath,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e2f0314-0667-4e1d-88d4-8d95116a4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_ids = torch.tensor([[42] * 10])\n",
    "# input_features = torch.ones(1, 80, 3000)\n",
    "# # Exporting the decoder requires a basic instance of the encoder\n",
    "# # Create one temporarily\n",
    "# simplified_encoder = WhisperEncoderTorchFile.TorchModule(model.get_encoder())\n",
    "# # Exports to ONNX\n",
    "# decoder_with_lm_head = WhisperDecoderTorchFile.TorchModule(\n",
    "#     model.get_decoder(), model.proj_out, model.config\n",
    "# )\n",
    "\n",
    "# inputs = WhisperModelTRTConfig.get_input_dims(network_metadata)[\"decoder\"]\n",
    "# outputs = WhisperModelTRTConfig.get_output_dims(network_metadata)[\"decoder\"]\n",
    "\n",
    "# # Exports to ONNX\n",
    "# opt_args = {}\n",
    "\n",
    "# version_major = int((torch.__version__).split(\".\")[0])\n",
    "# version_minor = int((torch.__version__).split(\".\")[1])\n",
    "# if version_major < 1 or (version_major == 1 and version_minor < 11):\n",
    "#     opt_args[\"use_external_data_format\"] = True\n",
    "\n",
    "# encoder_hidden_states = simplified_encoder(input_features)\n",
    "# decoder_output = decoder_with_lm_head(\n",
    "#     input_ids[:, :-1], encoder_hidden_states\n",
    "# )  # decoder output at t-1 step (logits, past_key_values from 0 to t-1)\n",
    "# past_key_values = decoder_output[1]\n",
    "\n",
    "# decoder_root, decoder_fullname = os.path.split(output_fpath)\n",
    "# # Split kv and non kv onnx into separate folders to avoid weight overlap\n",
    "\n",
    "# non_kv_root = os.path.join(decoder_root, \"non-kv\")\n",
    "# kv_root = os.path.join(decoder_root, \"kv\")\n",
    "# decoder_name, decoder_ext = os.path.splitext(decoder_fullname)\n",
    "# non_kv_fpath = os.path.join(\n",
    "#     non_kv_root, decoder_name + \"-non-kv\" + decoder_ext\n",
    "# )\n",
    "# kv_fpath = os.path.join(kv_root, decoder_fullname)\n",
    "\n",
    "# # This code allows for huggingface compatible torch class to use onnx exporter (change just before onnx.export)\n",
    "# old_forward = decoder_with_lm_head.forward\n",
    "\n",
    "# def _export_forward(input_ids, encoder_hidden_states, past_key_values):\n",
    "#     result = old_forward(\n",
    "#         input_ids, encoder_hidden_states, past_key_values=past_key_values\n",
    "#     )\n",
    "#     return (result[0], result[1])\n",
    "\n",
    "# decoder_with_lm_head.forward = _export_forward\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     decoder_with_lm_head,\n",
    "#     (input_ids[:, -1:], encoder_hidden_states, past_key_values),\n",
    "#     # (1) input_ids should be the t token (last one) while past_key_values is 0 to t-1 caches\n",
    "#     # (2) since past_key_values is kwargs, ideally use \"(input_ids[:,-1:], encoder_hidden_states, {\"past_key_values\": past_key_values})\",\n",
    "#     # but onnx.export seems to unable to take kwargs properly (although PyTorch 1.11 claims it supports already).\n",
    "#     # Therefore, we need to wrap inside _export_forward() and make past_key_values indeed a kwargs\n",
    "#     kv_fpath,\n",
    "#     export_params=True,\n",
    "#     opset_version=12,\n",
    "#     input_names=inputs.get_names(),\n",
    "#     output_names=outputs.get_names(),\n",
    "#     dynamic_axes={\n",
    "#         **inputs.get_torch_dynamic_axis_encoding(),\n",
    "#         **outputs.get_torch_dynamic_axis_encoding(),\n",
    "#     },\n",
    "#     training=torch.onnx.TrainingMode.EVAL,\n",
    "#     **opt_args,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4dc756b-0379-467b-8c47-941587ac1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # # dual-engine approach: also export non-kv onnx model. Note that this is different from the original \"non-kv\" model. This one traces the `use_cache` path and have present_key_values output\n",
    "    # def _export_forward(input_ids, encoder_hidden_states, use_cache):\n",
    "    #     result = old_forward(\n",
    "    #         input_ids, encoder_hidden_states, use_cache=use_cache\n",
    "    #     )\n",
    "    #     return (result[0], result[1])\n",
    "\n",
    "    # decoder_with_lm_head.forward = _export_forward\n",
    "\n",
    "    # # inputs are same as non-kv model\n",
    "    # # outputs are same as kv model\n",
    "    # dict_inputs = inputs.get_dims()\n",
    "    # dict_inputs_non_kv = OrderedDict(\n",
    "    #     {k: dict_inputs[k] for k in [\"input_ids\", \"encoder_hidden_states\"]}\n",
    "    # )\n",
    "    # inputs_non_kv = Dims(dict_inputs_non_kv)\n",
    "\n",
    "    # torch.onnx.export(\n",
    "    #     decoder_with_lm_head,\n",
    "    #     (input_ids[:, -1:], encoder_hidden_states, True),\n",
    "    #     non_kv_fpath,\n",
    "    #     export_params=True,\n",
    "    #     opset_version=12,\n",
    "    #     input_names=inputs_non_kv.get_names(),\n",
    "    #     output_names=outputs.get_names(),\n",
    "    #     dynamic_axes={\n",
    "    #         **inputs_non_kv.get_torch_dynamic_axis_encoding(),\n",
    "    #         **outputs.get_torch_dynamic_axis_encoding(),\n",
    "    #     },\n",
    "    #     training=torch.onnx.TrainingMode.EVAL,\n",
    "    #     **opt_args,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b1fd7ce-d39b-4142-9a05-647143518d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "TRT_KV = False\n",
    "\n",
    "wh_onnx_model_path = './models/{}/onnx'.format(Whisper_VARIANT)\n",
    "!mkdir -p $wh_onnx_model_path\n",
    "\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config = WhisperModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "wh_encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = WhisperModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "wh_encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder_fp16 = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), force_overwrite=False)\n",
    "onnx_whisper_decoder_fp16 = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.export import T5DecoderONNXFile, T5EncoderONNXFile\n",
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_ids', min=(1, 1), opt=(1, 256), max=(1, 512))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tensorrt_model_path = './models/{}/tensorrt'.format(T5_VARIANT)\n",
    "!mkdir -p t5_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_sequence_length // 2),\n",
    "    max=(batch_size, max_sequence_length),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "t5_encoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "t5_decoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(t5_encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        t5_encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(t5_encoder_engine_name, t5_metadata)\n",
    "\n",
    "if not os.path.exists(t5_decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(t5_decoder_onnx_model_path, t5_decoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        t5_decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(t5_decoder_engine_name, t5_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "572b2d68-4004-4724-abd4-079c5487e345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_features', min=(1, 80, 1500), opt=(1, 80, 3000), max=(1, 80, 3000))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 1500),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9514a333-83eb-4654-b74c-4586a91ec7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name):\n",
    "    whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name):\n",
    "    whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efba83df-ffd5-4ea5-b74d-070d045d83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16\n",
    "wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name_fp16):\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name_fp16):\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a007d3-67d9-4096-aa38-b2ca2cc4a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3ab2316-4f8e-470f-b756-b0bf8d5b327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_profiles = [\n",
    "#             Profile().add(\n",
    "#                 \"input_features\",\n",
    "#                 min=(batch_size, 1),\n",
    "#                 opt=(batch_size, opt_input_seq_len),\n",
    "#                 max=(batch_size, max_input_length),\n",
    "#             )\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e7eafad-a81a-47e4-bf5b-f3beb69d1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine\n",
      "./models/openai/whisper-tiny/tensorrt/Whisper-tiny-decoder-with-lm-head.onnx-bs1.engine\n"
     ]
    }
   ],
   "source": [
    "from Whisper.export import WhisperEncoderConverter, WhisperDecoderConverter\n",
    "from NNDF.models import ONNXModelFile\n",
    "\n",
    "from NNDF.networks import NetworkMetadata, Precision, Dims\n",
    "from NNDF.models import ModelFileConverter\n",
    "\n",
    "print(wh_encoder_engine_name)\n",
    "print(wh_decoder_engine_name)\n",
    "\n",
    "model = os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath) \n",
    "# encoder convert to tensorrt\n",
    "onmf = ONNXModelFile(model, WhisperEncoderConverter, wh_metadata)\n",
    "\n",
    "output_fpath = wh_encoder_engine_name\n",
    "profiles=[encoder_profile]\n",
    "preview_features=preview_features\n",
    "converter = onmf.default_converter\n",
    "fpath = model\n",
    "\n",
    "network_metadata_cp_dct = wh_metadata._asdict()\n",
    "del network_metadata_cp_dct[\"precision\"]\n",
    "network_metadata = NetworkMetadata(\n",
    "    **network_metadata_cp_dct, precision=Precision(fp16=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63df680-95b5-4d1e-91a3-febe7159eb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bea2fb89-7bb5-4523-888c-dc213860c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WhisperEncoderTRTEngine\n",
    "onnx_class = WhisperEncoderTorchFile\n",
    "torch_class = WhisperEncoderONNXFile\n",
    "trt_engine_class = WhisperEncoderTRTEngine\n",
    "\n",
    "from polygraphy.backend.trt import CreateConfig\n",
    "from tensorrt import PreviewFeature, MemoryPoolType\n",
    "\n",
    "# polygraphy\n",
    "from polygraphy.backend.trt import (\n",
    "    network_from_onnx_path,\n",
    "    engine_from_network,\n",
    "    save_engine,\n",
    "    Profile,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1a704c4-4a6a-4714-91d7-60ef9ed07df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trt_engine_class(output_fpath, network_metadata)\n",
    "\n",
    "trt_inference_config = CreateConfig(\n",
    "    tf32=True,\n",
    "    fp16=network_metadata.precision.fp16,\n",
    "    memory_pool_limits = {MemoryPoolType.WORKSPACE: result.max_trt_workspace * 1024 * 1024},\n",
    "    profiles=profiles,\n",
    "    precision_constraints=(\"obey\" if result.use_obey_precision_constraints() else None),\n",
    "    preview_features=preview_features\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ea5fe-0d14-426b-92ca-efe73af8200c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcca62-184f-4327-8064-dbe0e62b24e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62fc6f30-c83c-47a1-961a-544f21bd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import util as trt_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052a0b3-239b-48b9-aadc-afd63875a5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad7714a0-a789-452b-8312-5b778cf537bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I]     Configuring with profiles: [Profile().add('input_features', min=(1, 80, 3000), opt=(1, 80, 3000), max=(1, 80, 3000))]\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layer_norm/ReduceMean': 'FLOAT', 'encoder/layer_norm/Pow': 'FLOAT', 'encoder/layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layer_norm/Add': 'FLOAT', 'encoder/layer_norm/Sqrt': 'FLOAT', 'encoder/layer_norm/Div': 'FLOAT', 'encoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | [TF32]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 512.00 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 14.547 seconds\u001b[0m\n",
      "[I] Saving engine to ./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt.tensorrt.ICudaEngine at 0x7fca695021f0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_definition = result.get_network_definition(network_from_onnx_path(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath)))\n",
    "#network_definition[1].get_input(0).name='input_features'\n",
    "trt_engine = engine_from_network(\n",
    "    network_definition, config=trt_inference_config\n",
    ")\n",
    "save_engine(trt_engine, output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5fbd7-b589-41b6-b558-6cc89f5feb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "734f1ccf-acf0-4e4b-974f-648d97f8381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder tensorrt\n",
    "model = os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath) \n",
    "# encoder convert to tensorrt\n",
    "onmf = ONNXModelFile(model, WhisperDecoderConverter, wh_metadata)\n",
    "\n",
    "output_fpath = wh_decoder_engine_name\n",
    "profiles=[decoder_profile]\n",
    "preview_features=preview_features\n",
    "converter = onmf.default_converter\n",
    "fpath = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "faded129-f045-43d3-9881-3a2db87cbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "= trt_engine_class(output_fpath, network_metadata)\n",
    "\n",
    "trt_inference_config = CreateConfig(\n",
    "    tf32=True,\n",
    "    fp16=network_metadata.precision.fp16,\n",
    "    memory_pool_limits = {MemoryPoolType.WORKSPACE: result.max_trt_workspace * 1024 * 1024},\n",
    "    profiles=profiles,\n",
    "    precision_constraints=(\"obey\" if result.use_obey_precision_constraints() else None),\n",
    "    preview_features=preview_features\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "498a5ed0-06fc-411e-a2ff-ddd10e82bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I]     Configuring with profiles: [Profile().add('input_ids', min=(1, 1), opt=(1, 192), max=(1, 384)).add('encoder_hidden_states', min=(1, 1, 384), opt=(1, 192, 384), max=(1, 384, 384))]\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'/decoder/Cast_2': 'FLOAT', '/decoder/Cast_3': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/final_layer_norm/Add': 'FLOAT', '/decoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/final_layer_norm/Div': 'FLOAT', '/decoder/layers.0/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/final_layer_norm/Add': 'FLOAT', '/decoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/final_layer_norm/Div': 'FLOAT', '/decoder/layers.1/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/final_layer_norm/Add': 'FLOAT', '/decoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/final_layer_norm/Div': 'FLOAT', '/decoder/layers.2/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/final_layer_norm/Add': 'FLOAT', '/decoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/final_layer_norm/Div': 'FLOAT', '/decoder/layers.3/final_layer_norm/Mul': 'FLOAT', '/decoder/layer_norm/ReduceMean': 'FLOAT', '/decoder/layer_norm/Pow': 'FLOAT', '/decoder/layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layer_norm/Add': 'FLOAT', '/decoder/layer_norm/Sqrt': 'FLOAT', '/decoder/layer_norm/Div': 'FLOAT', '/decoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | [TF32]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 512.00 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 50.006 seconds\u001b[0m\n",
      "[I] Saving engine to ./models/openai/whisper-tiny/tensorrt/Whisper-tiny-decoder-with-lm-head.onnx-bs1.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt.tensorrt.ICudaEngine at 0x7fca6950ae30>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_definition = result.get_network_definition(network_from_onnx_path(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath)))\n",
    "#network_definition[1].get_input(0).name='input_features'\n",
    "trt_engine = engine_from_network(\n",
    "    network_definition, config=trt_inference_config\n",
    ")\n",
    "save_engine(trt_engine, output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873ac2a-fbda-40c8-8646-f15eae2284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "033c3627-d054-4bec-8e09-a6daf145021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper-tiny-encoder.onnx\n",
      "Whisper-tiny-decoder-with-lm-head.onnx\n",
      "<Whisper.export.WhisperEncoderTorchFile object at 0x7f17bc6ac8b0>\n",
      "<Whisper.export.WhisperDecoderTorchFile object at 0x7f17bc54c040>\n"
     ]
    }
   ],
   "source": [
    "print(wh_encoder_onnx_model_fpath)\n",
    "print(wh_decoder_onnx_model_fpath)\n",
    "print(onnx_whisper_encoder)\n",
    "print(onnx_whisper_decoder)\n",
    "#onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "#onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c153b-a0fe-414c-8aa9-7b538d2e44aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "028ad5bd-0eb7-4397-8e5c-9387751516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(encoder_onnx_model_path, t5_encoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(encoder_engine_name, t5_metadata)\n",
    "\n",
    "if not os.path.exists(decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(decoder_onnx_model_path, t5_decoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(decoder_engine_name, t5_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the T5 model, ready for us to carry out inference. \n",
    "\n",
    "#### Single example inference\n",
    "The T5 model with TensorRT backend can now be employed in place of the original HuggingFace T5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3954f2f4-c393-463b-a44b-3e5335032b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "from T5.trt import T5TRTEncoder, T5TRTDecoder\n",
    "\n",
    "t5_trt_encoder = T5TRTEncoder(\n",
    "                t5_trt_encoder_engine, t5_metadata, t5_config\n",
    "            )\n",
    "t5_trt_decoder = T5TRTDecoder(\n",
    "                t5_trt_decoder_engine, t5_metadata, t5_config, num_beams=num_beams\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9544ecb-2671-4b53-a544-08f13424cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a single sample\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)\n",
    "outputs = t5_trt_decoder(\n",
    "    expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d71a327-546f-4b5b-bd42-caaffcceafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequence for an input\n",
    "max_length = 64\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (1, 1), tokenizer.convert_tokens_to_ids(tokenizer.pad_token), dtype=torch.int32\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4a98-b034-470e-a9f8-096d4100b8d4",
   "metadata": {},
   "source": [
    "#### TRT engine inference benchmark: encoder and decoder stacks\n",
    "First, we will bechmark the encoder and decoder stacks as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70b37591-4398-40ff-8a39-5f75347192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.86 ms, sys: 3.14 ms, total: 13 ms\n",
      "Wall time: 12.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0009066950296983123"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_trt_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e5459da-a01b-4894-88dc-01b3637ded53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 ms, sys: 0 ns, total: 19.3 ms\n",
      "Wall time: 19 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0014868349535390735"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35624c0-7726-4166-9514-f11657722f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62ebfe03-7a60-4dd0-ad32-4e53d6012b07",
   "metadata": {},
   "source": [
    "### Full model inference benchmark\n",
    "\n",
    "Next, we will try the full TensorRT T5 engine for the task of translation. As before, note the time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f31cb550-24b9-48cd-a4ec-0bf18ac5e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n",
      "CPU times: user 13.3 ms, sys: 0 ns, total: 13.3 ms\n",
      "Wall time: 13.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "decoder_output, full_e2e_median_runtime = full_inference(\n",
    "    t5_trt_encoder,\n",
    "    t5_trt_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[t5_metadata.variant],\n",
    "    num_beams=num_beams,\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))\n",
    "full_e2e_median_runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in upto ~10x performance improvement (from 0.0802s to 0.0082s for the T5-small variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch T5 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace T5 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [T5/trt.py](../T5/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2422-c55d-4d02-85f8-4e2f659e0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c456c-d1b1-4a62-890e-e3bffcacf436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5da0a58-fbc1-41ce-be96-358cdca01f9a",
   "metadata": {},
   "source": [
    "# Whisper Tensorrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0bfb51e-84cc-42cd-87ea-20a9195e4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder, TRTHFRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63591388-3a8d-416a-ae18-d6be56fe818e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e92c5a-8b9b-4af4-a433-4520e63f1c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c11bc6fa-1441-4b11-b605-bbcfd40c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(Whisper_VARIANT, use_cache = metadata.other.kv_cache)\n",
    "\n",
    "# FP32\n",
    "whisper_trt_encoder = WhisperTRTEncoder(whisper_trt_encoder_engine, metadata, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder = WhisperTRTDecoder(whisper_trt_decoder_engine, metadata, trt_config, batch_size=batch_size, num_beams=num_beams)\n",
    "\n",
    "# FP16\n",
    "whisper_trt_encoder_fp16 = WhisperTRTEncoder(whisper_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size)\n",
    "whisper_trt_decoder_fp16 = WhisperTRTDecoder(whisper_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77f7a737-0cf9-45c0-b652-2530278c4a6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trt_engine_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wte \u001b[38;5;241m=\u001b[39m TRTHFRunner(\u001b[43mtrt_engine_file\u001b[49m, network_metadata, hf_config, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trt_engine_file' is not defined"
     ]
    }
   ],
   "source": [
    "wte = TRTHFRunner(trt_engine_file, network_metadata, hf_config, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2492d4b-36e6-4cec-b5e2-8c8f76dc50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a17f7e65-338d-4f47-93b8-81ff54665ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_encoder.get_optimization_profile(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e4447420-61ee-49d9-9e84-68fd184f1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c7a49ecf-6f31-414d-8fdc-3d0b1283d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_logger = trt.Logger()\n",
    "trt_runtime = trt.Runtime(trt_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4d24a84-048a-4cc3-8e98-7a7c444f28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(whisper_trt_encoder_engine.fpath, \"rb\") as f:\n",
    "    trt_logger = trt.Logger()\n",
    "    trt_runtime = trt.Runtime(trt_logger)\n",
    "    trt_engine = trt_runtime.deserialize_cuda_engine(f.read())\n",
    "    trt_context = trt_engine.create_execution_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33a35fbb-20c2-4798-a0c2-ab409afba10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_engine.num_optimization_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa2dba31-ce3d-4a5a-94cd-75f6ad43db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3932618/2412779385.py:1: DeprecationWarning: Use set_optimization_profile_async instead.\n",
      "  trt_context.active_optimization_profile = 0\n"
     ]
    }
   ],
   "source": [
    "trt_context.active_optimization_profile = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6cbd7c6f-44dd-4e83-8d5d-ce2637e398a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WhisperEncoderTRTEngine' object has no attribute 'create_execution_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwhisper_trt_encoder_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_execution_context\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WhisperEncoderTRTEngine' object has no attribute 'create_execution_context'"
     ]
    }
   ],
   "source": [
    "whisper_trt_encoder_engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b80fcc89-7640-4f76-9fa4-0b94109837a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtensorrt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorrt' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9877bd13-1123-4a81-b0fe-5c7c3887038f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: tensorrt.tensorrt.IExecutionContext, arg1: int) -> None\n\nInvoked with: <tensorrt.tensorrt.IExecutionContext object at 0x7f7898bc7d70>, None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/torch_utils.py:65\u001b[0m, in \u001b[0;36muse_cuda.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     new_kwargs \u001b[38;5;241m=\u001b[39m _send_args_to_device(caller_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# If a device has cuda installed but no compatible kernels, cuda.is_available() will still return True.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# This exception is necessary to catch remaining incompat errors.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m used_cuda:\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/measurements.py:84\u001b[0m, in \u001b[0;36mencoder_inference\u001b[0;34m(whisper_encoder, input_features, timing_profile, use_cuda)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@use_cuda\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_inference\u001b[39m(whisper_encoder, input_features, timing_profile, use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     83\u001b[0m     encoder_stmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: whisper_encoder(input_features\u001b[38;5;241m=\u001b[39minput_features)\n\u001b[0;32m---> 84\u001b[0m     encoder_e2e_time \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_python_inference_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_stmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiming_profile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (encoder_stmt(), encoder_e2e_time)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/general_utils.py:202\u001b[0m, in \u001b[0;36mmeasure_python_inference_code\u001b[0;34m(stmt, timing_profile)\u001b[0m\n\u001b[1;32m    196\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasuring inference call with warmup: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and number: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and iterations \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and duration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m secs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    198\u001b[0m         warmup, number, iterations, duration\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m warmup_mintime \u001b[38;5;241m=\u001b[39m \u001b[43mtimeit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarmup times: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(warmup_mintime))\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Actual measurement cycles\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/timeit.py:238\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(stmt, setup, timer, repeat, number, globals)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat\u001b[39m(stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, timer\u001b[38;5;241m=\u001b[39mdefault_timer,\n\u001b[1;32m    236\u001b[0m            repeat\u001b[38;5;241m=\u001b[39mdefault_repeat, number\u001b[38;5;241m=\u001b[39mdefault_number, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to create Timer object and call repeat method.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/timeit.py:205\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 205\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/measurements.py:83\u001b[0m, in \u001b[0;36mencoder_inference.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@use_cuda\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_inference\u001b[39m(whisper_encoder, input_features, timing_profile, use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 83\u001b[0m     encoder_stmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mwhisper_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     encoder_e2e_time \u001b[38;5;241m=\u001b[39m measure_python_inference_code(encoder_stmt, timing_profile)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (encoder_stmt(), encoder_e2e_time)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/tensorrt_utils.py:431\u001b[0m, in \u001b[0;36mTRTNativeRunner.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrt_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_optimization_profile\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_idx\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: (): incompatible function arguments. The following argument types are supported:\n    1. (arg0: tensorrt.tensorrt.IExecutionContext, arg1: int) -> None\n\nInvoked with: <tensorrt.tensorrt.IExecutionContext object at 0x7f7898bc7d70>, None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_trt_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e48b9-ffa1-41ec-b870-91f41cac0557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088717-bcf6-470f-9c8b-1f9d0564e321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91798bb7-12fc-4eb6-803b-31e747a83a0b",
   "metadata": {},
   "source": [
    "### End-to-End TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a214df2-cf28-444f-a68a-351aba8c9b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bart_trt_decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 73\u001b[0m\n\u001b[1;32m     62\u001b[0m             decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[1;32m     63\u001b[0m                 input_ids\u001b[38;5;241m=\u001b[39mdecoder_initial_input,\n\u001b[1;32m     64\u001b[0m                 beam_scorer\u001b[38;5;241m=\u001b[39mbeam_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     70\u001b[0m             )\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output\n\u001b[0;32m---> 73\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43me2e_trt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m outputs_trt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m trt_time \u001b[38;5;241m=\u001b[39m measure_python_inference_code(e2e_trt, timing_profile)\n",
      "Cell \u001b[0;32mIn[78], line 50\u001b[0m, in \u001b[0;36me2e_trt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m     44\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     45\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[1;32m     46\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m         do_early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m     )\n\u001b[0;32m---> 50\u001b[0m \u001b[43mbart_trt_decoder\u001b[49m\u001b[38;5;241m.\u001b[39mset_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_beams \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     53\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m whisper_trt_decoder\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[1;32m     54\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mdecoder_initial_input,\n\u001b[1;32m     55\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_last_hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bart_trt_decoder' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = WhisperModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = WhisperModelTRTConfig.MIN_OUTPUT_LENGTH[Whisper_VARIANT]\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(no_repeat_ngram_size), \n",
    "    MinLengthLogitsProcessor(min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(tokenizer.convert_tokens_to_ids(tokenizer.bos_token)),\n",
    "    ForcedEOSTokenLogitsProcessor(max_output_len, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "]) # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can ensure identical results with raw HuggingFace\n",
    "\n",
    "decoder_initial_input = torch.full(\n",
    "    (batch_size, 1), tokenizer.convert_tokens_to_ids(tokenizer.eos_token), dtype=torch.int32\n",
    ").to('cuda')\n",
    "\n",
    "if num_beams > 1:\n",
    "    decoder_initial_input = expand_inputs_for_beam_search(decoder_initial_input, expand_size=num_beams)\n",
    "    \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder(input_features=input_features)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        bart_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2246cb-cbfd-4a5a-9bd1-112605ba27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids.int().flatten().contiguous().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c378ebf-b0d0-4460-a21c-01915a9629a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd77b5e-45bf-4bbd-be04-927d199e34c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750c378-c788-4b33-9513-c493eb9ccb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92afb92c-6c00-42bc-9a7d-e2b32a0f96b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_features.int().flatten().contiguous().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea322cb7-ca33-4811-86bb-4002e2e03534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FP16\n",
    "def e2e_trt_fp16():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = whisper_trt_encoder_fp16(input_ids=input_ids)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "            \n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        whisper_trt_decoder_fp16.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = whisper_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = whisper_trt_decoder_fp16.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids_fp16 = e2e_trt_fp16()\n",
    "outputs_trt_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)\n",
    "trt_time_fp16 = measure_python_inference_code(e2e_trt_fp16, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eecfc972-3fde-4838-af73-af68f41bca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0dacea5-15c1-4fbc-b848-852ea84b0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Using engine: Whisper-tiny-bs1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outputs_trt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_string\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mengine_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput identical to HF results? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs_trt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39moutputs_hf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: FP32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRT time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentile_print(trt_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs_trt' is not defined"
     ]
    }
   ],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using engine: {metadata_string + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt == outputs_hf}')\n",
    "print(f\"Precision: FP32\")\n",
    "print(f'TRT time: {percentile_print(trt_time)}')\n",
    "print()\n",
    "print(f\"Using engine: {metadata_string_fp16 + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt_fp16 == outputs_hf}')\n",
    "print(f\"Precision: FP16\")\n",
    "print(f'TRT time: {percentile_print(trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e3bad-86c4-486e-aef3-4fb9da753d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c8d3-08f8-4dae-abec-ba3dcb0fcef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccd523-e56f-46fe-bdb8-b93730701958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
