{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace T5 Inference with TensorRT\n",
    "\n",
    "T5 is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding  different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch T5 model](https://huggingface.co/transformers/model_doc/t5.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace T5 model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5Config,\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_VARIANT = 't5-small' # choices: t5-small | t5-base | t5-large | t5-3b | t5-11b\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_VARIANT).to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_VARIANT)\n",
    "t5_config = T5Config.from_pretrained(T5_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f96c37-9dc8-45ef-9873-a5df53a34684",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/t5-small/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(T5_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "t5_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0f7e1-c146-4fc2-a43c-98e0669e0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544dea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\")\n",
    "num_beams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1edf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    t5_model = t5_model.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    t5_model = t5_model.cuda()\n",
    "    inputs = inputs.to('cuda:0')\n",
    "input_ids = inputs.input_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13913fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on a single example\n",
    "t5_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = t5_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f7fd8b-2ee3-4d25-9204-7713eb7e90b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-08-01 17:07:33.920060: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "# Generate sequence for an input\n",
    "outputs = t5_model.generate(input_ids, num_beams=num_beams)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bc0b8-648c-40f9-ba2f-553a61b9551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/nvadmin/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:0')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37005780-f1b4-4643-8185-90366abda4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c71b9b7-0d14-47ee-9b21-89654c2497ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.model.encoder(input_features=input_features)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.measurements import decoder_inference, encoder_inference, full_inference\n",
    "from T5.export import T5EncoderTorchFile, T5DecoderTorchFile, T5EncoderTRTEngine, T5DecoderTRTEngine\n",
    "\n",
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search\n",
    "\n",
    "t5_torch_encoder = T5EncoderTorchFile.TorchModule(t5_model.encoder)\n",
    "t5_torch_decoder = T5DecoderTorchFile.TorchModule(\n",
    "    t5_model.decoder, t5_model.lm_head, t5_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = whisper_model.generate(inputs=audio_inputs.input_features.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be755fbc-c53e-4f8d-a9c2-4817167cf93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.1 ms, sys: 0 ns, total: 59.1 ms\n",
      "Wall time: 58.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004694263101555407"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_torch_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573dc690-7643-42bc-9221-d22dc7606fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.5 ms, sys: 0 ns, total: 95.5 ms\n",
      "Wall time: 95.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007627135026268661"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585f4eb-b6ce-455e-8352-f90c6228a719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 ms, sys: 0 ns, total: 35 ms\n",
      "Wall time: 34.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0026641369331628084"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_features = audio_inputs.input_features.to('cuda')\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39699019-4c5b-4306-854d-9a48bb2c678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 384])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3b48f-a8e2-4ca2-b880-8fdb91208f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae7f4a-c83b-446d-a1f9-1d6d2fbfbe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.3 ms, sys: 4.12 ms, total: 68.4 ms\n",
      "Wall time: 67.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005610344931483269"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = w_decoder_inference(\n",
    "    whisper_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b05130-c461-4e52-b703-42899d756347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f05fc-f572-4832-ad82-8a75823866b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.T5ModelConfig import T5ModelTRTConfig, T5Metadata\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39d511cf-d963-4629-be54-22e9a258716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 ms, sys: 0 ns, total: 44.8 ms\n",
      "Wall time: 44.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "decoder_output, _ = full_inference(\n",
    "    t5_torch_encoder,\n",
    "    t5_torch_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    num_beams=num_beams,\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0be1f845-f574-4180-9fbf-706f3a9aa502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "\"Let us decode the model's output back into text.\"\n",
    "# De-tokenize output to raw text\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ad775-a312-4d4b-bc65-a93e0094ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5ac34d4-efd4-46b0-a142-397b7bbe6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3410dbdc-91a4-48c8-8acf-b13b51358689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.general_utils import measure_python_inference_code\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT).cuda()\n",
    "\n",
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = processor.tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "# if not cuda_114_mode:\n",
    "whisper_model= whisper_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a80940-4ff5-40d4-a82f-35a5cc1de908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "HF_KV=True\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "whisper_model.float()\n",
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = w_encoder_inference(whisper_torch_encoder, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time = w_decoder_inference(whisper_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# # FP16\n",
    "# if not cuda_114_mode:\n",
    "whisper_model.half()\n",
    "input_features= input_features.half()\n",
    "whisper_torch_encoder_fp16 = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder_fp16 = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = w_encoder_inference(whisper_torch_encoder_fp16, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = w_decoder_inference(whisper_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "989f32e5-6ca9-4609-b5f7-d300a3919f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 Output identical to HF results? False\n",
      "PyTorch FP16 Output identical to HF results? False\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "Encoder time: 0.002191108069382608\n",
      "Decoder time: 0.0036569020012393594\n",
      "Full E2E time: 0.16433088097255677\n",
      "Precision: FP16, Number of Beams: 1\n",
      "Encoder time: 0.00659384299069643\n",
      "Decoder time: 0.005291505018249154\n",
      "Full E2E time: 0.16812224697787315\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time_fp16}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time_fp16}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d128bd1f-a8b5-4fd4-9163-e38085877537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3663c68-147d-4ead-b3ae-c9d4f2aba230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"!<|startoftranscript|>.<|translate|><|notimestamps|> Mr. Kilder is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(output_ids_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b105ef60-4141-4058-be58-0b2d268dadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4788-c322-4f9f-b4e3-e0068a95235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the T5 model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "from NNDF.networks import NetworkMetadata, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c50346f7-6c2c-4e4b-ba70-875688947b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py:729: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(T5_VARIANT)\n",
    "\n",
    "t5_metadata=NetworkMetadata(variant=T5_VARIANT, precision=Precision(fp16=True), other=T5Metadata(kv_cache=False))\n",
    "\n",
    "t5_encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "t5_decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "!mkdir -p $t5_encoder_onnx_model_path\n",
    "!mkdir -p $t5_decoder_onnx_model_path\n",
    "\n",
    "t5_encoder_onnx_model_fpath = T5_VARIANT + \"-encoder.onnx\"\n",
    "t5_decoder_onnx_model_fpath = T5_VARIANT + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "t5_encoder = T5EncoderTorchFile(t5_model.to('cpu'), t5_metadata)\n",
    "t5_decoder = T5DecoderTorchFile(t5_model.to('cpu'), t5_metadata)\n",
    "\n",
    "onnx_t5_encoder = t5_encoder.as_onnx_model(\n",
    "    os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath), force_overwrite=True\n",
    ")\n",
    "onnx_t5_decoder = t5_decoder.as_onnx_model(\n",
    "    os.path.join(t5_decoder_onnx_model_path, t5_decoder_onnx_model_fpath), force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa4985-cf88-4aab-a1d8-3109113e108d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea945da2-e35f-44ff-b309-b763c8ba18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(Whisper_VARIANT)\n",
    "\n",
    "wh_metadata=NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=False))\n",
    "\n",
    "wh_encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "wh_decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "\n",
    "\n",
    "!mkdir -p $wh_encoder_onnx_model_path\n",
    "!mkdir -p $wh_decoder_onnx_model_path\n",
    "\n",
    "wh_encoder_onnx_model_fpath = Whisper_VARIANT.split('/')[1] + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = Whisper_VARIANT.split('/')[1] + \"-decoder-with-lm-head.onnx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4f25b5b-1aa1-4fc0-be6e-f17d78b7c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:198: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:237: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:742: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:72: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n"
     ]
    }
   ],
   "source": [
    "whisper_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), wh_metadata)\n",
    "whisper_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), wh_metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_encoder.as_onnx_model(\n",
    "    os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=True\n",
    ")\n",
    "onnx_whisper_decoder = whisper_decoder.as_onnx_model(\n",
    "    os.path.join(wh_decoder_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca2edb0f-e3ed-44d9-9da2-6b232c6f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NNDF.tensorrt_utils import OnnxProcessOperation, process_onnx, move_t5_cast_op\n",
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)\n",
    "# #output_fpath = os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath)\n",
    "# config = [OnnxProcessOperation.MOVE_CAST_OP2, OnnxProcessOperation.CLAMP_WEIGHTS]\n",
    "\n",
    "# import onnx_graphsurgeon as gs\n",
    "# import onnx\n",
    "# import numpy as np\n",
    "# graph = gs.import_onnx(onnx.load(output_fpath))\n",
    "# folder = os.path.split(output_fpath)[0]\n",
    "# for op in config:\n",
    "#     if op == OnnxProcessOperation.CLAMP_WEIGHTS:\n",
    "#         graph = clamp_weights_onnx_to_fp16_bounds(graph, **kwargs)\n",
    "#     elif op == OnnxProcessOperation.MOVE_CAST_OP2:\n",
    "#         graph = move_t5_cast_op(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2e046-861b-41f0-a8ad-373fa6221a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45bc60fe-ef72-4351-ab7f-efadfdaf35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)\n",
    "# converter= ModelFileConverter\n",
    "# force_overwrite= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0f5bbae-4ca9-406e-9800-a82d0c26d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter.torch_to_onnx(\n",
    "#     output_fpath, self.load_model(), self.network_metadata\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d689d0bd-2dd7-4790-99f2-6ee04fc09bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperEncoderConverter\n",
    "from NNDF.models import ONNXModelFile\n",
    "\n",
    "from NNDF.networks import NetworkMetadata, Precision, Dims\n",
    "from NNDF.models import ModelFileConverter\n",
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "\n",
    "\n",
    "network_metadata_cp_dct = wh_metadata._asdict()\n",
    "del network_metadata_cp_dct[\"precision\"]\n",
    "network_metadata = NetworkMetadata(\n",
    "    **network_metadata_cp_dct, precision=Precision(fp16=False)\n",
    ")\n",
    "ModelFileConverter(\n",
    "    WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine\n",
    ")\n",
    "suconverter = ModelFileConverter(WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine)\n",
    "# suconverter.onnx_to_trt(\n",
    "#     output_fpath, fpath, network_metadata, profiles, preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0742c7d7-8ed4-4b83-9234-153c5e937aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74eaedf8-fb67-4dc0-8d90-e619b98d25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2611d763-4809-422c-8fc9-1894b9d47319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = model.device\n",
    "# input_features = torch.ones(1, 80, 3000).to(device)\n",
    "# simplified_encoder = WhisperEncoderTorchFile.TorchModule(model.model.encoder)\n",
    "# inputs = WhisperModelTRTConfig.get_input_dims(network_metadata)[\"encoder\"]\n",
    "# outputs = WhisperModelTRTConfig.get_output_dims(network_metadata)[\"encoder\"]\n",
    "\n",
    "# # Exports to ONNX\n",
    "# opt_args = {}\n",
    "\n",
    "# version_major = int((torch.__version__).split(\".\")[0])\n",
    "# version_minor = int((torch.__version__).split(\".\")[1])\n",
    "# if version_major < 1 or (version_major == 1 and version_minor < 11):\n",
    "#     opt_args[\"use_external_data_format\"] = True\n",
    "# torch.onnx.export(\n",
    "#     simplified_encoder,\n",
    "#     input_features,\n",
    "#     output_fpath,\n",
    "#     do_constant_folding=True,\n",
    "#     opset_version=13,\n",
    "#     input_names=inputs.get_names(),\n",
    "#     output_names=outputs.get_names(),\n",
    "#     dynamic_axes={\n",
    "#         **inputs.get_torch_dynamic_axis_encoding(),\n",
    "#         **outputs.get_torch_dynamic_axis_encoding(),\n",
    "#     },\n",
    "#     training=torch.onnx.TrainingMode.EVAL,\n",
    "#     **opt_args,\n",
    "# )\n",
    "\n",
    "# if network_metadata.precision.fp16:\n",
    "#     process_onnx(\n",
    "#         [OnnxProcessOperation.MOVE_CAST_OP2, OnnxProcessOperation.CLAMP_WEIGHTS],\n",
    "#         output_fpath,\n",
    "#         output_fpath,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e2f0314-0667-4e1d-88d4-8d95116a4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_ids = torch.tensor([[42] * 10])\n",
    "# input_features = torch.ones(1, 80, 3000)\n",
    "# # Exporting the decoder requires a basic instance of the encoder\n",
    "# # Create one temporarily\n",
    "# simplified_encoder = WhisperEncoderTorchFile.TorchModule(model.get_encoder())\n",
    "# # Exports to ONNX\n",
    "# decoder_with_lm_head = WhisperDecoderTorchFile.TorchModule(\n",
    "#     model.get_decoder(), model.proj_out, model.config\n",
    "# )\n",
    "\n",
    "# inputs = WhisperModelTRTConfig.get_input_dims(network_metadata)[\"decoder\"]\n",
    "# outputs = WhisperModelTRTConfig.get_output_dims(network_metadata)[\"decoder\"]\n",
    "\n",
    "# # Exports to ONNX\n",
    "# opt_args = {}\n",
    "\n",
    "# version_major = int((torch.__version__).split(\".\")[0])\n",
    "# version_minor = int((torch.__version__).split(\".\")[1])\n",
    "# if version_major < 1 or (version_major == 1 and version_minor < 11):\n",
    "#     opt_args[\"use_external_data_format\"] = True\n",
    "\n",
    "# encoder_hidden_states = simplified_encoder(input_features)\n",
    "# decoder_output = decoder_with_lm_head(\n",
    "#     input_ids[:, :-1], encoder_hidden_states\n",
    "# )  # decoder output at t-1 step (logits, past_key_values from 0 to t-1)\n",
    "# past_key_values = decoder_output[1]\n",
    "\n",
    "# decoder_root, decoder_fullname = os.path.split(output_fpath)\n",
    "# # Split kv and non kv onnx into separate folders to avoid weight overlap\n",
    "\n",
    "# non_kv_root = os.path.join(decoder_root, \"non-kv\")\n",
    "# kv_root = os.path.join(decoder_root, \"kv\")\n",
    "# decoder_name, decoder_ext = os.path.splitext(decoder_fullname)\n",
    "# non_kv_fpath = os.path.join(\n",
    "#     non_kv_root, decoder_name + \"-non-kv\" + decoder_ext\n",
    "# )\n",
    "# kv_fpath = os.path.join(kv_root, decoder_fullname)\n",
    "\n",
    "# # This code allows for huggingface compatible torch class to use onnx exporter (change just before onnx.export)\n",
    "# old_forward = decoder_with_lm_head.forward\n",
    "\n",
    "# def _export_forward(input_ids, encoder_hidden_states, past_key_values):\n",
    "#     result = old_forward(\n",
    "#         input_ids, encoder_hidden_states, past_key_values=past_key_values\n",
    "#     )\n",
    "#     return (result[0], result[1])\n",
    "\n",
    "# decoder_with_lm_head.forward = _export_forward\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     decoder_with_lm_head,\n",
    "#     (input_ids[:, -1:], encoder_hidden_states, past_key_values),\n",
    "#     # (1) input_ids should be the t token (last one) while past_key_values is 0 to t-1 caches\n",
    "#     # (2) since past_key_values is kwargs, ideally use \"(input_ids[:,-1:], encoder_hidden_states, {\"past_key_values\": past_key_values})\",\n",
    "#     # but onnx.export seems to unable to take kwargs properly (although PyTorch 1.11 claims it supports already).\n",
    "#     # Therefore, we need to wrap inside _export_forward() and make past_key_values indeed a kwargs\n",
    "#     kv_fpath,\n",
    "#     export_params=True,\n",
    "#     opset_version=12,\n",
    "#     input_names=inputs.get_names(),\n",
    "#     output_names=outputs.get_names(),\n",
    "#     dynamic_axes={\n",
    "#         **inputs.get_torch_dynamic_axis_encoding(),\n",
    "#         **outputs.get_torch_dynamic_axis_encoding(),\n",
    "#     },\n",
    "#     training=torch.onnx.TrainingMode.EVAL,\n",
    "#     **opt_args,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4dc756b-0379-467b-8c47-941587ac1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # # dual-engine approach: also export non-kv onnx model. Note that this is different from the original \"non-kv\" model. This one traces the `use_cache` path and have present_key_values output\n",
    "    # def _export_forward(input_ids, encoder_hidden_states, use_cache):\n",
    "    #     result = old_forward(\n",
    "    #         input_ids, encoder_hidden_states, use_cache=use_cache\n",
    "    #     )\n",
    "    #     return (result[0], result[1])\n",
    "\n",
    "    # decoder_with_lm_head.forward = _export_forward\n",
    "\n",
    "    # # inputs are same as non-kv model\n",
    "    # # outputs are same as kv model\n",
    "    # dict_inputs = inputs.get_dims()\n",
    "    # dict_inputs_non_kv = OrderedDict(\n",
    "    #     {k: dict_inputs[k] for k in [\"input_ids\", \"encoder_hidden_states\"]}\n",
    "    # )\n",
    "    # inputs_non_kv = Dims(dict_inputs_non_kv)\n",
    "\n",
    "    # torch.onnx.export(\n",
    "    #     decoder_with_lm_head,\n",
    "    #     (input_ids[:, -1:], encoder_hidden_states, True),\n",
    "    #     non_kv_fpath,\n",
    "    #     export_params=True,\n",
    "    #     opset_version=12,\n",
    "    #     input_names=inputs_non_kv.get_names(),\n",
    "    #     output_names=outputs.get_names(),\n",
    "    #     dynamic_axes={\n",
    "    #         **inputs_non_kv.get_torch_dynamic_axis_encoding(),\n",
    "    #         **outputs.get_torch_dynamic_axis_encoding(),\n",
    "    #     },\n",
    "    #     training=torch.onnx.TrainingMode.EVAL,\n",
    "    #     **opt_args,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b1fd7ce-d39b-4142-9a05-647143518d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "TRT_KV = False\n",
    "\n",
    "wh_onnx_model_path = './models/{}/onnx'.format(Whisper_VARIANT)\n",
    "!mkdir -p $wh_onnx_model_path\n",
    "\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config = WhisperModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "wh_encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = WhisperModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "wh_encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder_fp16 = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), force_overwrite=False)\n",
    "onnx_whisper_decoder_fp16 = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.export import T5DecoderONNXFile, T5EncoderONNXFile\n",
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_ids', min=(1, 1), opt=(1, 256), max=(1, 512))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tensorrt_model_path = './models/{}/tensorrt'.format(T5_VARIANT)\n",
    "!mkdir -p t5_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_sequence_length // 2),\n",
    "    max=(batch_size, max_sequence_length),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "t5_encoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "t5_decoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(t5_encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        t5_encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(t5_encoder_engine_name, t5_metadata)\n",
    "\n",
    "if not os.path.exists(t5_decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(t5_decoder_onnx_model_path, t5_decoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        t5_decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(t5_decoder_engine_name, t5_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2bdb326-9079-40ca-9a82-48842c2f6cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whisper-tiny-encoder.onnx'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa7618ee-b1f5-40da-b90b-106959e32612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "572b2d68-4004-4724-abd4-079c5487e345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_features', min=(1, 80, 3000), opt=(1, 80, 3000), max=(1, 80, 3000))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 3000),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbb47e-8160-4447-8ad3-dbc7b2ce920f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9514a333-83eb-4654-b74c-4586a91ec7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name):\n",
    "    whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name):\n",
    "    whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba83df-ffd5-4ea5-b74d-070d045d83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16\n",
    "wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name_fp16):\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name_fp16):\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a007d3-67d9-4096-aa38-b2ca2cc4a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0da431-e684-4245-bc37-02d7477b104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60e42229-93fd-47b9-8dd3-33cea3a65b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.ModuleList"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(whisper_model.get_decoder().layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9be48e0b-fb11-4ab5-81af-bde166682411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # FP16\n",
    "# wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "# wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "# if not os.path.exists(wh_encoder_engine_name_fp16):\n",
    "#     whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "#         wh_encoder_engine_name_fp16, \n",
    "#         profiles=[encoder_profile], \n",
    "#         preview_features=preview_features\n",
    "#     )\n",
    "# else:\n",
    "#     whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "# if not os.path.exists(wh_decoder_engine_name_fp16):\n",
    "#     whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "#         wh_decoder_engine_name_fp16, \n",
    "#         profiles=[decoder_profile], \n",
    "#         preview_features=preview_features\n",
    "#     )\n",
    "# else:\n",
    "#     whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74886fff-1a0a-43c4-b69a-b9937208fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5061b60-4f44-40df-8110-4ce5334d89c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee06e7-a8b2-4d7b-98ba-c6a691f00e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "    wh_decoder_engine_name, \n",
    "    profiles=[decoder_profile], \n",
    "    preview_features=preview_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345b915-d5d7-43bb-b218-06cc1b799890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06321d96-f0d5-4c48-8f44-a4f199bf364f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3ab2316-4f8e-470f-b756-b0bf8d5b327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_profiles = [\n",
    "#             Profile().add(\n",
    "#                 \"input_features\",\n",
    "#                 min=(batch_size, 1),\n",
    "#                 opt=(batch_size, opt_input_seq_len),\n",
    "#                 max=(batch_size, max_input_length),\n",
    "#             )\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08b9e2b3-3cf6-44bb-a6c2-bad1a502a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperEncoderConverter, WhisperDecoderConverter\n",
    "from NNDF.models import ONNXModelFile\n",
    "\n",
    "from NNDF.networks import NetworkMetadata, Precision, Dims\n",
    "from NNDF.models import ModelFileConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1833e8b-68ea-4633-bc99-7e7de3ee07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine\n",
      "./models/openai/whisper-tiny/tensorrt/Whisper-tiny-decoder-with-lm-head.onnx-bs1.engine\n"
     ]
    }
   ],
   "source": [
    "print(wh_encoder_engine_name)\n",
    "print(wh_decoder_engine_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0a4dd9f-0288-47da-b633-ab023807f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath) \n",
    "# encoder convert to tensorrt\n",
    "onmf = ONNXModelFile(model, WhisperEncoderConverter, wh_metadata)\n",
    "\n",
    "output_fpath = wh_encoder_engine_name\n",
    "profiles=[encoder_profile]\n",
    "preview_features=preview_features\n",
    "converter = onmf.default_converter\n",
    "fpath = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63df680-95b5-4d1e-91a3-febe7159eb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65496901-515b-4837-8c1d-f43c5a7a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter.onnx_to_trt(\n",
    "#     output_fpath,\n",
    "#     fpath,\n",
    "#     wh_metadata,\n",
    "#     profiles,\n",
    "#     preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b9bd639-364f-415b-bc93-4219f1429d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_metadata_cp_dct = wh_metadata._asdict()\n",
    "del network_metadata_cp_dct[\"precision\"]\n",
    "network_metadata = NetworkMetadata(\n",
    "    **network_metadata_cp_dct, precision=Precision(fp16=False)\n",
    ")\n",
    "ModelFileConverter(\n",
    "    WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine\n",
    ")\n",
    "suconverter = ModelFileConverter(WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine)\n",
    "# suconverter.onnx_to_trt(\n",
    "#     output_fpath, fpath, network_metadata, profiles, preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bea2fb89-7bb5-4523-888c-dc213860c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WhisperEncoderTRTEngine\n",
    "onnx_class = WhisperEncoderTorchFile\n",
    "torch_class = WhisperEncoderONNXFile\n",
    "trt_engine_class = WhisperEncoderTRTEngine\n",
    "\n",
    "from polygraphy.backend.trt import CreateConfig\n",
    "from tensorrt import PreviewFeature, MemoryPoolType\n",
    "\n",
    "# polygraphy\n",
    "from polygraphy.backend.trt import (\n",
    "    network_from_onnx_path,\n",
    "    engine_from_network,\n",
    "    save_engine,\n",
    "    Profile,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1a704c4-4a6a-4714-91d7-60ef9ed07df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trt_engine_class(output_fpath, network_metadata)\n",
    "\n",
    "trt_inference_config = CreateConfig(\n",
    "    tf32=True,\n",
    "    fp16=network_metadata.precision.fp16,\n",
    "    memory_pool_limits = {MemoryPoolType.WORKSPACE: result.max_trt_workspace * 1024 * 1024},\n",
    "    profiles=profiles,\n",
    "    precision_constraints=(\"obey\" if result.use_obey_precision_constraints() else None),\n",
    "    preview_features=preview_features\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ea5fe-0d14-426b-92ca-efe73af8200c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcca62-184f-4327-8064-dbe0e62b24e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62fc6f30-c83c-47a1-961a-544f21bd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import util as trt_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052a0b3-239b-48b9-aadc-afd63875a5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2600dceb-d6e6-4ece-827f-e7f5a8ad5035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/openai/whisper-tiny/onnx'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_onnx_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d09c96c9-5568-4452-93fc-ac0cf86150d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whisper-tiny-encoder.onnx'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_encoder_onnx_model_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30e53459-5091-480a-a095-1d0023115d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorrt.tensorrt.Builder at 0x7fca694f93f0>,\n",
       " <tensorrt.tensorrt.INetworkDefinition at 0x7fca694f9bb0>,\n",
       " <tensorrt.tensorrt.OnnxParser at 0x7fca69502f70>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_from_onnx_path(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8cb71de-e691-448d-9f6f-88f387cb3094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/openai/whisper-tiny/onnx/Whisper-tiny-encoder.onnx'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6820d28e-9e49-4348-bf76-f9d7f1e66f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorrt.tensorrt.Builder at 0x7fca69502af0>,\n",
       " <tensorrt.tensorrt.INetworkDefinition at 0x7fca69502d70>,\n",
       " <tensorrt.tensorrt.OnnxParser at 0x7fca694f9230>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_from_onnx_path(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b52e7e60-e9d1-45a0-b714-089fdb283881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219a7e5-3567-4617-8302-e9a7538393c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad7714a0-a789-452b-8312-5b778cf537bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I]     Configuring with profiles: [Profile().add('input_features', min=(1, 80, 3000), opt=(1, 80, 3000), max=(1, 80, 3000))]\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layer_norm/ReduceMean': 'FLOAT', 'encoder/layer_norm/Pow': 'FLOAT', 'encoder/layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layer_norm/Add': 'FLOAT', 'encoder/layer_norm/Sqrt': 'FLOAT', 'encoder/layer_norm/Div': 'FLOAT', 'encoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | [TF32]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 512.00 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 14.547 seconds\u001b[0m\n",
      "[I] Saving engine to ./models/openai/whisper-tiny/tensorrt/Whisper-tiny-encoder.onnx-bs1.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt.tensorrt.ICudaEngine at 0x7fca695021f0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_definition = result.get_network_definition(network_from_onnx_path(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath)))\n",
    "#network_definition[1].get_input(0).name='input_features'\n",
    "trt_engine = engine_from_network(\n",
    "    network_definition, config=trt_inference_config\n",
    ")\n",
    "save_engine(trt_engine, output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5fbd7-b589-41b6-b558-6cc89f5feb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "734f1ccf-acf0-4e4b-974f-648d97f8381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder tensorrt\n",
    "model = os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath) \n",
    "# encoder convert to tensorrt\n",
    "onmf = ONNXModelFile(model, WhisperDecoderConverter, wh_metadata)\n",
    "\n",
    "output_fpath = wh_decoder_engine_name\n",
    "profiles=[decoder_profile]\n",
    "preview_features=preview_features\n",
    "converter = onmf.default_converter\n",
    "fpath = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aabff285-2a93-4265-bd0d-c4069bb04d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_ids', min=(1, 1), opt=(1, 192), max=(1, 384)).add('encoder_hidden_states', min=(1, 1, 384), opt=(1, 192, 384), max=(1, 384, 384))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "faded129-f045-43d3-9881-3a2db87cbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "= trt_engine_class(output_fpath, network_metadata)\n",
    "\n",
    "trt_inference_config = CreateConfig(\n",
    "    tf32=True,\n",
    "    fp16=network_metadata.precision.fp16,\n",
    "    memory_pool_limits = {MemoryPoolType.WORKSPACE: result.max_trt_workspace * 1024 * 1024},\n",
    "    profiles=profiles,\n",
    "    precision_constraints=(\"obey\" if result.use_obey_precision_constraints() else None),\n",
    "    preview_features=preview_features\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "498a5ed0-06fc-411e-a2ff-ddd10e82bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I]     Configuring with profiles: [Profile().add('input_ids', min=(1, 1), opt=(1, 192), max=(1, 384)).add('encoder_hidden_states', min=(1, 1, 384), opt=(1, 192, 384), max=(1, 384, 384))]\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'/decoder/Cast_2': 'FLOAT', '/decoder/Cast_3': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/final_layer_norm/Add': 'FLOAT', '/decoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/final_layer_norm/Div': 'FLOAT', '/decoder/layers.0/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/final_layer_norm/Add': 'FLOAT', '/decoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/final_layer_norm/Div': 'FLOAT', '/decoder/layers.1/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/final_layer_norm/Add': 'FLOAT', '/decoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/final_layer_norm/Div': 'FLOAT', '/decoder/layers.2/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/final_layer_norm/Add': 'FLOAT', '/decoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/final_layer_norm/Div': 'FLOAT', '/decoder/layers.3/final_layer_norm/Mul': 'FLOAT', '/decoder/layer_norm/ReduceMean': 'FLOAT', '/decoder/layer_norm/Pow': 'FLOAT', '/decoder/layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layer_norm/Add': 'FLOAT', '/decoder/layer_norm/Sqrt': 'FLOAT', '/decoder/layer_norm/Div': 'FLOAT', '/decoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | [TF32]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 512.00 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 50.006 seconds\u001b[0m\n",
      "[I] Saving engine to ./models/openai/whisper-tiny/tensorrt/Whisper-tiny-decoder-with-lm-head.onnx-bs1.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt.tensorrt.ICudaEngine at 0x7fca6950ae30>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_definition = result.get_network_definition(network_from_onnx_path(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath)))\n",
    "#network_definition[1].get_input(0).name='input_features'\n",
    "trt_engine = engine_from_network(\n",
    "    network_definition, config=trt_inference_config\n",
    ")\n",
    "save_engine(trt_engine, output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873ac2a-fbda-40c8-8646-f15eae2284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "033c3627-d054-4bec-8e09-a6daf145021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper-tiny-encoder.onnx\n",
      "Whisper-tiny-decoder-with-lm-head.onnx\n",
      "<Whisper.export.WhisperEncoderTorchFile object at 0x7f915bcb48b0>\n",
      "<Whisper.export.WhisperDecoderTorchFile object at 0x7f915baa87f0>\n"
     ]
    }
   ],
   "source": [
    "print(wh_encoder_onnx_model_fpath)\n",
    "print(wh_decoder_onnx_model_fpath)\n",
    "print(onnx_whisper_encoder)\n",
    "print(onnx_whisper_decoder)\n",
    "#onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "#onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5a2246a0-5932-4fa7-86ce-3345c1bf9e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/openai/whisper-tiny/onnx/Whisper-tiny-encoder.onnx'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4ca27166-2823-4fa2-88d8-eb701ada8a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/openai/whisper-tiny/ONNX/encoder/Whisper-tiny-encoder.onnx'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "661ff8ed-8e88-4b6e-9750-f7e3f1cc30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_definition = result.get_network_definition(network_from_onnx_path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341225e-c62e-435c-8ead-d2e7ef6c3e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486216d-8ff8-4aee-9e41-eeeb5ddcba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = self.default_converter if converter is None else converter()\n",
    "\n",
    "# TODO: Need to check if the old engine file is compatible with current setting\n",
    "if not force_overwrite and os.path.exists(output_fpath):\n",
    "    return converter.trt_engine_class(output_fpath, self.network_metadata)\n",
    "\n",
    "converter.onnx_to_trt(\n",
    "    output_fpath,\n",
    "    self.fpath,\n",
    "    self.network_metadata,\n",
    "    profiles,\n",
    "    preview_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52491007-f2dd-465b-8a8a-d95a5e418f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = self.trt_engine_class(output_fpath, network_metadata)\n",
    "\n",
    "G_LOGGER.info(\"Using optimization profiles: {:}\".format(profiles))\n",
    "\n",
    "try:\n",
    "    self.trt_inference_config = CreateConfig(\n",
    "        tf32=True,\n",
    "        fp16=network_metadata.precision.fp16,\n",
    "        memory_pool_limits = {MemoryPoolType.WORKSPACE: result.max_trt_workspace * 1024 * 1024},\n",
    "        profiles=profiles,\n",
    "        precision_constraints=(\"obey\" if result.use_obey_precision_constraints() else None),\n",
    "        preview_features=preview_features\n",
    "    )\n",
    "except TypeError as e:\n",
    "    G_LOGGER.error(f\"This demo may have an outdated polygraphy. Please see requirements.txt for more details.\")\n",
    "    raise e\n",
    "\n",
    "if G_LOGGER.level == G_LOGGER.DEBUG:\n",
    "    g_logger_verbosity = PG_LOGGER.EXTRA_VERBOSE\n",
    "elif G_LOGGER.level == G_LOGGER.INFO:\n",
    "    g_logger_verbosity = PG_LOGGER.INFO\n",
    "else:\n",
    "    g_logger_verbosity = PG_LOGGER.WARNING\n",
    "\n",
    "with PG_LOGGER.verbosity(g_logger_verbosity):\n",
    "    network_definition = result.get_network_definition(network_from_onnx_path(input_fpath))\n",
    "\n",
    "    trt_engine = engine_from_network(\n",
    "        network_definition, config=self.trt_inference_config\n",
    "    )\n",
    "    save_engine(trt_engine, output_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b1880-2012-4ba0-b770-60cfd7b3a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath), wh_metadata).as_trt_engine(\n",
    "    encoder_engine_name,\n",
    "    profiles=[encoder_profile],\n",
    "    preview_features=preview_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c153b4-4c24-47ca-a0ed-481e133c69c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11279926-98a8-4b6a-b77f-f94efd21498b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5f8ac-37f2-4587-86b0-1dbf8f082adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_trt_encoder_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c153b-a0fe-414c-8aa9-7b538d2e44aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ad5bd-0eb7-4397-8e5c-9387751516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_engine_name = os.path.join(tensorrt_model_path, t5_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name = os.path.join(tensorrt_model_path, t5_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(encoder_onnx_model_path, t5_encoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(encoder_engine_name, t5_metadata)\n",
    "\n",
    "if not os.path.exists(decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(decoder_onnx_model_path, t5_decoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(decoder_engine_name, t5_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d8635-4203-4b0f-9916-6370384ef7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the T5 model, ready for us to carry out inference. \n",
    "\n",
    "#### Single example inference\n",
    "The T5 model with TensorRT backend can now be employed in place of the original HuggingFace T5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954f2f4-c393-463b-a44b-3e5335032b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "from T5.trt import T5TRTEncoder, T5TRTDecoder\n",
    "\n",
    "t5_trt_encoder = T5TRTEncoder(\n",
    "                t5_trt_encoder_engine, t5_metadata, t5_config\n",
    "            )\n",
    "t5_trt_decoder = T5TRTDecoder(\n",
    "                t5_trt_decoder_engine, t5_metadata, t5_config, num_beams=num_beams\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9544ecb-2671-4b53-a544-08f13424cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a single sample\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)\n",
    "outputs = t5_trt_decoder(\n",
    "    expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71a327-546f-4b5b-bd42-caaffcceafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequence for an input\n",
    "max_length = 64\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (1, 1), tokenizer.convert_tokens_to_ids(tokenizer.pad_token), dtype=torch.int32\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4a98-b034-470e-a9f8-096d4100b8d4",
   "metadata": {},
   "source": [
    "#### TRT engine inference benchmark: encoder and decoder stacks\n",
    "First, we will bechmark the encoder and decoder stacks as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b37591-4398-40ff-8a39-5f75347192dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_trt_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5459da-a01b-4894-88dc-01b3637ded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35624c0-7726-4166-9514-f11657722f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2403816-2a78-4aac-938d-89aedd9793cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder\n",
    "\n",
    "wh_trt_encoder = WhisperTRTEncoder(\n",
    "                wh_trt_encoder_engine, wh_metadata, wh_config\n",
    "            )\n",
    "# t5_trt_decoder = T5TRTDecoder(\n",
    "#                 t5_trt_decoder_engine, t5_metadata, t5_config, num_beams=num_beams\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6adbbe-b272-4524-854c-cc3d8f1c4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebfe03-7a60-4dd0-ad32-4e53d6012b07",
   "metadata": {},
   "source": [
    "### Full model inference benchmark\n",
    "\n",
    "Next, we will try the full TensorRT T5 engine for the task of translation. As before, note the time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cb550-24b9-48cd-a4ec-0bf18ac5e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "decoder_output, full_e2e_median_runtime = full_inference(\n",
    "    t5_trt_encoder,\n",
    "    t5_trt_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[t5_metadata.variant],\n",
    "    num_beams=num_beams,\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))\n",
    "full_e2e_median_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f270-e7c8-4081-9587-2a3dc7ea95d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in upto ~10x performance improvement (from 0.0802s to 0.0082s for the T5-small variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch T5 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace T5 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [T5/trt.py](../T5/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2422-c55d-4d02-85f8-4e2f659e0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c456c-d1b1-4a62-890e-e3bffcacf436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144e0ea-94ff-423b-a6d7-e69a29f9dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f78ef8-cb17-4ea8-95e5-1c65fa99119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bc6fa-1441-4b11-b605-bbcfd40c3502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46752711-58a4-47a6-bffb-c80ced6a0e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a214df2-cf28-444f-a68a-351aba8c9b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
