{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "## Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace T5 Inference with TensorRT\n",
    "\n",
    "T5 is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding  different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch T5 model](https://huggingface.co/transformers/model_doc/t5.html) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace T5 model](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5Config,\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace T5 model and Whisper model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The T5 variants that are suported by TensorRT 8 are:  t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_VARIANT = 't5-small' # choices: t5-small | t5-base | t5-large | t5-3b | t5-11b\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_VARIANT).to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_VARIANT)\n",
    "t5_config = T5Config.from_pretrained(T5_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f96c37-9dc8-45ef-9873-a5df53a34684",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/t5-small/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(T5_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "t5_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b25893a-d9b3-4f40-9dc4-29047c44ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "Whisper_VARIANT = \"openai/whisper-tiny\"    # choices: openai/whisper-tiny | openai/whisper-base | openai/whisper-small | openai/whisper-medium | openai/whisper-large-v2\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(Whisper_VARIANT)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT)\n",
    "wh_config = WhisperConfig.from_pretrained(Whisper_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eba99d-8203-4157-8b59-a202db8598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/openai/whisper-tiny/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(Whisper_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "whisper_model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0f7e1-c146-4fc2-a43c-98e0669e0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "Next, we will carry out inference with the PyTorch model.\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544dea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: That is good.\", return_tensors=\"pt\")\n",
    "num_beams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1edf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    t5_model = t5_model.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    t5_model = t5_model.cuda()\n",
    "    inputs = inputs.to('cuda:0')\n",
    "input_ids = inputs.input_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13913fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on a single example\n",
    "t5_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = t5_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f7fd8b-2ee3-4d25-9204-7713eb7e90b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-07-31 17:45:37.811725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "# Generate sequence for an input\n",
    "outputs = t5_model.generate(input_ids, num_beams=num_beams)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bc0b8-648c-40f9-ba2f-553a61b9551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c2c472-20d3-4f32-8032-a5fcb5bd4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/nvadmin/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "audio_inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
    "input_features = audio_inputs.input_features\n",
    "\n",
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"LD_LIBRARY_PATH\" in os.environ and \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    whisper_model = whisper_model.cpu()\n",
    "    input_features = input_features.to('cpu')\n",
    "else:\n",
    "    whisper_model = whisper_model.cuda()\n",
    "    input_features = input_features.to('cuda:0')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f4eaa3-968c-4841-80d9-8692e01c93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = whisper_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription\n",
    "# ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37005780-f1b4-4643-8185-90366abda4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c71b9b7-0d14-47ee-9b21-89654c2497ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.0810,  0.0036, -0.0460,  ..., -0.0463,  0.1107, -0.0297],\n",
       "         [-0.8691,  0.2916,  0.8943,  ...,  1.1164,  0.0542,  0.1625],\n",
       "         [ 0.0308,  2.2723,  1.5943,  ...,  0.2215, -0.8278,  0.2897],\n",
       "         ...,\n",
       "         [ 0.7709, -1.6775,  0.2770,  ..., -0.0620, -0.4735,  0.5232],\n",
       "         [-0.1289, -0.4646,  0.1080,  ...,  0.6388,  0.0286,  0.2890],\n",
       "         [ 0.1964, -0.0994, -1.4564,  ...,  0.1363, -0.5059, -0.1779]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.model.encoder(input_features=input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {},
   "source": [
    "#### Model inference benchmark: encoder and decoder stacks\n",
    "\n",
    "For benchmarking purposes, we will employ a helper functions `encoder_inference` and `decoder_inference` which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596ea542-d9e5-4367-b643-d60027fa05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.measurements import decoder_inference, encoder_inference, full_inference\n",
    "from T5.export import T5EncoderTorchFile, T5DecoderTorchFile, T5EncoderTRTEngine, T5DecoderTRTEngine\n",
    "\n",
    "from Whisper.measurements import decoder_inference as w_decoder_inference, encoder_inference as w_encoder_inference, full_inference as w_full_inference, full_inference_greedy, full_inference_beam\n",
    "from Whisper.export import WhisperEncoderTorchFile, WhisperDecoderTorchFile, WhisperEncoderTRTEngine, WhisperDecoderTRTEngine\n",
    "\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search\n",
    "\n",
    "t5_torch_encoder = T5EncoderTorchFile.TorchModule(t5_model.encoder)\n",
    "t5_torch_decoder = T5DecoderTorchFile.TorchModule(\n",
    "    t5_model.decoder, t5_model.lm_head, t5_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e38c03-23d6-4e53-b0f5-758e205a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.model.encoder)\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(\n",
    "    whisper_model.model.decoder, whisper_model.proj_out, whisper_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9339f413-3b22-4c0d-a49a-e81b05e1105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = whisper_model.generate(inputs=audio_inputs.input_features.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be755fbc-c53e-4f8d-a9c2-4817167cf93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.8 ms, sys: 785 Âµs, total: 58.6 ms\n",
      "Wall time: 58.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004659962956793606"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_torch_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573dc690-7643-42bc-9221-d22dc7606fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.8 ms, sys: 0 ns, total: 95.8 ms\n",
      "Wall time: 95.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007617798983119428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585f4eb-b6ce-455e-8352-f90c6228a719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1337ba74-07be-4179-8804-30de41fb899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.2 ms, sys: 0 ns, total: 35.2 ms\n",
      "Wall time: 34.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002564045018516481"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "input_features = audio_inputs.input_features.to('cuda')\n",
    "\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = w_encoder_inference(\n",
    "    whisper_torch_encoder, input_features, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39699019-4c5b-4306-854d-9a48bb2c678b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4a1cf7-4ed3-47da-b223-2ff7579f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.1 ms, sys: 0 ns, total: 67.1 ms\n",
      "Wall time: 66.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0039661499904468656"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = w_decoder_inference(\n",
    "    whisper_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b05130-c461-4e52-b703-42899d756347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f05fc-f572-4832-ad82-8a75823866b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99d5a06-a8f5-4ce7-a34c-bc42f07ac706",
   "metadata": {},
   "source": [
    "#### Full model inference and benchmark\n",
    "\n",
    "Next, we will try the T5 model for the task of translation from English to German.\n",
    "\n",
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d0bdde-a285-40e5-a554-4e1b35f39b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.T5ModelConfig import T5ModelTRTConfig, T5Metadata\n",
    "from Whisper.WhisperModelConfig import WhisperModelTRTConfig, WhisperMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d511cf-d963-4629-be54-22e9a258716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 ms, sys: 0 ns, total: 37.3 ms\n",
      "Wall time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "decoder_output, _ = full_inference(\n",
    "    t5_torch_encoder,\n",
    "    t5_torch_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    num_beams=num_beams,\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0be1f845-f574-4180-9fbf-706f3a9aa502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "\"Let us decode the model's output back into text.\"\n",
    "# De-tokenize output to raw text\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ad775-a312-4d4b-bc65-a93e0094ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5ac34d4-efd4-46b0-a142-397b7bbe6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_output_len =0 \n",
    "max_output_len = whisper_model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3410dbdc-91a4-48c8-8acf-b13b51358689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.general_utils import measure_python_inference_code\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(Whisper_VARIANT).cuda()\n",
    "\n",
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = processor.tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: whisper_model.generate(input_features, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16, cuda 11.4 has cublas error that will fail in both cpu or cpu model for BART\n",
    "# if not cuda_114_mode:\n",
    "whisper_model= whisper_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: whisper_model.generate(input_features.half(), max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08a80940-4ff5-40d4-a82f-35a5cc1de908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# FP32\n",
    "HF_KV=True\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "whisper_model.float()\n",
    "whisper_torch_encoder = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = w_encoder_inference(whisper_torch_encoder, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time = w_decoder_inference(whisper_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(whisper_torch_encoder,whisper_torch_decoder,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# # FP16\n",
    "# if not cuda_114_mode:\n",
    "whisper_model.half()\n",
    "input_features= input_features.half()\n",
    "whisper_torch_encoder_fp16 = WhisperEncoderTorchFile.TorchModule(whisper_model.get_encoder())\n",
    "whisper_torch_decoder_fp16 = WhisperDecoderTorchFile.TorchModule(whisper_model.get_decoder(), whisper_model.proj_out, whisper_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = w_encoder_inference(whisper_torch_encoder_fp16, input_features, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = w_decoder_inference(whisper_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "989f32e5-6ca9-4609-b5f7-d300a3919f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 Output identical to HF results? False\n",
      "PyTorch FP16 Output identical to HF results? False\n",
      "\n",
      "\n",
      "Device: NVIDIA A100-SXM4-80GB\n",
      "Precision: FP32, Number of Beams: 1\n",
      "Encoder time: 0.002148094936273992\n",
      "Decoder time: 0.00320256594568491\n",
      "Full E2E time: 0.08238941302988678\n",
      "Precision: FP16, Number of Beams: 1\n",
      "Encoder time: 0.0028636789647862315\n",
      "Decoder time: 0.0033147369977086782\n",
      "Full E2E time: 0.0840385410701856\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {encoder_pytorch_time_fp16}\")\n",
    "print(f\"Decoder time: {decoder_pytorch_time_fp16}\")\n",
    "print(f\"Full E2E time: {full_pytorch_time_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d128bd1f-a8b5-4fd4-9163-e38085877537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(whisper_torch_encoder_fp16,whisper_torch_decoder_fp16,input_features,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3663c68-147d-4ead-b3ae-c9d4f2aba230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"!<|startoftranscript|>.<|translate|><|notimestamps|> Mr. Kilder is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(output_ids_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b105ef60-4141-4058-be58-0b2d268dadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4788-c322-4f9f-b4e3-e0068a95235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU. \n",
    "\n",
    "For the T5 model, we will convert the encoder and decoder seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "from NNDF.networks import NetworkMetadata, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c50346f7-6c2c-4e4b-ba70-875688947b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py:729: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(T5_VARIANT)\n",
    "\n",
    "t5_metadata=NetworkMetadata(variant=T5_VARIANT, precision=Precision(fp16=True), other=T5Metadata(kv_cache=False))\n",
    "\n",
    "t5_encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "t5_decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "!mkdir -p $t5_encoder_onnx_model_path\n",
    "!mkdir -p $t5_decoder_onnx_model_path\n",
    "\n",
    "t5_encoder_onnx_model_fpath = T5_VARIANT + \"-encoder.onnx\"\n",
    "t5_decoder_onnx_model_fpath = T5_VARIANT + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "t5_encoder = T5EncoderTorchFile(t5_model.to('cpu'), t5_metadata)\n",
    "t5_decoder = T5DecoderTorchFile(t5_model.to('cpu'), t5_metadata)\n",
    "\n",
    "onnx_t5_encoder = t5_encoder.as_onnx_model(\n",
    "    os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath), force_overwrite=True\n",
    ")\n",
    "onnx_t5_decoder = t5_decoder.as_onnx_model(\n",
    "    os.path.join(t5_decoder_onnx_model_path, t5_decoder_onnx_model_fpath), force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa4985-cf88-4aab-a1d8-3109113e108d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea945da2-e35f-44ff-b309-b763c8ba18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = './models/{}/ONNX'.format(Whisper_VARIANT)\n",
    "\n",
    "wh_metadata=NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=False))\n",
    "\n",
    "wh_encoder_onnx_model_path = os.path.join(onnx_model_path, \"encoder\")\n",
    "wh_decoder_onnx_model_path = os.path.join(onnx_model_path, \"decoder\")\n",
    "\n",
    "\n",
    "!mkdir -p $wh_encoder_onnx_model_path\n",
    "!mkdir -p $wh_decoder_onnx_model_path\n",
    "\n",
    "wh_encoder_onnx_model_fpath = Whisper_VARIANT.split('/')[1] + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = Whisper_VARIANT.split('/')[1] + \"-decoder-with-lm-head.onnx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4f25b5b-1aa1-4fc0-be6e-f17d78b7c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:198: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:237: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:742: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:72: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/nvadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n"
     ]
    }
   ],
   "source": [
    "whisper_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), wh_metadata)\n",
    "whisper_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), wh_metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_encoder.as_onnx_model(\n",
    "    os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=True\n",
    ")\n",
    "onnx_whisper_decoder = whisper_decoder.as_onnx_model(\n",
    "    os.path.join(wh_decoder_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca2edb0f-e3ed-44d9-9da2-6b232c6f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NNDF.tensorrt_utils import OnnxProcessOperation, process_onnx, move_t5_cast_op\n",
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)\n",
    "# #output_fpath = os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath)\n",
    "# config = [OnnxProcessOperation.MOVE_CAST_OP2, OnnxProcessOperation.CLAMP_WEIGHTS]\n",
    "\n",
    "# import onnx_graphsurgeon as gs\n",
    "# import onnx\n",
    "# import numpy as np\n",
    "# graph = gs.import_onnx(onnx.load(output_fpath))\n",
    "# folder = os.path.split(output_fpath)[0]\n",
    "# for op in config:\n",
    "#     if op == OnnxProcessOperation.CLAMP_WEIGHTS:\n",
    "#         graph = clamp_weights_onnx_to_fp16_bounds(graph, **kwargs)\n",
    "#     elif op == OnnxProcessOperation.MOVE_CAST_OP2:\n",
    "#         graph = move_t5_cast_op(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2e046-861b-41f0-a8ad-373fa6221a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45bc60fe-ef72-4351-ab7f-efadfdaf35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fpath =  os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath)\n",
    "# converter= ModelFileConverter\n",
    "# force_overwrite= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0f5bbae-4ca9-406e-9800-a82d0c26d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter.torch_to_onnx(\n",
    "#     output_fpath, self.load_model(), self.network_metadata\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d689d0bd-2dd7-4790-99f2-6ee04fc09bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperEncoderConverter\n",
    "from NNDF.models import ONNXModelFile\n",
    "\n",
    "from NNDF.networks import NetworkMetadata, Precision, Dims\n",
    "from NNDF.models import ModelFileConverter\n",
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "\n",
    "\n",
    "network_metadata_cp_dct = wh_metadata._asdict()\n",
    "del network_metadata_cp_dct[\"precision\"]\n",
    "network_metadata = NetworkMetadata(\n",
    "    **network_metadata_cp_dct, precision=Precision(fp16=False)\n",
    ")\n",
    "ModelFileConverter(\n",
    "    WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine\n",
    ")\n",
    "suconverter = ModelFileConverter(WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine)\n",
    "# suconverter.onnx_to_trt(\n",
    "#     output_fpath, fpath, network_metadata, profiles, preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0742c7d7-8ed4-4b83-9234-153c5e937aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edf02a-e54e-4d5c-a4c8-538c0c90cacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2611d763-4809-422c-8fc9-1894b9d47319",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_fpath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_major \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (version_major \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m version_minor \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     13\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_external_data_format\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(\n\u001b[1;32m     15\u001b[0m     simplified_encoder,\n\u001b[1;32m     16\u001b[0m     input_features,\n\u001b[0;32m---> 17\u001b[0m     \u001b[43moutput_fpath\u001b[49m,\n\u001b[1;32m     18\u001b[0m     do_constant_folding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     opset_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m,\n\u001b[1;32m     20\u001b[0m     input_names\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget_names(),\n\u001b[1;32m     21\u001b[0m     output_names\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mget_names(),\n\u001b[1;32m     22\u001b[0m     dynamic_axes\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mget_torch_dynamic_axis_encoding(),\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs\u001b[38;5;241m.\u001b[39mget_torch_dynamic_axis_encoding(),\n\u001b[1;32m     25\u001b[0m     },\n\u001b[1;32m     26\u001b[0m     training\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mTrainingMode\u001b[38;5;241m.\u001b[39mEVAL,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopt_args,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m network_metadata\u001b[38;5;241m.\u001b[39mprecision\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m     31\u001b[0m     process_onnx(\n\u001b[1;32m     32\u001b[0m         [OnnxProcessOperation\u001b[38;5;241m.\u001b[39mMOVE_CAST_OP2, OnnxProcessOperation\u001b[38;5;241m.\u001b[39mCLAMP_WEIGHTS],\n\u001b[1;32m     33\u001b[0m         output_fpath,\n\u001b[1;32m     34\u001b[0m         output_fpath,\n\u001b[1;32m     35\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_fpath' is not defined"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "input_features = torch.ones(1, 80, 3000).to(device)\n",
    "simplified_encoder = WhisperEncoderTorchFile.TorchModule(model.model.encoder)\n",
    "inputs = WhisperModelTRTConfig.get_input_dims(network_metadata)[\"encoder\"]\n",
    "outputs = WhisperModelTRTConfig.get_output_dims(network_metadata)[\"encoder\"]\n",
    "\n",
    "# Exports to ONNX\n",
    "opt_args = {}\n",
    "\n",
    "version_major = int((torch.__version__).split(\".\")[0])\n",
    "version_minor = int((torch.__version__).split(\".\")[1])\n",
    "if version_major < 1 or (version_major == 1 and version_minor < 11):\n",
    "    opt_args[\"use_external_data_format\"] = True\n",
    "torch.onnx.export(\n",
    "    simplified_encoder,\n",
    "    input_features,\n",
    "    output_fpath,\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    "    input_names=inputs.get_names(),\n",
    "    output_names=outputs.get_names(),\n",
    "    dynamic_axes={\n",
    "        **inputs.get_torch_dynamic_axis_encoding(),\n",
    "        **outputs.get_torch_dynamic_axis_encoding(),\n",
    "    },\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    "    **opt_args,\n",
    ")\n",
    "\n",
    "if network_metadata.precision.fp16:\n",
    "    process_onnx(\n",
    "        [OnnxProcessOperation.MOVE_CAST_OP2, OnnxProcessOperation.CLAMP_WEIGHTS],\n",
    "        output_fpath,\n",
    "        output_fpath,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f0314-0667-4e1d-88d4-8d95116a4050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5.export import T5DecoderONNXFile, T5EncoderONNXFile\n",
    "from Whisper.export import WhisperDecoderONNXFile, WhisperEncoderONNXFile\n",
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_ids', min=(1, 1), opt=(1, 256), max=(1, 512))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tensorrt_model_path = './models/{}/tensorrt'.format(T5_VARIANT)\n",
    "!mkdir -p t5_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_sequence_length // 2),\n",
    "    max=(batch_size, max_sequence_length),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfb64120-9012-40c8-b1e2-4a6366b71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "t5_encoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "t5_decoder_engine_name = os.path.join(t5_tensorrt_model_path, t5_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(t5_encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(t5_encoder_onnx_model_path, t5_encoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        t5_encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(t5_encoder_engine_name, t5_metadata)\n",
    "\n",
    "if not os.path.exists(t5_decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(t5_decoder_onnx_model_path, t5_decoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        t5_decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(t5_decoder_engine_name, t5_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdb326-9079-40ca-9a82-48842c2f6cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "572b2d68-4004-4724-abd4-079c5487e345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_features', min=(1, 80, 3000), opt=(1, 80, 3000), max=(1, 80, 3000))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 3000),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377eae3b-2f8b-4003-a623-7f351683a340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da03bda0-bbad-43ae-b9b8-738e262a77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "TRT_KV = False\n",
    "\n",
    "wh_onnx_model_path = './models/{}/onnx'.format(Whisper_VARIANT)\n",
    "!mkdir -p $wh_onnx_model_path\n",
    "\n",
    "# FP32\n",
    "whisper_model.float()\n",
    "metadata = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=False), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config = WhisperModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "wh_encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), force_overwrite=False)\n",
    "onnx_whisper_decoder = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), force_overwrite=False)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=Whisper_VARIANT, precision=Precision(fp16=True), other=WhisperMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = WhisperModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "wh_encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "wh_decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "whisper_torchfile_encoder = WhisperEncoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "whisper_torchfile_decoder = WhisperDecoderTorchFile(whisper_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_whisper_encoder_fp16 = whisper_torchfile_encoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), force_overwrite=False)\n",
    "onnx_whisper_decoder_fp16 = whisper_torchfile_decoder.as_onnx_model(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40415854-b370-462c-9556-9602f4727911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile().add('input_features', min=(1, 80, 3000), opt=(1, 80, 3000), max=(1, 80, 3000))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tensorrt_model_path = './models/{}/tensorrt'.format(Whisper_VARIANT)\n",
    "!mkdir -p wh_tensorrt_model_path\n",
    "# Decoder optimization profiles\n",
    "batch_size = 1\n",
    "max_sequence_length = WhisperModelTRTConfig.MAX_SEQUENCE_LENGTH[Whisper_VARIANT]\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2),\n",
    "    max=(batch_size * num_beams, max_sequence_length),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, max_sequence_length),\n",
    "    opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),\n",
    "    max=(batch_size * num_beams, max_sequence_length, max_sequence_length),\n",
    ")\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_features\",\n",
    "    min=(batch_size, 80, 3000),\n",
    "    opt=(batch_size, 80, 3000),\n",
    "    max=(batch_size, 80, 3000)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514a333-83eb-4654-b74c-4586a91ec7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa58a794-8895-44df-83a7-bc92884b40c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\u001b[0m\n",
      "\u001b[38;5;11m[W] onnx2trt_utils.cpp:403: One or more weights outside the range of INT32 was clamped\u001b[0m\n",
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'/decoder/Cast_2': 'FLOAT', '/decoder/Cast_3': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/final_layer_norm/Add': 'FLOAT', '/decoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/final_layer_norm/Div': 'FLOAT', '/decoder/layers.0/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/final_layer_norm/Add': 'FLOAT', '/decoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/final_layer_norm/Div': 'FLOAT', '/decoder/layers.1/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/final_layer_norm/Add': 'FLOAT', '/decoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/final_layer_norm/Div': 'FLOAT', '/decoder/layers.2/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/final_layer_norm/Add': 'FLOAT', '/decoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/final_layer_norm/Div': 'FLOAT', '/decoder/layers.3/final_layer_norm/Mul': 'FLOAT', '/decoder/layer_norm/ReduceMean': 'FLOAT', '/decoder/layer_norm/Pow': 'FLOAT', '/decoder/layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layer_norm/Add': 'FLOAT', '/decoder/layer_norm/Sqrt': 'FLOAT', '/decoder/layer_norm/Div': 'FLOAT', '/decoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;9m[E] 4: [graphShapeAnalyzer.cpp::processCheck::722] Error Code 4: Internal Error (/decoder/layers.0/encoder_attn/k_proj/MatMul: attempt to multiply two matrices with mismatching dimensions)\u001b[0m\n",
      "\u001b[38;5;9m[E] 2: [builder.cpp::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;9m[!] Invalid Engine. Please ensure the engine was built correctly\u001b[0m\n"
     ]
    },
    {
     "ename": "PolygraphyException",
     "evalue": "Invalid Engine. Please ensure the engine was built correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPolygraphyException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     whisper_trt_encoder_engine \u001b[38;5;241m=\u001b[39m WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(wh_decoder_engine_name):\n\u001b[0;32m---> 26\u001b[0m     whisper_trt_decoder_engine \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperDecoderONNXFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwh_onnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwh_decoder_onnx_model_fpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_trt_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwh_decoder_engine_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdecoder_profile\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreview_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     whisper_trt_decoder_engine \u001b[38;5;241m=\u001b[39m WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:476\u001b[0m, in \u001b[0;36mONNXModelFile.as_trt_engine\u001b[0;34m(self, output_fpath, converter, force_overwrite, profiles, preview_features)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_overwrite \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_fpath):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mtrt_engine_class(output_fpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_metadata)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_to_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:144\u001b[0m, in \u001b[0;36mModelFileConverter.onnx_to_trt\u001b[0;34m(self, output_fpath, input_fpath, network_metadata, profiles, preview_features)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PG_LOGGER\u001b[38;5;241m.\u001b[39mverbosity(g_logger_verbosity):\n\u001b[1;32m    142\u001b[0m     network_definition \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_network_definition(network_from_onnx_path(input_fpath))\n\u001b[0;32m--> 144\u001b[0m     trt_engine \u001b[38;5;241m=\u001b[39m \u001b[43mengine_from_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork_definition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrt_inference_config\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     save_engine(trt_engine, output_fpath)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mengine_from_network\u001b[0;34m(network, config, save_timing_cache, runtime)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:617\u001b[0m, in \u001b[0;36mEngineFromNetwork.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    trt.ICudaEngine: The engine that was created.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# We do not invoke super().call_impl here because we would otherwise be responsible\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# for freeing it's return values.\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_from_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runtime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mengine_from_bytes\u001b[0;34m(serialized_engine, runtime)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:646\u001b[0m, in \u001b[0;36mEngineFromBytes.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_called_by(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    642\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m        trt.ICudaEngine: The deserialized engine.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     buffer, owns_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_if_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized_engine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     runtime, owns_runtime \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39minvoke_if_callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime)\n\u001b[1;32m    649\u001b[0m     trt\u001b[38;5;241m.\u001b[39minit_libnvinfer_plugins(trt_util\u001b[38;5;241m.\u001b[39mget_trt_logger(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:663\u001b[0m, in \u001b[0;36minvoke_if_callable\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03mAttempts to invoke a function with arguments. If `func` is not callable, then returns `func`\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03mThe second return value of this function indicates whether the argument was a callable.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m--> 663\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:550\u001b[0m, in \u001b[0;36mEngineBytesFromNetwork.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m engine_bytes:\n\u001b[0;32m--> 550\u001b[0m     \u001b[43mG_LOGGER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInvalid Engine. Please ensure the engine was built correctly\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mfinish(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished engine building in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiming_cache_path:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/logger/logger.py:597\u001b[0m, in \u001b[0;36mLogger.critical\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(message, Logger\u001b[38;5;241m.\u001b[39mCRITICAL, stack_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolygraphyException\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PolygraphyException(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mPolygraphyException\u001b[0m: Invalid Engine. Please ensure the engine was built correctly"
     ]
    }
   ],
   "source": [
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features.append(PreviewFeature.FASTER_DYNAMIC_SHAPES_0805)\n",
    "\n",
    "# FP32\n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name):\n",
    "    whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name):\n",
    "    whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a69dfb80-8c61-4b05-9c06-bbf2adc099da",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "    wh_encoder_engine_name, \n",
    "    profiles=[encoder_profile], \n",
    "    preview_features=preview_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff24c2-2e4e-4678-b3ce-bf9e01a8e55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b4e54ea-d9ce-4a3c-a78d-0bcde0670297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'/decoder/Cast_2': 'FLOAT', '/decoder/Cast_3': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.0/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.0/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.0/final_layer_norm/Add': 'FLOAT', '/decoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.0/final_layer_norm/Div': 'FLOAT', '/decoder/layers.0/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.1/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.1/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.1/final_layer_norm/Add': 'FLOAT', '/decoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.1/final_layer_norm/Div': 'FLOAT', '/decoder/layers.1/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.2/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.2/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.2/final_layer_norm/Add': 'FLOAT', '/decoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.2/final_layer_norm/Div': 'FLOAT', '/decoder/layers.2/final_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Add': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Div': 'FLOAT', '/decoder/layers.3/encoder_attn_layer_norm/Mul': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', '/decoder/layers.3/final_layer_norm/Pow': 'FLOAT', '/decoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layers.3/final_layer_norm/Add': 'FLOAT', '/decoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', '/decoder/layers.3/final_layer_norm/Div': 'FLOAT', '/decoder/layers.3/final_layer_norm/Mul': 'FLOAT', '/decoder/layer_norm/ReduceMean': 'FLOAT', '/decoder/layer_norm/Pow': 'FLOAT', '/decoder/layer_norm/ReduceMean_1': 'FLOAT', '/decoder/layer_norm/Add': 'FLOAT', '/decoder/layer_norm/Sqrt': 'FLOAT', '/decoder/layer_norm/Div': 'FLOAT', '/decoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;9m[E] 4: [graphShapeAnalyzer.cpp::processCheck::722] Error Code 4: Internal Error (/decoder/layers.0/encoder_attn/k_proj/MatMul: attempt to multiply two matrices with mismatching dimensions)\u001b[0m\n",
      "\u001b[38;5;9m[E] 2: [builder.cpp::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;9m[!] Invalid Engine. Please ensure the engine was built correctly\u001b[0m\n"
     ]
    },
    {
     "ename": "PolygraphyException",
     "evalue": "Invalid Engine. Please ensure the engine was built correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPolygraphyException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m whisper_trt_decoder_engine \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperDecoderONNXFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwh_onnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwh_decoder_onnx_model_fpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_trt_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwh_decoder_engine_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdecoder_profile\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreview_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:476\u001b[0m, in \u001b[0;36mONNXModelFile.as_trt_engine\u001b[0;34m(self, output_fpath, converter, force_overwrite, profiles, preview_features)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_overwrite \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_fpath):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mtrt_engine_class(output_fpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_metadata)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_to_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:144\u001b[0m, in \u001b[0;36mModelFileConverter.onnx_to_trt\u001b[0;34m(self, output_fpath, input_fpath, network_metadata, profiles, preview_features)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PG_LOGGER\u001b[38;5;241m.\u001b[39mverbosity(g_logger_verbosity):\n\u001b[1;32m    142\u001b[0m     network_definition \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_network_definition(network_from_onnx_path(input_fpath))\n\u001b[0;32m--> 144\u001b[0m     trt_engine \u001b[38;5;241m=\u001b[39m \u001b[43mengine_from_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork_definition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrt_inference_config\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     save_engine(trt_engine, output_fpath)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mengine_from_network\u001b[0;34m(network, config, save_timing_cache, runtime)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:617\u001b[0m, in \u001b[0;36mEngineFromNetwork.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    trt.ICudaEngine: The engine that was created.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# We do not invoke super().call_impl here because we would otherwise be responsible\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# for freeing it's return values.\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_from_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runtime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mengine_from_bytes\u001b[0;34m(serialized_engine, runtime)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:646\u001b[0m, in \u001b[0;36mEngineFromBytes.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_called_by(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    642\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m        trt.ICudaEngine: The deserialized engine.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     buffer, owns_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_if_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized_engine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     runtime, owns_runtime \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39minvoke_if_callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime)\n\u001b[1;32m    649\u001b[0m     trt\u001b[38;5;241m.\u001b[39minit_libnvinfer_plugins(trt_util\u001b[38;5;241m.\u001b[39mget_trt_logger(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:663\u001b[0m, in \u001b[0;36minvoke_if_callable\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03mAttempts to invoke a function with arguments. If `func` is not callable, then returns `func`\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03mThe second return value of this function indicates whether the argument was a callable.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m--> 663\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:550\u001b[0m, in \u001b[0;36mEngineBytesFromNetwork.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m engine_bytes:\n\u001b[0;32m--> 550\u001b[0m     \u001b[43mG_LOGGER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInvalid Engine. Please ensure the engine was built correctly\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mfinish(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished engine building in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiming_cache_path:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/logger/logger.py:597\u001b[0m, in \u001b[0;36mLogger.critical\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(message, Logger\u001b[38;5;241m.\u001b[39mCRITICAL, stack_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolygraphyException\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PolygraphyException(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mPolygraphyException\u001b[0m: Invalid Engine. Please ensure the engine was built correctly"
     ]
    }
   ],
   "source": [
    "whisper_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "    wh_decoder_engine_name, \n",
    "    profiles=[decoder_profile], \n",
    "    preview_features=preview_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50067f-2c76-4a53-bde1-d24c96aa5c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9be48e0b-fb11-4ab5-81af-bde166682411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[W] It looks like some layers in the network have compute precision set, but precision constraints were not enabled. \n",
      "    Precision constraints must be set to 'prefer' or 'obey' for layer compute precision to take effect. \n",
      "    Note: Layers and their requested precisions were: {'encoder/layers.0/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.0/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.0/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.0/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.0/final_layer_norm/Add': 'FLOAT', 'encoder/layers.0/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.0/final_layer_norm/Div': 'FLOAT', 'encoder/layers.0/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.1/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.1/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.1/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.1/final_layer_norm/Add': 'FLOAT', 'encoder/layers.1/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.1/final_layer_norm/Div': 'FLOAT', 'encoder/layers.1/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.2/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.2/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.2/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.2/final_layer_norm/Add': 'FLOAT', 'encoder/layers.2/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.2/final_layer_norm/Div': 'FLOAT', 'encoder/layers.2/final_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Add': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Div': 'FLOAT', 'encoder/layers.3/self_attn_layer_norm/Mul': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean': 'FLOAT', 'encoder/layers.3/final_layer_norm/Pow': 'FLOAT', 'encoder/layers.3/final_layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layers.3/final_layer_norm/Add': 'FLOAT', 'encoder/layers.3/final_layer_norm/Sqrt': 'FLOAT', 'encoder/layers.3/final_layer_norm/Div': 'FLOAT', 'encoder/layers.3/final_layer_norm/Mul': 'FLOAT', 'encoder/layer_norm/ReduceMean': 'FLOAT', 'encoder/layer_norm/Pow': 'FLOAT', 'encoder/layer_norm/ReduceMean_1': 'FLOAT', 'encoder/layer_norm/Add': 'FLOAT', 'encoder/layer_norm/Sqrt': 'FLOAT', 'encoder/layer_norm/Div': 'FLOAT', 'encoder/layer_norm/Mul': 'FLOAT'}\u001b[0m\n",
      "\u001b[38;5;9m[E] 4: [graphShapeAnalyzer.cpp::processCheck::722] Error Code 4: Internal Error (/decoder/layers.0/encoder_attn/k_proj/MatMul: attempt to multiply two matrices with mismatching dimensions)\u001b[0m\n",
      "\u001b[38;5;9m[E] 2: [builder.cpp::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;9m[!] Invalid Engine. Please ensure the engine was built correctly\u001b[0m\n"
     ]
    },
    {
     "ename": "PolygraphyException",
     "evalue": "Invalid Engine. Please ensure the engine was built correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPolygraphyException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     whisper_trt_encoder_engine_fp16 \u001b[38;5;241m=\u001b[39m WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(wh_decoder_engine_name_fp16):\n\u001b[0;32m---> 15\u001b[0m     whisper_trt_decoder_engine_fp16 \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperDecoderONNXFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwh_onnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwh_decoder_onnx_model_fpath_fp16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_fp16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_trt_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwh_decoder_engine_name_fp16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdecoder_profile\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreview_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     whisper_trt_decoder_engine_fp16 \u001b[38;5;241m=\u001b[39m WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:476\u001b[0m, in \u001b[0;36mONNXModelFile.as_trt_engine\u001b[0;34m(self, output_fpath, converter, force_overwrite, profiles, preview_features)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_overwrite \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_fpath):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mtrt_engine_class(output_fpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_metadata)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_to_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:144\u001b[0m, in \u001b[0;36mModelFileConverter.onnx_to_trt\u001b[0;34m(self, output_fpath, input_fpath, network_metadata, profiles, preview_features)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PG_LOGGER\u001b[38;5;241m.\u001b[39mverbosity(g_logger_verbosity):\n\u001b[1;32m    142\u001b[0m     network_definition \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_network_definition(network_from_onnx_path(input_fpath))\n\u001b[0;32m--> 144\u001b[0m     trt_engine \u001b[38;5;241m=\u001b[39m \u001b[43mengine_from_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork_definition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrt_inference_config\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     save_engine(trt_engine, output_fpath)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mengine_from_network\u001b[0;34m(network, config, save_timing_cache, runtime)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:617\u001b[0m, in \u001b[0;36mEngineFromNetwork.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    trt.ICudaEngine: The engine that was created.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# We do not invoke super().call_impl here because we would otherwise be responsible\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# for freeing it's return values.\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_from_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runtime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mengine_from_bytes\u001b[0;34m(serialized_engine, runtime)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:646\u001b[0m, in \u001b[0;36mEngineFromBytes.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_called_by(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    642\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m        trt.ICudaEngine: The deserialized engine.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     buffer, owns_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_if_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized_engine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     runtime, owns_runtime \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39minvoke_if_callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runtime)\n\u001b[1;32m    649\u001b[0m     trt\u001b[38;5;241m.\u001b[39minit_libnvinfer_plugins(trt_util\u001b[38;5;241m.\u001b[39mget_trt_logger(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:663\u001b[0m, in \u001b[0;36minvoke_if_callable\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03mAttempts to invoke a function with arguments. If `func` is not callable, then returns `func`\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03mThe second return value of this function indicates whether the argument was a callable.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m--> 663\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:550\u001b[0m, in \u001b[0;36mEngineBytesFromNetwork.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m engine_bytes:\n\u001b[0;32m--> 550\u001b[0m     \u001b[43mG_LOGGER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInvalid Engine. Please ensure the engine was built correctly\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m G_LOGGER\u001b[38;5;241m.\u001b[39mfinish(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished engine building in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiming_cache_path:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/logger/logger.py:597\u001b[0m, in \u001b[0;36mLogger.critical\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(message, Logger\u001b[38;5;241m.\u001b[39mCRITICAL, stack_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolygraphyException\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PolygraphyException(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mPolygraphyException\u001b[0m: Invalid Engine. Please ensure the engine was built correctly"
     ]
    }
   ],
   "source": [
    "\n",
    "# FP16\n",
    "wh_encoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name_fp16 = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name_fp16):\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderONNXFile(os.path.join(wh_onnx_model_path, wh_encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_encoder_engine_fp16 = WhisperEncoderTRTEngine(wh_encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(wh_decoder_engine_name_fp16):\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderONNXFile(os.path.join(wh_onnx_model_path, wh_decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        wh_decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    whisper_trt_decoder_engine_fp16 = WhisperDecoderTRTEngine(wh_decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74886fff-1a0a-43c4-b69a-b9937208fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5061b60-4f44-40df-8110-4ce5334d89c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee06e7-a8b2-4d7b-98ba-c6a691f00e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7bb0aba-1d23-48bb-9ade-bc7e017911c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not open file ./models/t5-small/ONNX/encoder/whisper-base-encoder.onnx\n",
      "Could not open file ./models/t5-small/ONNX/encoder/whisper-base-encoder.onnx\n",
      "\u001b[38;5;9m[!] Failed to parse ONNX model. Does the model file exist and contain a valid ONNX model?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;9m[E] ModelImporter.cpp:688: Failed to parse ONNX model from file: ./models/t5-small/ONNX/encoder/whisper-base-encoder.onnx\u001b[0m\n"
     ]
    },
    {
     "ename": "PolygraphyException",
     "evalue": "Failed to parse ONNX model. Does the model file exist and contain a valid ONNX model?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPolygraphyException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m wh_decoder_engine_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.engine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(wh_encoder_engine_name):\n\u001b[0;32m---> 17\u001b[0m     wh_trt_encoder_engine \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperEncoderONNXFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt5_encoder_onnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwh_encoder_onnx_model_fpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwh_metadata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_trt_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwh_encoder_engine_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mencoder_profile\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreview_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreview_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     wh_trt_encoder_engine \u001b[38;5;241m=\u001b[39m WhisperEncoderTRTEngine(wh_encoder_engine_name, wh_metadata)\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:476\u001b[0m, in \u001b[0;36mONNXModelFile.as_trt_engine\u001b[0;34m(self, output_fpath, converter, force_overwrite, profiles, preview_features)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_overwrite \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_fpath):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mtrt_engine_class(output_fpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_metadata)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_to_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/Whisper/export.py:499\u001b[0m, in \u001b[0;36mWhisperEncoderConverter.onnx_to_trt\u001b[0;34m(self, output_fpath, input_fpath, network_metadata, profiles, preview_features)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m network_metadata_cp_dct[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    495\u001b[0m     network_metadata \u001b[38;5;241m=\u001b[39m NetworkMetadata(\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnetwork_metadata_cp_dct, precision\u001b[38;5;241m=\u001b[39mPrecision(fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    497\u001b[0m     )\n\u001b[0;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_to_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_fpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreview_features\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jisu/triton/tensorrt/HuggingFace/NNDF/models.py:142\u001b[0m, in \u001b[0;36mModelFileConverter.onnx_to_trt\u001b[0;34m(self, output_fpath, input_fpath, network_metadata, profiles, preview_features)\u001b[0m\n\u001b[1;32m    139\u001b[0m     g_logger_verbosity \u001b[38;5;241m=\u001b[39m PG_LOGGER\u001b[38;5;241m.\u001b[39mWARNING\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PG_LOGGER\u001b[38;5;241m.\u001b[39mverbosity(g_logger_verbosity):\n\u001b[0;32m--> 142\u001b[0m     network_definition \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_network_definition(\u001b[43mnetwork_from_onnx_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    144\u001b[0m     trt_engine \u001b[38;5;241m=\u001b[39m engine_from_network(\n\u001b[1;32m    145\u001b[0m         network_definition, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_inference_config\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     save_engine(trt_engine, output_fpath)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mnetwork_from_onnx_path\u001b[0;34m(path, flags)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/base/loader.py:40\u001b[0m, in \u001b[0;36mBaseLoader.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mInvokes the loader by forwarding arguments to ``call_impl``.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mNote: ``call_impl`` should *not* be called directly - use this function instead.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_impl\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/util/util.py:694\u001b[0m, in \u001b[0;36mcheck_called_by.<locals>.check_called_by_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual_caller_name \u001b[38;5;241m!=\u001b[39m expected_caller_name:\n\u001b[1;32m    690\u001b[0m         G_LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly is not recommended. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_caller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/loader.py:211\u001b[0m, in \u001b[0;36mNetworkFromOnnxPath.call_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39mFreeOnException(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_impl()) \u001b[38;5;28;01mas\u001b[39;00m (builder, network, parser):\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# We need to use parse_from_file for the ONNX parser to keep track of the location of the ONNX file for\u001b[39;00m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;66;03m# potentially parsing any external weights.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m         success \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_from_file(path)\n\u001b[0;32m--> 211\u001b[0m         \u001b[43mtrt_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_onnx_parser_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m builder, network, parser\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/backend/trt/util.py:92\u001b[0m, in \u001b[0;36mcheck_onnx_parser_errors\u001b[0;34m(parser, success)\u001b[0m\n\u001b[1;32m     89\u001b[0m     G_LOGGER\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse ONNX correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mG_LOGGER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFailed to parse ONNX model. Does the model file exist and contain a valid ONNX model?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/polygraphy/logger/logger.py:597\u001b[0m, in \u001b[0;36mLogger.critical\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(message, Logger\u001b[38;5;241m.\u001b[39mCRITICAL, stack_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolygraphyException\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PolygraphyException(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mPolygraphyException\u001b[0m: Failed to parse ONNX model. Does the model file exist and contain a valid ONNX model?"
     ]
    }
   ],
   "source": [
    "disable_preview_dynamic_shapes = False\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "    \n",
    "wh_encoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "wh_decoder_engine_name = os.path.join(wh_tensorrt_model_path, wh_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(wh_encoder_engine_name):\n",
    "    wh_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(t5_encoder_onnx_model_path, wh_encoder_onnx_model_fpath), wh_metadata).as_trt_engine(\n",
    "        wh_encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    wh_trt_encoder_engine = WhisperEncoderTRTEngine(wh_encoder_engine_name, wh_metadata)\n",
    "\n",
    "if not os.path.exists(wh_decoder_engine_name):\n",
    "    wh_trt_decoder_engine = WhisperDecoderONNXFile(os.path.join(wh_decoder_onnx_model_path, wh_decoder_onnx_model_fpath), wh_metadata).as_trt_engine(\n",
    "        wh_decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    wh_trt_decoder_engine = WhisperDecoderTRTEngine(wh_decoder_engine_name, wh_metadata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345b915-d5d7-43bb-b218-06cc1b799890",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_decoder_engine_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06321d96-f0d5-4c48-8f44-a4f199bf364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.build_serialized_network(network, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab2316-4f8e-470f-b756-b0bf8d5b327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_profiles = [\n",
    "#             Profile().add(\n",
    "#                 \"input_features\",\n",
    "#                 min=(batch_size, 1),\n",
    "#                 opt=(batch_size, opt_input_seq_len),\n",
    "#                 max=(batch_size, max_input_length),\n",
    "#             )\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0257f7-8167-45ea-88e9-5bc7ec06fdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b5d65-2870-499a-a629-35f3aadf4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file = WhisperEncoderONNXFile(os.path.join(encoder_onnx_model_path, wh_encoder_onnx_model_fpath), wh_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9e2b3-3cf6-44bb-a6c2-bad1a502a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Whisper.export import WhisperEncoderConverter\n",
    "from NNDF.models import ONNXModelFile\n",
    "\n",
    "from NNDF.networks import NetworkMetadata, Precision, Dims\n",
    "from NNDF.models import ModelFileConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4dd9f-0288-47da-b633-ab023807f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = os.path.join(encoder_onnx_model_path, wh_encoder_onnx_model_fpath) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962c02f-2e85-442e-9042-e9675377d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "onmf = ONNXModelFile(model, WhisperEncoderConverter, wh_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789994e5-8591-46f8-b666-90529b224a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fpath = encoder_engine_name\n",
    "profiles=[encoder_profile]\n",
    "preview_features=preview_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75936b9-927d-424b-a5f8-271a3c5795ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_engine_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf3c74-0244-4eec-8967-041426f5c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = onmf.default_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee36dca-a6b8-4c00-af05-fc817c7b5b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb9e11-e691-4872-84fd-c26b39018aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63df680-95b5-4d1e-91a3-febe7159eb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65496901-515b-4837-8c1d-f43c5a7a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter.onnx_to_trt(\n",
    "#     output_fpath,\n",
    "#     fpath,\n",
    "#     wh_metadata,\n",
    "#     profiles,\n",
    "#     preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bd639-364f-415b-bc93-4219f1429d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_metadata_cp_dct = wh_metadata._asdict()\n",
    "del network_metadata_cp_dct[\"precision\"]\n",
    "network_metadata = NetworkMetadata(\n",
    "    **network_metadata_cp_dct, precision=Precision(fp16=False)\n",
    ")\n",
    "ModelFileConverter(\n",
    "    WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine\n",
    ")\n",
    "suconverter = ModelFileConverter(WhisperEncoderTorchFile, WhisperEncoderONNXFile, WhisperEncoderTRTEngine)\n",
    "# suconverter.onnx_to_trt(\n",
    "#     output_fpath, fpath, network_metadata, profiles, preview_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2fb89-7bb5-4523-888c-dc213860c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WhisperEncoderTRTEngine\n",
    "onnx_class = WhisperEncoderTorchFile\n",
    "torch_class = WhisperEncoderONNXFile\n",
    "trt_engine_class = WhisperEncoderTRTEngine\n",
    "\n",
    "from polygraphy.backend.trt import CreateConfig\n",
    "from tensorrt import PreviewFeature, MemoryPoolType\n",
    "\n",
    "# polygraphy\n",
    "from polygraphy.backend.trt import (\n",
    "    network_from_onnx_path,\n",
    "    engine_from_network,\n",
    "    save_engine,\n",
    "    Profile,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a704c4-4a6a-4714-91d7-60ef9ed07df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trt_engine_class(output_fpath, network_metadata)\n",
    "\n",
    "trt_inference_config = CreateConfig(\n",
    "    tf32=True,\n",
    "    fp16=network_metadata.precision.fp16,\n",
    "    memory_pool_limits = {MemoryPoolType.WORKSPACE: result.max_trt_workspace * 1024 * 1024},\n",
    "    profiles=profiles,\n",
    "    precision_constraints=(\"obey\" if result.use_obey_precision_constraints() else None),\n",
    "    preview_features=preview_features\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb96b4-1420-44a0-b8d9-01c6404abd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_inference_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97caf38f-66cd-4326-8eb0-e5cf601dc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(network_definition[1].num_inputs):\n",
    "    inp = network_definition[1].get_input(idx)\n",
    "    print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80b761-32b8-4146-b559-e37ea25f7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(encoder_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad4902-eab8-4e9d-a9fb-9054ad59e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profiles:\n",
    "    # Last profile is used for set_calibration_profile.\n",
    "    calib_profile = profile.fill_defaults(network).to_trt(builder, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452a4b0-d028-46cd-9df9-aba4294c6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5108111-d69f-4f63-8a8c-b1b8a7e99f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = network_definition[0]\n",
    "network = network_definition[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b022-6363-4f41-9516-86563ee352c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_profile.fill_defaults(network).to_trt(builder, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc6f30-c83c-47a1-961a-544f21bd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import util as trt_util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1deb287-d33c-4600-bc2d-c7ffc3031a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_profile = builder.create_optimization_profile()\n",
    "unused_keys = set(encoder_profile.keys())\n",
    "available_inputs = set()\n",
    "\n",
    "for idx in range(network.num_inputs):\n",
    "    inp = network.get_input(idx)\n",
    "    if inp.name in unused_keys:\n",
    "        unused_keys.remove(inp.name)\n",
    "    available_inputs.add(inp.name)\n",
    "\n",
    "    is_shape_tensor = inp.is_shape_tensor\n",
    "\n",
    "    if is_shape_tensor:\n",
    "        if inp.name in encoder_profile:\n",
    "            shapes = encoder_profile[inp.name]\n",
    "            trt_profile.set_shape_input(inp.name, shapes.min, shapes.opt, shapes.max)\n",
    "            \n",
    "        else:\n",
    "            print(\n",
    "                f\"{trt_util.str_from_tensor(inp, is_shape_tensor)} | No values provided. Assuming this is not a dynamic shape-tensor.\"\n",
    "            )\n",
    "    else:\n",
    "        shapes = encoder_profile[inp.name]\n",
    "        trt_profile.set_shape(inp.name, shapes.min, shapes.opt, shapes.max)\n",
    "        \n",
    "\n",
    "if unused_keys:\n",
    "    print(\n",
    "        f\"Invalid inputs were provided to the optimization profile: {unused_keys}\\n\"\n",
    "        f\"Note: Inputs available in the TensorRT network are: {available_inputs}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052a0b3-239b-48b9-aadc-afd63875a5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee760e-88ae-4cb8-89f4-61a70c320457",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_profile = builder.create_optimization_profile()\n",
    "bool(trt_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f427b-8112-4c1e-b0d2-14244e7805aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_profile.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f595fb3-7b1a-4e78-9a0c-64d182b402ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = encoder_profile[inp.name]\n",
    "#trt_profile.set_shape(inp.name, shapes.min, shapes.opt, shapes.max)\n",
    "trt_profile.set_shape(\"input_ids\", (1,2,4), (1,1), (1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b36ea9-8699-4fac-81ae-e714c3a0200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(trt_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819b06c-53dc-4f9b-818c-ff04f31ef85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_profile.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2a1ee-12dc-4843-8550-a09199c51df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_util.check_profile(trt_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83082f42-7064-43ff-8a22-153ca818f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600dceb-d6e6-4ece-827f-e7f5a8ad5035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e53459-5091-480a-a095-1d0023115d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31be02-135c-481c-8478-570a7ca20a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e7e60-e9d1-45a0-b714-089fdb283881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7714a0-a789-452b-8312-5b778cf537bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_definition = result.get_network_definition(network_from_onnx_path(fpath))\n",
    "#network_definition[1].get_input(0).name='input_features'\n",
    "trt_engine = engine_from_network(\n",
    "    network_definition, config=trt_inference_config\n",
    ")\n",
    "save_engine(trt_engine, output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beef458-ffd5-4df5-ab93-fd501c8e0f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadacc83-436a-4e96-b5b3-7bf71f665f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = self.default_converter #if converter is None else converter()\n",
    "\n",
    "# TODO: Need to check if the old engine file is compatible with current setting\n",
    "if not force_overwrite and os.path.exists(output_fpath):\n",
    "    return converter.trt_engine_class(output_fpath, self.network_metadata)\n",
    "\n",
    "return converter.onnx_to_trt(\n",
    "    output_fpath,\n",
    "    self.fpath,\n",
    "    self.network_metadata,\n",
    "    profiles,\n",
    "    preview_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24816e5b-94e7-4281-a347-d0e285147591",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9a452-4bb3-446f-b1c7-004c44d923ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNXModelFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456b177-ffc6-427b-be5c-a7c97aa6b24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbf6cb-72b7-4ea6-9523-4384814c5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file.network_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486216d-8ff8-4aee-9e41-eeeb5ddcba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = self.default_converter if converter is None else converter()\n",
    "\n",
    "# TODO: Need to check if the old engine file is compatible with current setting\n",
    "if not force_overwrite and os.path.exists(output_fpath):\n",
    "    return converter.trt_engine_class(output_fpath, self.network_metadata)\n",
    "\n",
    "converter.onnx_to_trt(\n",
    "    output_fpath,\n",
    "    self.fpath,\n",
    "    self.network_metadata,\n",
    "    profiles,\n",
    "    preview_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b1880-2012-4ba0-b770-60cfd7b3a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_trt_encoder_engine = WhisperEncoderONNXFile(os.path.join(wh_encoder_onnx_model_path, wh_encoder_onnx_model_fpath), wh_metadata).as_trt_engine(\n",
    "    encoder_engine_name,\n",
    "    profiles=[encoder_profile],\n",
    "    preview_features=preview_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c153b4-4c24-47ca-a0ed-481e133c69c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5f8ac-37f2-4587-86b0-1dbf8f082adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_trt_encoder_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c153b-a0fe-414c-8aa9-7b538d2e44aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ad5bd-0eb7-4397-8e5c-9387751516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_engine_name = os.path.join(tensorrt_model_path, t5_encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name = os.path.join(tensorrt_model_path, t5_decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name):\n",
    "    t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(encoder_onnx_model_path, t5_encoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        encoder_engine_name,\n",
    "        profiles=[encoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_encoder_engine = T5EncoderTRTEngine(encoder_engine_name, t5_metadata)\n",
    "\n",
    "if not os.path.exists(decoder_engine_name):\n",
    "    t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(decoder_onnx_model_path, t5_decoder_onnx_model_fpath), t5_metadata).as_trt_engine(\n",
    "        decoder_engine_name,\n",
    "        profiles=[decoder_profile],\n",
    "        preview_features=preview_features)\n",
    "else:\n",
    "    t5_trt_decoder_engine = T5DecoderTRTEngine(decoder_engine_name, t5_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d8635-4203-4b0f-9916-6370384ef7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the T5 model, ready for us to carry out inference. \n",
    "\n",
    "#### Single example inference\n",
    "The T5 model with TensorRT backend can now be employed in place of the original HuggingFace T5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954f2f4-c393-463b-a44b-3e5335032b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "from T5.trt import T5TRTEncoder, T5TRTDecoder\n",
    "\n",
    "t5_trt_encoder = T5TRTEncoder(\n",
    "                t5_trt_encoder_engine, t5_metadata, t5_config\n",
    "            )\n",
    "t5_trt_decoder = T5TRTDecoder(\n",
    "                t5_trt_decoder_engine, t5_metadata, t5_config, num_beams=num_beams\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9544ecb-2671-4b53-a544-08f13424cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a single sample\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)\n",
    "outputs = t5_trt_decoder(\n",
    "    expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71a327-546f-4b5b-bd42-caaffcceafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequence for an input\n",
    "max_length = 64\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (1, 1), tokenizer.convert_tokens_to_ids(tokenizer.pad_token), dtype=torch.int32\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4a98-b034-470e-a9f8-096d4100b8d4",
   "metadata": {},
   "source": [
    "#### TRT engine inference benchmark: encoder and decoder stacks\n",
    "First, we will bechmark the encoder and decoder stacks as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b37591-4398-40ff-8a39-5f75347192dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(\n",
    "    t5_trt_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "encoder_e2e_median_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5459da-a01b-4894-88dc-01b3637ded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_, decoder_e2e_median_time = decoder_inference(\n",
    "    t5_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, \n",
    "    expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    ")\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35624c0-7726-4166-9514-f11657722f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2403816-2a78-4aac-938d-89aedd9793cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "from Whisper.trt import WhisperTRTEncoder, WhisperTRTDecoder\n",
    "\n",
    "wh_trt_encoder = WhisperTRTEncoder(\n",
    "                wh_trt_encoder_engine, wh_metadata, wh_config\n",
    "            )\n",
    "# t5_trt_decoder = T5TRTDecoder(\n",
    "#                 t5_trt_decoder_engine, t5_metadata, t5_config, num_beams=num_beams\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6adbbe-b272-4524-854c-cc3d8f1c4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebfe03-7a60-4dd0-ad32-4e53d6012b07",
   "metadata": {},
   "source": [
    "### Full model inference benchmark\n",
    "\n",
    "Next, we will try the full TensorRT T5 engine for the task of translation. As before, note the time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cb550-24b9-48cd-a4ec-0bf18ac5e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "decoder_output, full_e2e_median_runtime = full_inference(\n",
    "    t5_trt_encoder,\n",
    "    t5_trt_decoder,\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[t5_metadata.variant],\n",
    "    num_beams=num_beams,\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))\n",
    "full_e2e_median_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f270-e7c8-4081-9587-2a3dc7ea95d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in upto ~10x performance improvement (from 0.0802s to 0.0082s for the T5-small variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch T5 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace T5 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [T5/trt.py](../T5/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2422-c55d-4d02-85f8-4e2f659e0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c456c-d1b1-4a62-890e-e3bffcacf436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144e0ea-94ff-423b-a6d7-e69a29f9dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f78ef8-cb17-4ea8-95e5-1c65fa99119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bc6fa-1441-4b11-b605-bbcfd40c3502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46752711-58a4-47a6-bffb-c80ced6a0e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a214df2-cf28-444f-a68a-351aba8c9b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
